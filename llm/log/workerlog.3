/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:14:19,992] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I1230 11:14:19.994216 4019294 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48565
I1230 11:14:20.027529 4019294 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:14:20.027556 4019294 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:14:20,027] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:14:20.034180 4019294 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:14:20.038718 4019294 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:19:32,047] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1230 11:19:32.049212 4022992 tcp_utils.cc:107] Retry to connect to 10.3.242.26:38255 while the server is not yet listening.
I1230 11:19:35.049535 4022992 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38255
I1230 11:19:35.067610 4022992 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:35.067691 4022992 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:35,068] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:19:35.071241 4022992 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:19:35.072110 4022992 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-30 11:19:39,098] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:19:39,099] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:19:39.099294 4022992 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:39.099323 4022992 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:39,099] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:19:39.099421 4022992 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:39.099428 4022992 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:39,099] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:19:39,099] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:19:39,100] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:19:39,100] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:39,100] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:19:39,100] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:39,100] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:39,100] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:19:39,101] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:19:39,102] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:19:39,103] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:19:39,104] [   DEBUG][0m - [0m
[32m[2024-12-30 11:19:39,106] [    INFO][0m - The global seed is set to 45, local seed is set to 53 and random seed is set to 42.[0m
[33m[2024-12-30 11:19:39,106] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
[32m[2024-12-30 11:19:39,107] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:19:39,109] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:19:39,109] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:26:13,127] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I1230 11:26:13.128397 4028348 tcp_utils.cc:107] Retry to connect to 10.3.242.26:38214 while the server is not yet listening.
I1230 11:26:16.128695 4028348 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38214
I1230 11:26:16.135710 4028348 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:16.136869 4028348 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:16,137] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:26:16.139396 4028348 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:26:16.140415 4028348 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-30 11:26:19,420] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:26:19,421] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:26:19.421303 4028348 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:19.421330 4028348 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:19,421] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:26:19.421415 4028348 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:19.421419 4028348 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:19,421] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:26:19,421] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:26:19,422] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:26:19,422] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:26:19,423] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:26:19,424] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:26:19,425] [   DEBUG][0m - [0m
[32m[2024-12-30 11:26:19,426] [    INFO][0m - The global seed is set to 45, local seed is set to 53 and random seed is set to 42.[0m
[33m[2024-12-30 11:26:19,426] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
[32m[2024-12-30 11:26:19,427] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:26:19,429] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:26:19,429] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:51:45,531] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I1231 15:51:45.533743 82309 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38873
I1231 15:51:45.534257 82309 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:45.534266 82309 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:45,534] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:51:45.536106 82309 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:51:45.537719 82309 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-31 15:51:47,734] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:51:47,734] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:51:47.734515 82309 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.734530 82309 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,734] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:51:47.734597 82309 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.734601 82309 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,734] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:51:47,734] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:51:47,735] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:51:47,735] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:51:47,736] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,737] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:51:47,738] [   DEBUG][0m - [0m
[32m[2024-12-31 15:51:47,739] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[33m[2024-12-31 15:51:47,739] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:51:47,739] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:51:47,741] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:51:47,741] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:51:47,741] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:53:07,255] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:56:39,031] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 15:56:39.032509 85670 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48936
I1231 15:56:39.095366 85670 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:39.095391 85670 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:39,095] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:56:39.099056 85670 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:56:39.099890 85670 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-31 15:56:41,356] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:56:41,356] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:56:41.356942 85670 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:41.356959 85670 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:41,356] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:56:41.357043 85670 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:41.357048 85670 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:41,357] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:56:41,357] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:56:41,357] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:56:41,357] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,357] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:56:41,357] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:56:41,358] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:56:41,359] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:56:41,360] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:56:41,361] [   DEBUG][0m - [0m
[32m[2024-12-31 15:56:41,363] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[33m[2024-12-31 15:56:41,363] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:56:41,363] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:56:41,365] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:56:41,365] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:56:41,366] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:57:46,639] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 15:58:49,594] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 15:58:49,595] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 15:58:49,597] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 15:58:51,243] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 15:58:51,304] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 15:58:51,305] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 15:58:52,435] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-12-31 15:58:52,573] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_rank                  : 3[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_15-56-39_ubuntu[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - optimizer_name_suffix         : shard03[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 15:58:52,671] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 15:58:52,672] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - sharding_parallel_rank        : 3[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 15:58:52,673] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 15:58:52,674] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 15:58:52,675] [   DEBUG][0m - [0m
[32m[2024-12-31 15:58:52,676] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 15:58:52,680] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 15:58:53,439] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 15:58:53,440] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 15:58:53,441] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 15:58:54,080] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 15:58:54) [0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 15:58:54,080] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 15:58:54,085] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 78, in tokenize_unsupervised_example
    raise DataFormatError(
utils.data.DataFormatError: Example format is wrong, please check: {'text': "\\section{Introduction}\n\nTotal variation (TV) denoising is a nonlinear filtering method based on\nthe assumption that the underlying signal is piecewise constant\n(equivalently, the derivative of the underlying signal is \\emph{sparse}) \\cite{ROF_1992}.\nSuch signals arise in geoscience, biophysics, and other areas \\cite{Little_2011_RSoc_Part1}.\nThe TV denoising technique is also used in conjunction with\nother methods in order to process more general types of signals \\cite{Gholami_2013_SP, Easley_2009_shearlet_tv, DurandFroment_2003_SIAM, Ding_2015_SPL}. \n\nTotal variation denoising is prototypical of methods based on sparse signal models.\nIt is defined by the minimization of a convex cost function \ncomprising a quadratic data fidelity term and a non-differentiable convex penalty term.\nThe penalty term \nis the composition of a linear operator and the $ \\ell_1 $ norm.\nAlthough the $ \\ell_1 $ norm stands out as the convex penalty that most \neffectively induces sparsity \\cite{Hastie_2015_CRC_book},\nnon-convex penalties \ncan lead to more accurate estimation of the underlying signal \\cite{Nikolova_2000_SIAM, Nikolova_2005_MMS, Nikolova_2010_TIP, Rodriguez_2009_TIP, Storath_2014_TSP}.\n\n\n\nA few recent papers consider the prescription of non-convex penalties \nthat maintain the convexity of the TV denoising cost function \\cite{Lanza_2016_JMIV, MalekMohammadi_2016_TSP, Selesnick_SPL_2015, Astrom_2015_cnf}.\n(The motivation for this is to leverage the benefits of both non-convex penalization and convex optimization,\ne.g., to accurately estimate the amplitude of jump discontinuities while guaranteeing the uniqueness of the solution.)\nThe penalties considered in these works are separable (additive).\nBut non-separable penalties can outperform separable penalties in this context.\nThis is because preserving the convexity of the cost function is a severely limiting requirement. \nNon-separable penalties can more successfully meet this requirement \nbecause they are more general than separable penalties \\cite{Selesnick_2016_TSP_BISR}.\n\n\n\nThis paper proposes a non-separable non-convex penalty for total variation denoising that generalizes the standard penalty\nand maintains the convexity of the cost function to be minimized.%\n\\footnote{Software is available at {http://eeweb.poly.edu/iselesni/mtvd}} \nThe new penalty, which is based on the Moreau envelope,\ncan more accurately estimate the amplitudes of jump discontinuities\nin an underlying piecewise constant signal. \n\n\\subsection{Relation to Prior Work} \n\nNumerous non-convex penalties and algorithms have been proposed\nto outperform $ \\ell_1 $-norm regularization for the estimation\nof sparse signals\ne.g., \\cite{Castella_2015_camsap_noabr, Candes_2008_JFAP, Nikolova_2011_chap, Mohimani_2009_TSP, Marnissi_2013_ICIP, Chartrand_2014_ICASSP, Chouzenoux_2013_SIAM, Portilla_2007_SPIE, Zou_2008_AS, Chen_2014_TSP_Convergence, Wipf_2011_tinfo}.\nHowever, few of these methods maintain the convexity of the cost function.\nThe prescription of non-convex penalties maintaining cost function convexity\nwas pioneered by Blake, Zisserman, and Nikolova \\cite{Blake_1987, Nikolova_1998_ICIP, Nikolova_2010_TIP, Nikolova_2011_chap}, \nand further developed in Refs.~\\cite{Bayram_2015_SPL, Bayram_2016_TSP, Chen_2014_TSP_ncogs, Ding_2015_SPL, He_2016_MSSP, Lanza_2016_JMIV, MalekMohammadi_2016_TSP, Parekh_2016_SPL_ELMA, Selesnick_2014_TSP_MSC, Selesnick_SPL_2015}.\nThese works rely on the presence of both strongly and weakly convex terms,\nwhich is also exploited in \\cite{Mollenhoff_2015_SIAM}.\n\nThe proposed penalty is expressed as\na differentiable convex function subtracted from the standard penalty (i.e., $ \\ell_1 $ norm).\nPrevious works also use this idea\n\\cite{Parekh_2015_SPL, Selesnick_2016_TSP_BISR, Parekh_2016_SPL_ELMA}.\nBut the differentiable convex functions used therein are either separable\n\\cite{Parekh_2015_SPL, Parekh_2016_SPL_ELMA}\nor sums of bivariate functions \\cite{Selesnick_2016_TSP_BISR}. \n\nIn parallel with the submission of this paper, Carlsson has also proposed using Moreau envelopes\nto prescribe non-trivial convex cost functions \\cite{Carlsson_2016_arxiv}.\nWhile the approach in \\cite{Carlsson_2016_arxiv} \nstarts with a given non-convex cost function (e.g., with the $ \\ell_0 $ pseudo-norm penalty)\nand seeks the convex envelope, \nour approach starts with the $ \\ell_1 $-norm penalty\nand seeks a class of convexity-preserving penalties.\n\nSome forms of generalized TV are based on infimal convolution\n(related to the Moreau envelope)\n\\cite{Setzer_2011_CMS, Chambolle_1997_NumerMath, Burger_2016, Becker_2014_JNCA_nomonth}.\nBut these works\npropose convex penalties suitable for non-piecewise-constant signals,\nwhile\nwe propose non-convex penalties suitable for piecewise-constant signals.\n\n\\section{Total Variation Denoising}\n\n\\begin{defn}\nGiven $ y \\in \\mathbb{R}^N $ and $ {\\lambda}  > 0 $, \ntotal variation denoising is defined as\n\\begin{align}\n\t\\label{eq:tvd}\n\t\\tvd(y ; {\\lambda} )\n\t& =\n\t\\arg\\min_{ x \\in \\mathbb{R}^N } \n\t\\bigl\\{\n\t\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}   \\norm{ D x }_1\n\t\\bigr\\}\n\t\\\\\n\t\\label{eq:prox}\n\t& = \n\t\\operatorname{prox}_{{\\lambda}  \\norm{ D \\, \\cdot \\, }_1 }(y)\n\\end{align}\nwhere\n$ D $ is the $ (N-1) \\times N $ matrix\n\\begin{equation}\n\t\\label{eq:defD}\n\tD = \n\t\\begin{bmatrix}\n\t\t-1 & 1 & & & \\\\\n\t\t & -1 & 1 & & \\\\\n\t\t &  & \\ddots & \\ddots &  \\\\\n\t\t& & & -1 & 1\n\t\\end{bmatrix}.\n\\end{equation}\n\\end{defn}\n\nAs indicated in \\eqref{eq:prox},  TV denoising is the proximity operator \\cite{Combettes_2011_chap}\nof the function $ x \\mapsto {\\lambda}  \\norm{ D x }_1 $.\nIt is convenient that TV denoising can be calculated exactly in finite-time \\cite{Condat_2013, Dumbgen_2009, Johnson_2013_JCGS, Darbon_2006_JMIV_part1}. \n\n\\section{Moreau Envelope}\n\nBefore we define the non-differentiable non-convex penalty\nin Sec.~\\ref{sec:pen},\nwe first define a differentiable convex function.\nWe use the Moreau envelope from convex analysis \\cite{Bauschke_2011}.\n\n\n\\begin{defn}\nLet $ \\alpha \\ge 0 $.\nWe define $ S_\\alpha \\colon \\mathbb{R}^N \\to \\mathbb{R} $\nas\n\\begin{equation}\n\t\\label{eq:defS}\n\tS_\\alpha( x ) = \\min_{ v \\in \\mathbb{R}^N }\n\t\\bigl\\{\n\t\t\\norm{ D v }_1 + \\tfrac{ \\alpha }{ 2 } \\norm{ x - v }_2^2  \\, \n\t\\bigr\\}\n\\end{equation}\nwhere $ D $ is the first-order difference matrix \\eqref{eq:defD}.\n\\end{defn}\n\nIf $ \\alpha > 0 $, then $ S_\\alpha $ is the \\emph{Moreau envelope} of index $ \\alpha^{-1} $ of the function $ x \\mapsto \\norm{ D x }_1 $.\n\n\n\\begin{prop}\n\\label{prop:calcS}\nThe function $ S_\\alpha $ can be calculated by\n\\ifTwoColumn\n\\begin{align}\n\t\\label{eq:zeroS}\n\tS_0( x )\n\t& = 0\n\t\\\\\n\t\\nonumber\n\tS_\\alpha(x)\n\t& = \\norm{ D \\tvd(x ; 1/\\alpha)  }_1\n\t\\\\\n\t&\n\t\\hspace{3em} {} + \\tfrac{ \\alpha }{ 2 } \\norm{ x - \\tvd(x ;  1/\\alpha) }_2^2  ,\n\t\\quad\n\t\\alpha > 0.\n\\end{align}\n\\else\n\\begin{align}\n\t\\label{eq:zeroS}\n\tS_0( x )\n\t& = 0\n\t\\\\\n\tS_\\alpha(x)\n\t& = \\norm{ D \\tvd(x ; 1/\\alpha)  }_1 + \\tfrac{ \\alpha }{ 2 } \\norm{ x - \\tvd(x ;  1/\\alpha) }_2^2  ,\n\t\\quad\n\t\\alpha > 0.\n\\end{align}\n\\fi\n\\end{prop}\n\\begin{proof}\nFor $ \\alpha = 0 $:\nSetting $ \\alpha = 0 $ and  $ v = 0 $ in \\eqref{eq:defS} gives \\eqref{eq:zeroS}.\nFor $ \\alpha > 0 $:\nBy the definition of TV denoising, \nthe $ v \\in \\mathbb{R}^N $ minimizing the function in \\eqref{eq:defS}\nis the TV denoising of $ x $, i.e.,\n$ v\\opt = \\tvd( x , 1/\\alpha ) $.\n\\end{proof}\n\n\n\\begin{prop}\nLet $ \\alpha \\ge 0 $.\nThe function $ S_\\alpha $ satisfies\n\\begin{equation}\n\t\\label{eq:boundS}\n\t0 \\le S_\\alpha(x) \\le \\norm{ D x }_1,\n\t\\ \\ \\forall x \\in \\mathbb{R}^N.\n\\end{equation}\n\\end{prop}\n\\begin{proof}\nFrom \\eqref{eq:defS}, we have\n$\n\tS_\\alpha( x )\n\t \\le \n\t\\norm{ D v }_1 + (\\alpha/2) \\norm{ x - v }_2^2  \n$\nfor all $ v \\in \\mathbb{R}^N $.\nIn particular, $ v = x $ leads to $ S_\\alpha(x) \\le \\norm{ D x }_1 $.\nAlso, $ S_\\alpha(x) \\ge 0 $ since $ S_\\alpha(x) $ is defined as the minimum of a non-negative function.\n\\end{proof}\n\n\\begin{prop}\nLet $ \\alpha \\ge 0 $. \nThe function $ S_\\alpha $ is convex and differentiable. \n\\end{prop}\n\\begin{proof}\nIt follows from Proposition 12.15 in Ref.~\\cite{Bauschke_2011}.\n\\end{proof}\n\n\\begin{prop}\n\\label{prop:Sgrad}\nLet $ \\alpha \\ge 0 $. \nThe gradient of $ S_\\alpha $ is given by\n\\begin{align}\n\t\\label{eq:gradS0}\n\t\\nabla S_0(x)\n\t& = 0\n\t\\\\\n\t\\label{eq:gradS}\n\t\\nabla S_\\alpha(x)\n\t& =\n\t\\alpha \\bigl( x - \\tvd( x ; 1 / \\alpha ) \\bigr),\n\t\\quad \\alpha > 0\n\\end{align}\nwhere $ \\tvd $ denotes total variation denoising \\eqref{eq:tvd}.\n\\end{prop}\n\\begin{proof}\nSince $ S_\\alpha $ is the Moreau envelope of index $ \\alpha^{-1} $ of the function $ x \\mapsto \\norm{ D x }_1 $ when $ \\alpha > 0 $, \nit follows by Proposition 12.29 in Ref.~\\cite{Bauschke_2011} that\n\\begin{equation}\n\t\\nabla S_\\alpha(x)\n\t= \n\t\\alpha \\bigl( x - \\operatorname{prox}_{(1/\\alpha)\\norm{ D \\, \\cdot \\, }_1 }(x) \\bigr).\n\\end{equation}\nThis proximity operator is TV denoising, giving \\eqref{eq:gradS}.\n\\end{proof}\n\n\\section{Non-convex Penalty}\n\\label{sec:pen}\n\nTo strongly induce sparsity of $ D x $, we define a non-convex generalization of the standard TV penalty.\nThe new penalty is defined by subtracting a differentiable convex function from the standard penalty. \n\n\\begin{defn}\nLet $ \\alpha \\ge 0 $.\nWe define the penalty $ \\psi_\\alpha \\colon \\mathbb{R}^N \\to \\mathbb{R} $ as\n\\begin{equation}\n\t\\label{eq:defpsi}\n\t\\psi_\\alpha( x ) = \\norm{ D x }_1 - S_\\alpha( x ) \n\\end{equation}\nwhere $ D $ is the matrix \\eqref{eq:defD}\nand\n$ S_\\alpha $ is defined by \\eqref{eq:defS}.\n\\end{defn}\n\nThe proposed penalty is upper bounded by the standard TV penalty,\nwhich is recovered as a special case. \n\n\\begin{prop}\nLet $ \\alpha \\ge 0 $. \nThe penalty $ \\psi_\\alpha $ satisfies \n\\begin{equation}\n\t\\psi_0( x ) = \\norm{ D x }_1,\n\t\\ \\ \\forall x \\in \\mathbb{R}^N\n\\end{equation}\nand\n\\begin{equation}\n\t\\label{eq:psibound}\n\t0 \\le \\psi_\\alpha(x) \\le \\norm{ D x }_1,\n\t\\ \\ \\forall x \\in \\mathbb{R}^N.\n\\end{equation}\n\\end{prop}\n\n\\begin{proof}\nIt follows from \\eqref{eq:zeroS} and \\eqref{eq:boundS}.\n\\end{proof}\n\nWhen a convex function is subtracted from another convex function\n[as in \\eqref{eq:defpsi}],\nthe resulting function may well be negative on part of its domain. \nInequality \\eqref{eq:psibound} states that the proposed penalty $ \\psi_\\alpha $ avoids this fate.\nThis is relevant because the penalty function should be non-negative.\n\nFigures in the supplemental material show examples\nof the proposed penalty $ \\psi_\\alpha $ and the function $ S_\\alpha $.\n\n\n\n\\section{Enhanced TV Denoising}\n\nWe define `Moreau-enhanced' TV denoising.\nIf $ \\alpha > 0 $, then the proposed penalty penalizes large amplitude values of $ D x $ \nless than the $ \\ell_1 $ norm does (i.e., $ \\psi_\\alpha(x) \\le \\norm{D x}_1 $),\nhence it is less likely to underestimate jump discontinuities.\n\n\\begin{defn}\nGiven $ y \\in \\mathbb{R}^N $,  $ {\\lambda}  > 0 $,  and $ \\alpha \\ge 0 $, \nwe define Moreau-enhanced total variation denoising as\n\\begin{equation}\n\t\\label{eq:mtvd}\n\t\\mtvd( y ; {\\lambda} , \\alpha) =\n\t\\arg \\min_{ x \\in \\mathbb{R}^N }\n\t\\bigl\\{\n\t\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\psi_\\alpha( x )\n\t\\bigr\\}\n\\end{equation}\nwhere $ \\psi_\\alpha $ is given by \\eqref{eq:defpsi}.\n\\end{defn}\n\nThe parameter $ \\alpha $ controls the non-convexity of the penalty. \nIf $ \\alpha = 0 $, then the penalty is convex\nand Moreau-enhanced TV denoising reduces to TV denoising.\nGreater values of $ \\alpha $ make the penalty more non-convex. \nWhat is the greatest value of $ \\alpha $ that maintains convexity \nof the cost function?\nThe critical value is given by Theorem \\ref{thm:cond}.\n\n\\begin{theorem}\n\\label{thm:cond}\nLet $ {\\lambda}  > 0 $ and $ \\alpha \\ge 0 $.\nDefine \n$ F_\\alpha \\colon \\mathbb{R}^N \\to \\mathbb{R} $ \nas\n\\begin{equation}\n\t\\label{eq:defF}\n\tF_\\alpha(x) = \\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\psi_\\alpha( x )\n\\end{equation}\nwhere $ \\psi_\\alpha $ is given by \\eqref{eq:defpsi}.\nIf\n\\begin{equation}\n\t\\label{eq:convcond}\n\t0 \\le \\alpha \\le  1 / {\\lambda}  \n\\end{equation}\nthen $ F_\\alpha $ is convex.\nIf $ 0 \\le \\alpha < 1/{\\lambda}  $ then $ F_\\alpha $ is strongly convex.\n\\end{theorem}\n\\begin{proof}\nWe write the cost function as\n\\ifTwoColumn\n   \\begin{align}\n\tF_\\alpha(x) \n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 - {\\lambda}  S_\\alpha( x )  \n\t\\\\\n\t\\nonumber\n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 \n\t\t\\\\\n\t\t&\n\t\t\\hspace{3em}\n\t\t{} - \n\t\t {\\lambda}  \\min_{ v \\in \\mathbb{R}^N } \\bigl\\{  \\norm{ D v }_1 + \\tfrac{ \\alpha }{ 2 } \\norm{ x - v }_2^2  \\, \\bigr\\}\n\t\\\\\n\t\\nonumber\n\t& = \n\t \\max_{ v \\in \\mathbb{R}^N }\n\t \\bigl\\{   \n\t\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1\n\t\t\\\\\n\t\t&\n\t\t\\hspace{6em}\n\t\t{} - \n\t\t{\\lambda}  \\norm{ D v }_1 - \\tfrac{ {\\lambda}  \\alpha }{ 2 } \\norm{ x - v }_2^2  \n\t \\, \\bigr\\}\n\t\\\\\n\t& = \n\t \\max_{ v \\in \\mathbb{R}^N }\n\t \\bigl\\{   \n\t\t\\tfrac{1}{2} ( 1 - {\\lambda}  \\alpha ) \\norm{ x }_2^2 + {\\lambda}  \\norm{ D x }_1 + g(x, v)\n\t \\, \\bigr\\}\n\t\\\\\n\t& = \n\t\\tfrac{1}{2} ( 1 - {\\lambda}  \\alpha ) \\norm{ x }_2^2 + {\\lambda}  \\norm{ D x }_1\n\t+\n\t\\max_{ v \\in \\mathbb{R}^N }\n\tg(x, v)\n   \\end{align}\n\\else\n   \\begin{align}\n\tF_\\alpha(x) \n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 - {\\lambda}  S_\\alpha( x )  \n\t\\\\\n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 - \n\t\t {\\lambda}  \\min_{ v \\in \\mathbb{R}^N } \\bigl\\{  \\norm{ D v }_1 + \\tfrac{ \\alpha }{ 2 } \\norm{ x - v }_2^2  \\, \\bigr\\}\n\t\\\\\n\t& = \n\t \\max_{ v \\in \\mathbb{R}^N }\n\t \\bigl\\{   \n\t\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 - \n\t\t{\\lambda}  \\norm{ D v }_1 - \\tfrac{ {\\lambda}  \\alpha }{ 2 } \\norm{ x - v }_2^2  \n\t \\, \\bigr\\}\n\t\\\\\n\t& = \n\t \\max_{ v \\in \\mathbb{R}^N }\n\t \\bigl\\{   \n\t\t\\tfrac{1}{2} ( 1 - {\\lambda}  \\alpha ) \\norm{ x }_2^2 + {\\lambda}  \\norm{ D x }_1 + g(x, v)\n\t \\, \\bigr\\}\n\t\\\\\n\t& = \n\t\\tfrac{1}{2} ( 1 - {\\lambda}  \\alpha ) \\norm{ x }_2^2 + {\\lambda}  \\norm{ D x }_1\n\t+\n\t\\max_{ v \\in \\mathbb{R}^N }\n\tg(x, v)\n   \\end{align}\n\\fi\nwhere $ g(x, v) $ is affine in $ x $.\nThe last term is convex as it is the point-wise maximum of a set of convex functions.\nHence, $ F_\\alpha $ is a convex function if $ 1 - {\\lambda}  \\alpha \\ge 0 $.\nIf  $ 1 - {\\lambda}  \\alpha > 0 $, then\n$ F_\\alpha $ is strongly convex (and strictly convex).\n\\end{proof}\n\n\n\\section{Algorithm}\n\\label{sec:alg}\n\n\\begin{prop}\nLet $ y \\in \\mathbb{R}^N $,  $ {\\lambda}  > 0 $,  and $ 0 < \\alpha < 1/{\\lambda}  $.\nThen $ x\\iter{k} $ produced by the iteration\n\\begin{subequations}\n\\label{eq:alg}\n\\begin{align}\n\tz\\iter{k} & = \n\t\ty + {\\lambda}  \\alpha \\bigl(  x\\iter{k} -  \\tvd( x\\iter{k} ; 1 / \\alpha ) \\bigr)\n\t\\\\\n\tx\\iter{k+1} & =\n\t\\tvd( z\\iter{k} ; {\\lambda}  ).\n\\end{align}\n\\end{subequations}\nconverges to the solution of the Moreau-enhanced TV denoising problem \\eqref{eq:mtvd}.\n\\end{prop}\n\n\\begin{proof}\nIf the cost function \\eqref{eq:defF} is strongly convex, then the minimizer \ncan be calculated using \nthe forward-backward splitting (FBS) algorithm \\cite{Bauschke_2011, Combettes_2011_chap}.\nThis algorithm minimizes a function of the form\n\\begin{equation}\n\t\\label{eq:FBSd}\n\tF(x) = f_1(x) + f_2(x)\n\\end{equation}\nwhere\nboth $ f_1 $ and $ f_2 $ are convex and $ \\nabla f_1 $ is additionally Lipschitz continuous.\nThe FBS algorithm is given by\n\\begin{subequations}\n\\label{eq:fbs}\n\\begin{align}\n\tz\\iter{k} & = \n\tx\\iter{k} - \\mu \\bigl[  \\nabla f_1( x\\iter{k} ) \\bigr]\n\t\\\\\n\tx\\iter{k+1} & = \\arg \\min_x \\big\\{ \\tfrac{1}{2} \\norm{ z\\iter{k} - x }_2^2 + \\mu f_2( x ) \\big\\}\n\\end{align}\n\\end{subequations}\nwhere\n$ 0 < \\mu < 2 / \\rho $\nand\n$ \\rho $ is the Lipschitz constant of $ \\nabla f_1 $.\nThe iterates $ x\\iter{k} $ \n converge to a minimizer of $ F $.\n\nTo apply the FBS algorithm to the proposed cost function \\eqref{eq:defF},\nwe write it as\n\\begin{align}\n\tF_\\alpha(x) \n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\psi_\\alpha( x ),\n\t\\\\\n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2 + {\\lambda}  \\norm{ D x }_1 - {\\lambda}  S_\\alpha( x )  \n\t\\\\\n\t& = \n\tf_1(x) + f_2(x)\n\\end{align}\nwhere\n\\begin{subequations}\n\\label{eq:deff12}\n\\begin{align}\n\t\\label{eq:deff1}\n\tf_1(x)\n\t& = \n\t\\tfrac{1}{2} \\norm{ y - x }_2^2\n\t-\n\t{\\lambda}  S_\\alpha( x )  \n\t\\\\\n\t\\label{eq:deff2}\n\tf_2(x)\n\t& = \n\t{\\lambda}  \\norm{ D x }_1.\n\\end{align}\n\\end{subequations}\nThe gradient of $ f_1 $ is given by\n\\begin{align}\n\t\\nabla f_1(x) \n\t& =\n\tx - y - {\\lambda}  \\nabla S_\\alpha( x )\n\t\\\\\n\t& =\n\tx - y - {\\lambda}  \\alpha \\bigl(  x -  \\tvd( x ; 1 / \\alpha ) \\bigr)\n\\end{align}\nusing Proposition \\ref{prop:Sgrad}.\nSubtracting $ S_\\alpha $ from $ f_1 $ does not increase the Lipschitz constant of $ \\nabla f_1 $,\nthe value of which is 1.\nHence, we may set $ 0 < \\mu < 2 $.\n\nUsing \\eqref{eq:deff12}, the FBS algorithm \\eqref{eq:fbs} becomes\n\\ifTwoColumn\n   \\begin{subequations}\n   \\begin{align}\n   \t\\nonumber\n\tz\\iter{k} & = \n\tx\\iter{k} - \\mu \\bigl[\n\t\tx\\iter{k} - y\n\t\\\\\n\t\t& \\hspace{4em} {} - {\\lambda}  \\alpha \\bigl(  x\\iter{k} -  \\tvd( x\\iter{k} ; 1 / \\alpha ) \\bigr)\n\t\\bigr]\n\t\\\\\n\t\\label{eq:xupdate}\n\tx\\iter{k+1} & = \\arg \\min_x \\big\\{ \\tfrac{1}{2} \\norm{ z\\iter{k} - x }_2^2 + \\mu {\\lambda}  \\norm{ D x }_1 \\big\\}.\n   \\end{align}\n   \\end{subequations}\n\\else\n   \\begin{subequations}\n   \\begin{align}\n\tz\\iter{k} & = \n\tx\\iter{k} - \\mu \\bigl[\n\t\tx\\iter{k} - y - {\\lambda}  \\alpha \\bigl(  x\\iter{k} -  \\tvd( x\\iter{k} ; 1 / \\alpha ) \\bigr)\n\t\\bigr]\n\t\\\\\n\t\\label{eq:xupdate}\n\tx\\iter{k+1} & = \\arg \\min_x \\big\\{ \\tfrac{1}{2} \\norm{ z\\iter{k} - x }_2^2 + \\mu {\\lambda}  \\norm{ D x }_1 \\big\\}.\n   \\end{align}\n   \\end{subequations}\n\\fi\nNote that \\eqref{eq:xupdate} is TV denoising \\eqref{eq:tvd}.\nUsing the value $ \\mu = 1 $ gives iteration \\eqref{eq:alg}.\n(Experimentally, we found this value yields fast convergence.) \n\\end{proof}\n\n\nEach iteration of \\eqref{eq:alg} entails solving two standard TV denoising problems.\nIn this work, we calculate TV denoising using the fast exact C language program by Condat \\cite{Condat_2013}.\nLike the\niterative shrinkage/thresholding algorithm (ISTA) \\cite{Daubechies_2004, Fig_2003_TIP},\nalgorithm \\eqref{eq:alg} \ncan be accelerated in various ways.\n\n We suggest not setting $ \\alpha $ too close to the critical value $ 1/{\\lambda}  $ \nbecause the FBS algorithm generally converges faster when \nthe cost function is more strongly convex ($ \\alpha < 1 $).\n\nIn summary, the proposed\nMoreau-enhanced TV denoising method comprises the steps:\n\\begin{enumerate}\n\\item\nSet the regularization parameter $ {\\lambda}  $ ($ {\\lambda}  > 0 $).\n\\item\nSet the non-convexity parameter $ \\alpha $ ($ 0 \\le \\alpha < 1/{\\lambda}  $).\n\\item\nInitialize $ x\\iter{0} = 0 $.\n\\item\nRun iteration \\eqref{eq:alg} until convergence.\n\\end{enumerate}\n\n\n\n\\section{Optimality Condition}\n\nTo avoid terminating the iterative algorithm too early, \nit is useful to verify convergence using an optimality condition.\n\n\\begin{prop}\n\\label{prop:opt}\nLet $ y \\in \\mathbb{R}^N $,  $ {\\lambda}  > 0 $,  and $ 0 < \\alpha < 1/{\\lambda}  $.\nIf $ x $ is a solution to \\eqref{eq:mtvd},\nthen\n\\begin{equation}\n\t\\label{eq:opt}\n\t\\bigl[ C \\bigl( (x - y)/{\\lambda}  + \\alpha (  \\tvd( x ; 1 / \\alpha ) - x )  \\bigr)     \\bigr]_n \n\t\\in\n\t\\sign( [ D x ]_n )\n\\end{equation}\nfor $ n = 0, \\dots, N-1 $,\nwhere $ C \\in \\mathbb{R}^{ (N-1) \\times N } $ is\ngiven by\n\\begin{equation}\n\t\\label{eq:defC}\n\tC_{m, n} =\n\t\\begin{cases}\n\t\t1, \\  & m \\ge n\n\t\t\\\\\n\t\t0, & m < n,\n\t\\end{cases}\n\t\\quad\n\t\\text{i.e.,}\n\t\\quad\t\n\t[C x]_n = \\sum_{ m \\le n } x_m\n\\end{equation}\nand $ \\sign $ is the set-valued signum function\n\\begin{equation}\n\t\\sign(t) = \n\t\\begin{cases}\n\t\t\\{ -1 \\}, \\ \\ & t < 0\n\t\t\\\\\n\t\t[-1, 1], & t = 0\n\t\t\\\\\n\t\t\\{ 1 \\}, & t > 0.\n\t\\end{cases}\n\\end{equation}\n\\end{prop}\n\nAccording to \\eqref{eq:opt},\nif $ x \\in \\mathbb{R}^N $ is a minimizer, then\nthe points $ ( [Dx]_n, u_n) \\in \\mathbb{R}^2 $ must lie on the graph of the signum function,\nwhere $ u_n $ denotes the value on the left-hand side of \\eqref{eq:opt}.\nHence, the optimality condition can be depicted as a scatter plot. \nFigures in the supplemental material show how the points in the scatter plot converge\nto the signum function as the algorithm \\eqref{eq:alg} progresses.\n\n\\begin{proof}[Proof of Proposition \\ref{prop:opt}]\nA vector $ x $ minimizes a convex function $ F $ if $ 0 \\in \\partial F( x ) $\nwhere $ \\partial F(x) $ is the subdifferential of $ F $ at $ x $.\nThe subdifferential of the cost function \\eqref{eq:defF} is given by\n\\begin{equation}\n\t\\partial F_\\alpha( x ) = x - y - {\\lambda}  \\nabla S_\\alpha(x) + \\partial (  {\\lambda}  \\norm{ D \\, \\cdot \\, }_1 )(x)\n\\end{equation}\nwhich can be written as\n\\ifTwoColumn\n\t\\begin{multline}\n\t\t\\partial F_\\alpha( x ) = \\{ x - y - {\\lambda}  \\nabla S_\\alpha(x) +  {\\lambda}  D\\tr \\!  u\n\t\t\\\\\n\t\t{}  :  u_n \\in \\sign( [ D x ]_n ) , \\, u \\in \\mathbb{R}^{N-1} \\}.\n\t\\end{multline}\n\\else\n\t\\begin{equation}\n\t\t\\partial F_\\alpha( x ) = \\{ x - y - {\\lambda}  \\nabla S_\\alpha(x) +  {\\lambda}  D\\tr \\!  u  :  u_n \\in \\sign( [ D x ]_n ) , \\, u \\in \\mathbb{R}^{N-1} \\}.\n\t\\end{equation}\n\\fi\nHence, the condition $ 0 \\in \\partial F_\\alpha( x ) $ can be written as\n\\ifTwoColumn\n\t\\begin{multline}\n\t\t(y - x)/{\\lambda}  + \\nabla S_\\alpha(x)\n\t\t\\\\\n\t\t{} \\in\n\t\t\\{ D\\tr \\! u :  u_n \\in \\sign( [ D x]_n ) , \\, u \\in \\mathbb{R}^{N-1}  \\}.\n\t\\end{multline}\n\\else\n\t\\begin{equation}\n\t\t(y - x)/{\\lambda}  + \\nabla S_\\alpha(x)\n\t\t\\in\n\t\t\\{ D\\tr \\! u :  u_n \\in \\sign( [ D x]_n ) , \\, u \\in \\mathbb{R}^{N-1}  \\}.\n\t\\end{equation}\n\\fi\n\nLet $ C $ be a matrix of size $ (N-1) \\times N $ such that  $ C D\\tr = -I $, \ne.g., \\eqref{eq:defC}.\nIt follows that the condition $ 0 \\in \\partial F_\\alpha( x ) $ implies that\n\\begin{equation}\n\t \\bigl[ C \\bigl( (x - y)/{\\lambda}  - \\nabla S_\\alpha(x) \\bigr) \\bigr]_n \n\t\\in\n\t\\sign( [ D x]_n )\n\\end{equation}\nfor $ n = 0, \\dots, N-1 $.\nUsing Proposition \\ref{prop:Sgrad} gives \\eqref{eq:opt}.\n\\end{proof}\n\n\n\n\n\\section{Example}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics{example1_singlecolumn}\n\t\\caption{\n\t\tTotal variation denoising using three different penalties. \n\t\t(The dashed line is the true noise-free signal.)\n\t}\n\t\\label{fig:example1}\n\\end{figure}\n\nThis example applies TV denoising to the noisy piecewise constant signal \nshown in Fig.~\\ref{fig:example1}(a).\nThis is the `blocks'  signal (length $ N = 256 $) \ngenerated by the Wavelab  \\cite{wavelab} function \\texttt{MakeSignal}\nwith additive white Gaussian noise ($ \\sigma = 0.5 $).\nWe set the regularization parameter to $ {\\lambda}  = \\sqrt{N} \\sigma /4 $\nfollowing a discussion in Ref.~\\cite{Dumbgen_2009}.\nFor Moreau-enhanced TV denoising,\nwe set the non-convexity parameter to $ \\alpha = 0.7 / {\\lambda}  $.\n\nFigure~\\ref{fig:example1} shows the result of TV denoising with three different penalties.\nIn each case, a \\emph{convex} cost function is minimized. \nFigure~\\ref{fig:example1}(b)\nshows the result using standard TV denoising (i.e., using the $ \\ell_1 $-norm). \nThis denoised signal consistently underestimates the amplitudes \nof jump discontinuities, especially those occurring near other jump discontinuities of opposite sign. \nFigure~\\ref{fig:example1}(c)\nshows the result using a separable non-convex penalty \\cite{Selesnick_SPL_2015}.\nThis method can use any non-convex scalar penalty satisfying a prescribed set of properties. \nHere we use the minimax-concave (MC) penalty~\\cite{Zhang_2010_AnnalsStat, Bayram_2015_SPL}\nwith non-convexity parameter set to maintain cost function convexity.\nThis result significantly improves the root-mean-square error (RMSE)\nand mean-absolute-deviation (MAE), \nbut still underestimates the amplitudes of jump discontinuities.\n\nMoreau-enhanced TV denoising, shown in Fig.~\\ref{fig:example1}(d),\nfurther reduces the RMSE and MAE and more accurately estimates\nthe amplitudes of jump discontinuities. \nThe proposed non-separable non-convex penalty \navoids the consistent underestimation of discontinuities\nseen in Figs.~\\ref{fig:example1}(b) and \\ref{fig:example1}(c).\n\nTo further compare the denoising capability of the considered penalties, \nwe calculate the average RMSE as a function of the noise level. \nWe let the noise standard deviation span the interval $ 0.2 \\le \\sigma \\le 1.0 $.\nFor each $ \\sigma $ value, we calculate the average RMSE of 100 noise realizations.\nFigure~\\ref{fig:rmse} shows that the proposed penalty yields\nthe lowest average RMSE for all $ \\sigma \\ge 0.4 $. \nHowever, at low noise levels, separable convexity-preserving penalties \\cite{Selesnick_SPL_2015}\nperform better than the proposed non-separable convexity-preserving penalty.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics{example2_rmse}\n\t\\caption{\n\t\tTV denoising using four penalties:\n\t\tRMSE as a function of noise level.\n\t}\n\t\\label{fig:rmse}\n\\end{figure}\n\n\n\n\\section{Conclusion}\n\nThis paper demonstrates the use of the Moreau envelope to \ndefine a non-separable non-convex TV denoising penalty \nthat maintains the convexity of the TV denoising cost function. \nThe basic idea is to subtract from a convex penalty its Moreau envelope.\nThis idea should also be useful for other problems, \ne.g., analysis tight-frame denoising \\cite{Parekh_2015_SPL}.\n\n\nSeparable convexity-preserving penalties \\cite{Selesnick_SPL_2015}\noutperformed the proposed one at low noise levels in the example.\nIt is yet to be determined if a more general class of convexity-preserving penalties can\noutperform both across all noise levels. \n\n\\bibliographystyle{plain}\n\n", 'meta': {'timestamp': datetime.datetime(2017, 1, 3, 2, 8, 2), 'yymm': '1701', 'arxiv_id': '1701.00439', 'language': 'en', 'url': 'https://arxiv.org/abs/1701.00439'}} or rewrite tokenize_example in data.py 
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:00:10,681] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I1231 16:00:10.682951 88150 tcp_utils.cc:107] Retry to connect to 10.3.242.26:58758 while the server is not yet listening.
I1231 16:00:13.683269 88150 tcp_utils.cc:130] Successfully connected to 10.3.242.26:58758
I1231 16:00:13.687372 88150 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:13.687383 88150 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:13,687] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:00:13.692504 88150 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:00:13.693292 88150 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-31 16:00:15,881] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:00:15,881] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:00:15.881626 88150 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.881651 88150 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,881] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:00:15.881731 88150 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.881736 88150 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,881] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:00:15,881] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:00:15,882] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:00:15,882] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:00:15,883] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:00:15,884] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:00:15,885] [   DEBUG][0m - [0m
[32m[2024-12-31 16:00:15,887] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[33m[2024-12-31 16:00:15,887] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:00:15,887] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:00:15,889] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:00:15,889] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:00:15,890] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:01:39,643] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:02:42,879] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:02:42,881] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:02:42,883] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:02:44,572] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:02:44,653] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:02:44,653] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:02:46,212] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-12-31 16:02:46,342] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_rank                  : 3[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-00-10_ubuntu[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - optimizer_name_suffix         : shard03[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:02:46,447] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:02:46,448] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - sharding_parallel_rank        : 3[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:02:46,449] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:02:46,450] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:02:46,451] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:02:46,451] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:02:46,451] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:02:46,451] [   DEBUG][0m - [0m
[32m[2024-12-31 16:02:46,452] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:02:46,477] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:02:47,286] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:02:47,287] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:02:47,291] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:02:47,890] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:02:47) [0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:02:47,890] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:02:47,895] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 85, in tokenize_unsupervised_example
    max_length=data_args.scaled_max_length,
AttributeError: 'DataConfig' object has no attribute 'scaled_max_length'. Did you mean: 'pad_to_max_length'?
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:04:53,616] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I1231 16:04:53.617760 96378 tcp_utils.cc:107] Retry to connect to 10.3.242.26:54723 while the server is not yet listening.
I1231 16:04:56.618410 96378 tcp_utils.cc:130] Successfully connected to 10.3.242.26:54723
I1231 16:04:56.687399 96378 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:56.687438 96378 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:56,687] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:04:56.691241 96378 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:04:56.692229 96378 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-12-31 16:04:59,163] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:04:59,163] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:04:59.164093 96378 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:59.164132 96378 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:59,164] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:04:59.164232 96378 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:59.164237 96378 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:59,164] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:04:59,164] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:04:59,165] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:04:59,165] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:04:59,166] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:59,167] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:59,168] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:04:59,169] [   DEBUG][0m - [0m
[32m[2024-12-31 16:04:59,170] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[33m[2024-12-31 16:04:59,170] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:04:59,171] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:04:59,173] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:04:59,174] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:04:59,175] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:06:09,760] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:07:23,235] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:07:23,238] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:07:23,242] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:07:25,185] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:07:25,272] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:07:25,272] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:07:26,416] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-12-31 16:07:26,563] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_rank                  : 3[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-04-53_ubuntu[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - optimizer_name_suffix         : shard03[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - sharding_parallel_rank        : 3[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - [0m
[32m[2024-12-31 16:07:26,665] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:07:26,668] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:07:27,451] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:07:27,452] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:07:27,453] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:07:28,086] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:07:28) [0m
[32m[2024-12-31 16:07:28,086] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:07:28,086] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:07:28,086] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:07:28,087] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:07:28,087] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:07:28,087] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:07:28,087] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:07:28,087] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:07:28,090] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2360, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2300, in compute_loss
    outputs = model(**inputs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 2101, in forward
    outputs = self.llama(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1741, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1607, in _prepare_decoder_attention_mask
    expanded_attn_mask = paddle.where(expanded_attn_mask.cast("bool"), 0.0, paddle.finfo(dtype).min)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/tensor/search.py", line 789, in where
    broadcast_zeros = paddle.add(broadcast_zeros, zeros_like_condition)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/tensor/math.py", line 735, in add
    return _C_ops.add(x, y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_add(_object*, _object*, _object*)
1   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
3   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
4   void phi::AddRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::Allocator::Allocate(unsigned long)
14  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
15  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
16  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 15.052577GB memory on GPU 3, 65.308655GB memory has been allocated and available memory is only 13.842346GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:14:26,279] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:14:26,279] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:14:26,279] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:14:26,279] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:14:26,279] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I1231 16:14:26.280869 112009 tcp_utils.cc:130] Successfully connected to 10.3.242.26:55726
I1231 16:14:26.281211 112009 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.281219 112009 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:26.281474 112009 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.281483 112009 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:26,281] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:14:26.285072 112009 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:14:26.287086 112009 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-12-31 16:14:29,188] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:14:29,189] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:14:29,189] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:14:29.190132 112009 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.190164 112009 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:29.190266 112009 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.190280 112009 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:29,190] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [0, 1, 2, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:14:29,190] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:14:29,192] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:14:29,206] [   DEBUG][0m - [0m
[32m[2024-12-31 16:14:29,210] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[33m[2024-12-31 16:14:29,210] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:14:29,213] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:14:29,218] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:14:29,219] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:14:29,220] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:15:51,005] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:15:51,014] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:15:51,014] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:15:51,014] [    INFO] pp_layers.py:631 - stage=0, global_rank=3 ,layer_number=9
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:631 - stage=1, global_rank=3 ,layer_number=8
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:631 - stage=2, global_rank=3 ,layer_number=8
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:631 - stage=3, global_rank=3 ,layer_number=10
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:15:51,015] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:15:51,126] [    INFO] pp_layers.py:708 - flush 10 of layers into run_function
[33m[2024-12-31 16:16:09,893] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:16:09,894] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:16:11,152] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:16:11,157] [   DEBUG][0m - Frozen parameters: 1.75e+09 || Trainable parameters:5.00e+06 || Total parameters:1.76e+09|| Trainable:0.28%[0m
[35m[2024-12-31 16:16:11,157] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:16:14,088] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[32m[2024-12-31 16:16:14,223] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-14-26_ubuntu[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - optimizer_name_suffix         : pp03[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pipeline_parallel_rank        : 3[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - weight_name_suffix            : pp03[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - [0m
[32m[2024-12-31 16:16:14,257] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:16:14,258] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:16:14,259] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 3
[2024-12-31 16:16:14,259] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:16:14,259] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:16:14) [0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:16:14,261] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:16:14,261] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 634, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:28:46,434] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:28:46,434] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:28:46,434] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I1231 16:28:46.435884 129652 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40539
I1231 16:28:46.436750 129652 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.436785 129652 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:46.437244 129652 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.437259 129652 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:46,437] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:28:46.439386 129652 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:28:46.440958 129652 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-12-31 16:28:49,487] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:28:49,488] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:28:49,489] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:28:49.489302 129652 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.489353 129652 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:49.489452 129652 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.489467 129652 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:49,489] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [0, 1, 2, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:28:49,490] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:28:49,491] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:28:49,505] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:28:49,505] [   DEBUG][0m - [0m
[32m[2024-12-31 16:28:49,509] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[33m[2024-12-31 16:28:49,509] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:28:49,512] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:28:49,517] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:28:49,518] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:28:49,519] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:30:21,329] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:30:21,334] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:30:21,334] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:30:21,334] [    INFO] pp_layers.py:631 - stage=0, global_rank=3 ,layer_number=9
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:631 - stage=1, global_rank=3 ,layer_number=8
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:631 - stage=2, global_rank=3 ,layer_number=8
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:631 - stage=3, global_rank=3 ,layer_number=10
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:30:21,335] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:30:21,411] [    INFO] pp_layers.py:708 - flush 10 of layers into run_function
[33m[2024-12-31 16:30:40,398] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:30:40,399] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:30:41,778] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:30:41,784] [   DEBUG][0m - Frozen parameters: 1.75e+09 || Trainable parameters:5.00e+06 || Total parameters:1.76e+09|| Trainable:0.28%[0m
[35m[2024-12-31 16:30:41,784] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:30:45,119] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[32m[2024-12-31 16:30:45,227] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:30:45,251] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:30:45,252] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:30:45,253] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:30:45,254] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-28-46_ubuntu[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:30:45,255] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - optimizer_name_suffix         : pp03[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - pipeline_parallel_rank        : 3[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:30:45,256] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:30:45,257] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:30:45,258] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:30:45,259] [   DEBUG][0m - weight_name_suffix            : pp03[0m
[35m[2024-12-31 16:30:45,260] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:30:45,260] [   DEBUG][0m - [0m
[32m[2024-12-31 16:30:45,260] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:30:45,298] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:30:45,298] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 3
[2024-12-31 16:30:45,298] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:30:45,298] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:30:45) [0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:30:45,300] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:30:45,300] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Exception in thread Thread-2 (_thread_loop):
ValueError: 8192 is not a valid PaddingStrategy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 81, in tokenize_unsupervised_example
    tokenized_source = tokenizer(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2440, in __call__
    return self.encode(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2510, in encode
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2106, in _get_padding_truncation_strategies
    padding_strategy = PaddingStrategy(padding)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 384, in __call__
    return cls.__new__(cls, value)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 708, in __new__
    raise exc
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 692, in __new__
    result = cls._missing_(value)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 120, in _missing_
    raise ValueError(
ValueError: 8192 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:31:44,200] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:31:44,200] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:31:44,200] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 16:31:44.202035 133208 tcp_utils.cc:107] Retry to connect to 10.3.242.26:51282 while the server is not yet listening.
I1231 16:31:47.202332 133208 tcp_utils.cc:130] Successfully connected to 10.3.242.26:51282
I1231 16:31:47.275554 133208 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.275615 133208 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:47.276100 133208 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.276113 133208 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:47,276] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:31:47.279078 133208 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:31:47.280596 133208 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-12-31 16:31:50,165] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:31:50,166] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:31:50,166] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:31:50.166682 133208 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.166734 133208 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:50.166858 133208 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.166872 133208 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:50,167] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [0, 1, 2, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:31:50,167] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:31:50,169] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:31:50,170] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,170] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - [0m
[32m[2024-12-31 16:31:50,188] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[33m[2024-12-31 16:31:50,189] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:31:50,192] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:31:50,198] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:31:50,200] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:31:50,200] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:32:53,254] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:32:53,262] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:631 - stage=0, global_rank=3 ,layer_number=9
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:631 - stage=1, global_rank=3 ,layer_number=8
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:631 - stage=2, global_rank=3 ,layer_number=8
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:631 - stage=3, global_rank=3 ,layer_number=10
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:32:53,263] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:32:53,354] [    INFO] pp_layers.py:708 - flush 10 of layers into run_function
[33m[2024-12-31 16:33:12,119] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:33:12,119] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:33:13,481] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:33:13,487] [   DEBUG][0m - Frozen parameters: 1.75e+09 || Trainable parameters:5.00e+06 || Total parameters:1.76e+09|| Trainable:0.28%[0m
[35m[2024-12-31 16:33:13,487] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:33:20,763] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[32m[2024-12-31 16:33:20,913] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-31-44_ubuntu[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - optimizer_name_suffix         : pp03[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - pipeline_parallel_rank        : 3[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:33:20,945] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - weight_name_suffix            : pp03[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:33:20,946] [   DEBUG][0m - [0m
[32m[2024-12-31 16:33:20,946] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:33:20,948] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:33:20,948] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 3
[2024-12-31 16:33:20,948] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:33:20,948] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:33:20) [0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:33:20,949] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:33:20,950] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 634, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:37:07,389] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:37:07,390] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:37:07,390] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:37:07,390] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:37:07,390] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I1231 16:37:07.391381 138020 tcp_utils.cc:107] Retry to connect to 10.3.242.26:40133 while the server is not yet listening.
I1231 16:37:10.391647 138020 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40133
I1231 16:37:10.411573 138020 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.411609 138020 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:10.412523 138020 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.412556 138020 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:10,412] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:37:10.421936 138020 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:37:10.422894 138020 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-12-31 16:37:12,987] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:37:12,988] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:37:12,988] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:37:12.989209 138020 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.989260 138020 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:12.989367 138020 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.989382 138020 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:12,989] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [0, 1, 2, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:37:12,990] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:37:12,992] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:13,002] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:13,002] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:37:13,002] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:37:13,002] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:37:13,002] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:37:13,003] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:13,004] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:13,005] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:37:13,005] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:13,005] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:13,005] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:37:13,005] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:37:13,006] [   DEBUG][0m - [0m
[32m[2024-12-31 16:37:13,010] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[33m[2024-12-31 16:37:13,011] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:37:13,014] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:37:13,020] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:37:13,021] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:37:13,022] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:38:19,329] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:38:19,334] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:631 - stage=0, global_rank=3 ,layer_number=9
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:631 - stage=1, global_rank=3 ,layer_number=8
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:631 - stage=2, global_rank=3 ,layer_number=8
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:631 - stage=3, global_rank=3 ,layer_number=10
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,335] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:38:19,336] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:38:19,416] [    INFO] pp_layers.py:708 - flush 10 of layers into run_function
[33m[2024-12-31 16:38:39,189] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:38:39,190] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:38:40,541] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:38:40,546] [   DEBUG][0m - Frozen parameters: 1.75e+09 || Trainable parameters:5.00e+06 || Total parameters:1.76e+09|| Trainable:0.28%[0m
[35m[2024-12-31 16:38:40,547] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:38:41,291] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[32m[2024-12-31 16:38:41,431] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-37-07_ubuntu[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:38:41,461] [   DEBUG][0m - optimizer_name_suffix         : pp03[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - pipeline_parallel_rank        : 3[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:38:41,462] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:38:41,463] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:38:41,464] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - weight_name_suffix            : pp03[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:38:41,465] [   DEBUG][0m - [0m
[32m[2024-12-31 16:38:41,466] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 3
[2024-12-31 16:38:41,474] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:38:41,474] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:38:41) [0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:38:41,476] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:38:41,477] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 634, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:41:54,575] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:41:54,575] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:41:54,575] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:41:54,575] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:41:54,575] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I1231 16:41:54.577220 141494 tcp_utils.cc:130] Successfully connected to 10.3.242.26:47469
I1231 16:41:54.603307 141494 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.603327 141494 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:54.603659 141494 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.603672 141494 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:54,603] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:41:54.605443 141494 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:41:54.606374 141494 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
[2024-12-31 16:41:57,562] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:41:57,563] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:41:57,563] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:41:57.563670 141494 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.563702 141494 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:57.563820 141494 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.563836 141494 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:57,564] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [3],  sharding_group: [3], pp_group: [0, 1, 2, 3], dp_group: [3], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:41:57,564] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:41:57,566] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - [0m
[32m[2024-12-31 16:41:57,584] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[33m[2024-12-31 16:41:57,584] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:41:57,586] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:41:57,591] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:41:57,593] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:41:57,593] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:43:06,318] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:631 - stage=0, global_rank=3 ,layer_number=9
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:631 - stage=1, global_rank=3 ,layer_number=8
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:631 - stage=2, global_rank=3 ,layer_number=8
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,323] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:631 - stage=3, global_rank=3 ,layer_number=10
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:43:06,324] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:43:06,402] [    INFO] pp_layers.py:708 - flush 10 of layers into run_function
[33m[2024-12-31 16:43:27,606] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:43:27,607] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:43:28,963] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:43:28,968] [   DEBUG][0m - Frozen parameters: 1.75e+09 || Trainable parameters:5.00e+06 || Total parameters:1.76e+09|| Trainable:0.28%[0m
[35m[2024-12-31 16:43:28,968] [   DEBUG][0m - 3: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:43:30,723] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 342.[0m
[32m[2024-12-31 16:43:30,792] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:43:30,810] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - current_device                : gpu:3[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:43:30,811] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,812] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - local_process_index           : 3[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - local_rank                    : 3[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-41-54_ubuntu[0m
[35m[2024-12-31 16:43:30,813] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - logical_process_index         : 3[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - optimizer_name_suffix         : pp03[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,814] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - pipeline_parallel_rank        : 3[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - process_index                 : 3[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,815] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:43:30,816] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - weight_name_suffix            : pp03[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:43:30,817] [   DEBUG][0m - [0m
[32m[2024-12-31 16:43:30,818] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:43:30,898] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:43:30,898] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 3
[2024-12-31 16:43:30,898] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:43:30,898] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:43:30) [0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:43:30,900] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:43:30,902] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
W1231 16:43:35.012359 141494 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
