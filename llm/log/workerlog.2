/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:14:19,991] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I1230 11:14:19.992388 4019290 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48565
I1230 11:14:20.027856 4019290 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:14:20.027920 4019290 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:14:20,028] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:14:20.034701 4019290 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:14:20.038501 4019290 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:19:32,056] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1230 11:19:32.058324 4022990 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38255
I1230 11:19:32.143441 4022990 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:32.143473 4022990 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:32,143] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:19:32.165792 4022990 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:19:32.167093 4022990 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-30 11:19:36,452] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:19:36,452] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:19:36.453073 4022990 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:36.453091 4022990 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:36,453] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:19:36.453171 4022990 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:36.453176 4022990 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:36,453] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:19:36,453] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:19:36,453] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:19:36,454] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:19:36,455] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:19:36,456] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:19:36,457] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:19:36,458] [   DEBUG][0m - [0m
[32m[2024-12-30 11:19:36,459] [    INFO][0m - The global seed is set to 44, local seed is set to 52 and random seed is set to 42.[0m
[33m[2024-12-30 11:19:36,459] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
(â€¦)munity/meta-llama/Llama-2-7b/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s](â€¦)munity/meta-llama/Llama-2-7b/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 579/579 [00:00<00:00, 1.65MB/s]
[32m[2024-12-30 11:19:36,658] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:19:36,662] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:19:36,663] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
(â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 0.00/13.5G [00:00<?, ?B/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 10.5M/13.5G [00:04<1:25:52, 2.61MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 21.0M/13.5G [00:06<1:03:58, 3.51MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 31.5M/13.5G [00:08<54:36, 4.10MB/s]  (â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 41.9M/13.5G [00:09<45:33, 4.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 52.4M/13.5G [00:11<44:58, 4.97MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   0%|          | 62.9M/13.5G [00:13<40:47, 5.48MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 73.4M/13.5G [00:15<42:14, 5.29MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 83.9M/13.5G [00:17<39:09, 5.70MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 94.4M/13.5G [00:18<37:44, 5.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 105M/13.5G [00:20<40:27, 5.51MB/s] (â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 115M/13.5G [00:22<38:10, 5.83MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 126M/13.5G [00:23<34:47, 6.40MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 136M/13.5G [00:25<33:27, 6.65MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 147M/13.5G [00:26<31:12, 7.12MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 157M/13.5G [00:27<30:46, 7.21MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|          | 168M/13.5G [00:29<29:18, 7.57MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|â–         | 178M/13.5G [00:31<33:57, 6.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|â–         | 189M/13.5G [00:32<33:35, 6.59MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   1%|â–         | 199M/13.5G [00:34<33:55, 6.52MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 210M/13.5G [00:35<32:09, 6.87MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 220M/13.5G [00:36<29:43, 7.43MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 231M/13.5G [00:38<29:07, 7.58MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 241M/13.5G [00:39<29:53, 7.38MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 252M/13.5G [00:41<34:14, 6.44MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 262M/13.5G [00:43<36:21, 6.06MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 273M/13.5G [00:46<40:21, 5.45MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 283M/13.5G [00:48<42:19, 5.20MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 294M/13.5G [00:50<41:00, 5.36MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 304M/13.5G [00:51<37:22, 5.87MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 315M/13.5G [00:52<33:32, 6.54MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 325M/13.5G [00:54<34:48, 6.30MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   2%|â–         | 336M/13.5G [00:55<33:17, 6.58MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 346M/13.5G [00:57<32:57, 6.64MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 357M/13.5G [00:59<33:44, 6.48MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 367M/13.5G [01:00<32:05, 6.81MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 377M/13.5G [01:02<34:52, 6.26MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 388M/13.5G [01:04<33:38, 6.48MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 398M/13.5G [01:05<31:13, 6.98MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 409M/13.5G [01:07<33:28, 6.51MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 419M/13.5G [01:09<35:02, 6.21MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 430M/13.5G [01:10<36:05, 6.02MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 440M/13.5G [01:12<34:10, 6.36MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 451M/13.5G [01:13<31:24, 6.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   3%|â–Ž         | 461M/13.5G [01:14<30:31, 7.10MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–Ž         | 472M/13.5G [01:16<30:59, 6.99MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–Ž         | 482M/13.5G [01:18<34:59, 6.19MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–Ž         | 493M/13.5G [01:20<33:58, 6.37MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–Ž         | 503M/13.5G [01:21<31:32, 6.85MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 514M/13.5G [01:22<28:51, 7.49MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 524M/13.5G [01:23<28:06, 7.68MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 535M/13.5G [01:24<26:36, 8.11MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 545M/13.5G [01:26<29:39, 7.27MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 556M/13.5G [01:28<29:24, 7.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 566M/13.5G [01:29<27:51, 7.72MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 577M/13.5G [01:30<25:56, 8.29MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 587M/13.5G [01:32<29:20, 7.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   4%|â–         | 598M/13.5G [01:33<31:47, 6.75MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 608M/13.5G [01:35<30:51, 6.95MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 619M/13.5G [01:37<33:55, 6.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 629M/13.5G [01:38<32:57, 6.50MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 640M/13.5G [01:40<30:36, 6.99MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 650M/13.5G [01:41<27:59, 7.64MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 661M/13.5G [01:42<27:23, 7.80MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–         | 671M/13.5G [01:44<30:38, 6.96MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 682M/13.5G [01:46<32:48, 6.50MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 692M/13.5G [01:47<31:41, 6.72MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 703M/13.5G [01:48<29:30, 7.22MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 713M/13.5G [01:49<27:02, 7.86MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 724M/13.5G [01:50<24:42, 8.60MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   5%|â–Œ         | 734M/13.5G [01:52<25:33, 8.31MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 744M/13.5G [01:54<28:59, 7.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 755M/13.5G [01:55<31:25, 6.75MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 765M/13.5G [01:57<33:15, 6.37MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 776M/13.5G [01:59<31:58, 6.62MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 786M/13.5G [02:01<34:42, 6.09MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 797M/13.5G [02:03<36:04, 5.86MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 807M/13.5G [02:05<36:37, 5.77MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 818M/13.5G [02:06<34:18, 6.15MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 828M/13.5G [02:07<31:13, 6.75MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–Œ         | 839M/13.5G [02:09<32:54, 6.40MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–‹         | 849M/13.5G [02:11<31:39, 6.65MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–‹         | 860M/13.5G [02:12<29:23, 7.15MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   6%|â–‹         | 870M/13.5G [02:14<31:34, 6.65MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 881M/13.5G [02:15<30:41, 6.84MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 891M/13.5G [02:16<28:41, 7.31MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 902M/13.5G [02:18<33:40, 6.22MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 912M/13.5G [02:20<33:11, 6.31MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 923M/13.5G [02:21<31:02, 6.74MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 933M/13.5G [02:23<33:16, 6.28MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 944M/13.5G [02:25<34:41, 6.02MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 954M/13.5G [02:27<32:50, 6.36MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 965M/13.5G [02:28<30:09, 6.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 975M/13.5G [02:30<32:05, 6.49MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 986M/13.5G [02:32<33:31, 6.21MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 996M/13.5G [02:34<37:31, 5.54MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   7%|â–‹         | 1.01G/13.5G [02:36<35:48, 5.80MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.02G/13.5G [02:37<35:02, 5.93MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.03G/13.5G [02:39<35:06, 5.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.04G/13.5G [02:40<32:44, 6.33MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.05G/13.5G [02:42<29:53, 6.93MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.06G/13.5G [02:43<31:39, 6.54MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.07G/13.5G [02:46<36:04, 5.73MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.08G/13.5G [02:48<37:23, 5.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.09G/13.5G [02:50<37:34, 5.49MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.10G/13.5G [02:51<34:53, 5.91MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.11G/13.5G [02:53<33:46, 6.10MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.12G/13.5G [02:54<31:19, 6.57MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.13G/13.5G [02:55<28:33, 7.20MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   8%|â–Š         | 1.14G/13.5G [02:57<27:39, 7.43MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–Š         | 1.15G/13.5G [02:58<26:00, 7.90MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–Š         | 1.16G/13.5G [02:59<24:14, 8.47MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–Š         | 1.17G/13.5G [03:00<24:10, 8.48MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.18G/13.5G [03:02<27:47, 7.37MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.20G/13.5G [03:03<27:48, 7.36MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.21G/13.5G [03:05<28:35, 7.15MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.22G/13.5G [03:07<30:06, 6.79MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.23G/13.5G [03:08<31:19, 6.52MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.24G/13.5G [03:10<29:58, 6.81MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.25G/13.5G [03:11<27:49, 7.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.26G/13.5G [03:13<30:01, 6.78MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.27G/13.5G [03:14<29:12, 6.97MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:   9%|â–‰         | 1.28G/13.5G [03:15<27:18, 7.44MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.29G/13.5G [03:16<25:14, 8.05MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.30G/13.5G [03:18<27:28, 7.39MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.31G/13.5G [03:20<32:20, 6.27MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.32G/13.5G [03:22<31:41, 6.39MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.33G/13.5G [03:23<29:35, 6.84MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–‰         | 1.34G/13.5G [03:25<28:59, 6.98MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.35G/13.5G [03:26<27:24, 7.37MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.36G/13.5G [03:27<27:18, 7.39MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.37G/13.5G [03:28<26:11, 7.70MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.38G/13.5G [03:30<26:26, 7.62MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.39G/13.5G [03:31<25:37, 7.86MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  10%|â–ˆ         | 1.41G/13.5G [03:33<28:40, 7.01MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.42G/13.5G [03:35<33:29, 6.00MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.43G/13.5G [03:37<32:32, 6.17MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.44G/13.5G [03:38<30:07, 6.66MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.45G/13.5G [03:40<34:38, 5.79MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.46G/13.5G [03:42<33:36, 5.96MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.47G/13.5G [03:43<30:59, 6.46MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.48G/13.5G [03:45<30:01, 6.66MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.49G/13.5G [03:47<32:49, 6.09MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.50G/13.5G [03:49<34:09, 5.84MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆ         | 1.51G/13.5G [03:50<32:20, 6.17MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆâ–        | 1.52G/13.5G [03:52<34:20, 5.80MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆâ–        | 1.53G/13.5G [03:54<35:10, 5.66MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  11%|â–ˆâ–        | 1.54G/13.5G [03:56<35:27, 5.61MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.55G/13.5G [03:58<35:17, 5.63MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.56G/13.5G [04:00<32:49, 6.05MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.57G/13.5G [04:01<29:47, 6.66MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.58G/13.5G [04:03<31:15, 6.34MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.59G/13.5G [04:04<32:22, 6.12MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.60G/13.5G [04:06<33:09, 5.97MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.61G/13.5G [04:08<31:17, 6.32MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.63G/13.5G [04:09<28:40, 6.89MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.64G/13.5G [04:10<27:51, 7.08MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.65G/13.5G [04:12<28:17, 6.97MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.66G/13.5G [04:13<27:00, 7.30MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.67G/13.5G [04:14<25:06, 7.84MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  12%|â–ˆâ–        | 1.68G/13.5G [04:16<27:18, 7.20MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.69G/13.5G [04:18<27:54, 7.04MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.70G/13.5G [04:20<30:14, 6.49MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.71G/13.5G [04:21<29:15, 6.70MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.72G/13.5G [04:23<31:59, 6.12MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.73G/13.5G [04:25<33:13, 5.89MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.74G/13.5G [04:26<31:23, 6.23MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.75G/13.5G [04:28<30:48, 6.34MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.76G/13.5G [04:30<33:40, 5.80MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.77G/13.5G [04:32<32:13, 6.05MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.78G/13.5G [04:34<34:19, 5.68MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.79G/13.5G [04:35<32:34, 5.98MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.80G/13.5G [04:37<31:54, 6.10MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  13%|â–ˆâ–Ž        | 1.81G/13.5G [04:38<29:44, 6.54MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–Ž        | 1.82G/13.5G [04:39<27:05, 7.17MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–Ž        | 1.84G/13.5G [04:40<24:32, 7.90MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–Ž        | 1.85G/13.5G [04:42<26:14, 7.39MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.86G/13.5G [04:44<27:51, 6.95MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.87G/13.5G [04:46<29:30, 6.56MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.88G/13.5G [04:47<30:39, 6.31MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.89G/13.5G [04:49<29:13, 6.61MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.90G/13.5G [04:50<28:55, 6.67MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.91G/13.5G [04:53<31:54, 6.04MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.92G/13.5G [04:54<30:47, 6.26MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.93G/13.5G [04:55<28:28, 6.76MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.94G/13.5G [04:56<25:52, 7.43MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  14%|â–ˆâ–        | 1.95G/13.5G [04:58<27:45, 6.92MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 1.96G/13.5G [05:00<29:22, 6.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 1.97G/13.5G [05:01<28:17, 6.78MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 1.98G/13.5G [05:03<28:12, 6.79MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 1.99G/13.5G [05:05<29:05, 6.58MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 2.00G/13.5G [05:06<29:58, 6.38MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–        | 2.01G/13.5G [05:08<30:52, 6.19MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.02G/13.5G [05:10<29:16, 6.52MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.03G/13.5G [05:12<31:28, 6.06MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.04G/13.5G [05:13<30:23, 6.27MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.06G/13.5G [05:15<30:01, 6.34MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.07G/13.5G [05:16<28:07, 6.76MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.08G/13.5G [05:17<25:48, 7.36MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  15%|â–ˆâ–Œ        | 2.09G/13.5G [05:19<25:08, 7.55MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.10G/13.5G [05:20<23:44, 7.99MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.11G/13.5G [05:22<29:00, 6.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.12G/13.5G [05:24<28:59, 6.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.13G/13.5G [05:25<29:23, 6.43MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.14G/13.5G [05:27<27:54, 6.77MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.15G/13.5G [05:28<25:45, 7.33MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.16G/13.5G [05:29<23:33, 8.01MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.17G/13.5G [05:31<27:44, 6.79MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–Œ        | 2.18G/13.5G [05:32<27:47, 6.78MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–‹        | 2.19G/13.5G [05:34<28:21, 6.63MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–‹        | 2.20G/13.5G [05:36<29:17, 6.41MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–‹        | 2.21G/13.5G [05:38<30:04, 6.24MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  16%|â–ˆâ–‹        | 2.22G/13.5G [05:39<30:52, 6.08MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.23G/13.5G [05:42<33:53, 5.53MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.24G/13.5G [05:44<34:36, 5.41MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.25G/13.5G [05:45<32:16, 5.80MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.26G/13.5G [05:47<29:09, 6.41MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.28G/13.5G [05:48<27:55, 6.69MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.29G/13.5G [05:49<25:58, 7.18MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.30G/13.5G [05:50<23:54, 7.79MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.31G/13.5G [05:51<23:25, 7.95MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.32G/13.5G [05:53<26:26, 7.03MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.33G/13.5G [05:55<26:14, 7.08MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.34G/13.5G [05:56<24:45, 7.50MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  17%|â–ˆâ–‹        | 2.35G/13.5G [05:57<22:55, 8.09MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.36G/13.5G [05:58<22:34, 8.21MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.37G/13.5G [05:59<21:30, 8.60MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.38G/13.5G [06:02<26:38, 6.94MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.39G/13.5G [06:03<27:01, 6.84MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.40G/13.5G [06:04<25:40, 7.19MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.41G/13.5G [06:06<25:31, 7.22MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.42G/13.5G [06:07<24:20, 7.57MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.43G/13.5G [06:09<26:57, 6.83MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.44G/13.5G [06:11<28:46, 6.39MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.45G/13.5G [06:13<29:55, 6.14MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.46G/13.5G [06:14<28:26, 6.45MB/s](â€¦)ta-llama/Llama-2-7b/model_state.pdparams:  18%|â–ˆâ–Š        | 2.47G/13.5G [06:16<28:04, 6.53MB/s]/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:26:13,432] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1230 11:26:13.433704 4028346 tcp_utils.cc:107] Retry to connect to 10.3.242.26:38214 while the server is not yet listening.
I1230 11:26:16.433890 4028346 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38214
I1230 11:26:16.434104 4028346 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:16.434114 4028346 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:16,434] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:26:16.436040 4028346 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:26:16.436762 4028346 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-30 11:26:20,203] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:26:20,203] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:26:20.203532 4028346 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:20.203553 4028346 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:20,203] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:26:20.203630 4028346 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:20.203634 4028346 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:20,203] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:26:20,203] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:26:20,204] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:26:20,204] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:26:20,205] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:26:20,206] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:26:20,207] [   DEBUG][0m - [0m
[32m[2024-12-30 11:26:20,208] [    INFO][0m - The global seed is set to 44, local seed is set to 52 and random seed is set to 42.[0m
[33m[2024-12-30 11:26:20,209] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
[32m[2024-12-30 11:26:20,209] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:26:20,211] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:26:20,211] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:51:45,531] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I1231 15:51:45.533746 82307 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38873
I1231 15:51:45.534220 82307 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:45.534229 82307 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:45,534] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:51:45.537387 82307 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:51:45.538077 82307 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-31 15:51:47,457] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:51:47,457] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:51:47.457669 82307 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.457680 82307 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,457] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:51:47.457744 82307 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.457748 82307 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,457] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:51:47,457] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:51:47,458] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:51:47,458] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:51:47,459] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:51:47,460] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:51:47,461] [   DEBUG][0m - [0m
[32m[2024-12-31 15:51:47,462] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[33m[2024-12-31 15:51:47,462] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:51:47,462] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:51:47,464] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:51:47,464] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:51:47,465] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:53:03,407] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 15:54:02,803] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 15:54:02,804] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 15:54:02,807] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 341 examples [00:00, 1719.04 examples/s]Generating train split: 666 examples [00:00, 2219.74 examples/s]Generating train split: 1019 examples [00:00, 2513.18 examples/s]Generating train split: 1351 examples [00:00, 2565.60 examples/s]Generating train split: 1510 examples [00:00, 2476.90 examples/s]
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 197, in load_dataset
    reader_cls = import_main_class(path_or_read_func)
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 95, in import_main_class
    module = importlib.import_module(module_path)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'paddlenlp.datasets.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 119, in load_from_hf
    hf_datasets = load_hf_dataset(path, name=name, split=splits, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 56, in load_from_ppnlp
    return origin_load_dataset(path, trust_remote_code=True, *args, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/load.py", line 2132, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/load.py", line 1853, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/load.py", line 1562, in dataset_module_factory
    ).get_module()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/load.py", line 942, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/sjx/tr-paddle-1230/train_data/dev.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 677, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 266, in main
    train_ds, dev_ds, test_ds = create_dataset(data_args, training_args)
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 628, in create_dataset
    dev_ds = load_dataset(
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 199, in load_dataset
    datasets = load_from_hf(
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 121, in load_from_hf
    raise FileNotFoundError("Couldn't find the dataset script for '" + path + "' on PaddleNLP or HuggingFace")
FileNotFoundError: Couldn't find the dataset script for 'json' on PaddleNLP or HuggingFace
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:56:39,053] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I1231 15:56:39.054256 85668 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48936
I1231 15:56:39.095649 85668 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:39.095707 85668 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:39,096] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:56:39.108572 85668 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:56:39.109438 85668 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-31 15:56:41,130] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:56:41,130] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:56:41.130931 85668 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:41.130947 85668 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:41,130] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:56:41.131016 85668 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:41.131021 85668 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:41,131] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:56:41,131] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:56:41,131] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:56:41,131] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:56:41,132] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:56:41,133] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:56:41,134] [   DEBUG][0m - [0m
[32m[2024-12-31 15:56:41,135] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[33m[2024-12-31 15:56:41,135] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:56:41,136] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:56:41,137] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:56:41,137] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:56:41,138] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:57:40,805] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 15:58:43,504] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 15:58:43,505] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 15:58:43,508] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 15:58:45,088] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 15:58:45,149] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 15:58:45,150] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 15:58:52,435] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-12-31 15:58:52,549] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 15:58:52,638] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 15:58:52,639] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - dataset_rank                  : 2[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 15:58:52,640] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 15:58:52,641] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_15-56-39_ubuntu[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 15:58:52,642] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - optimizer_name_suffix         : shard02[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 15:58:52,643] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 15:58:52,644] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - sharding_parallel_rank        : 2[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 15:58:52,645] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 15:58:52,646] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 15:58:52,647] [   DEBUG][0m - [0m
[32m[2024-12-31 15:58:52,648] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 15:58:52,678] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 15:58:53,448] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 15:58:53,448] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 15:58:53,451] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 15:58:54,058] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 15:58:54) [0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 15:58:54,063] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 78, in tokenize_unsupervised_example
    raise DataFormatError(
utils.data.DataFormatError: Example format is wrong, please check: {'text': '\\section{Introduction} \nThis paper focuses on an {\\em arithmetic average Asian option} in continuous time having a terminal payoff of the form\n\\begin{equation}\n    \\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\,.\n\\end{equation}\nHere, the function $\\Phi:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a prescribed payoff function, $T$ is a constant that denotes the maturity, and $(S_t)_{t\\ge 0}$ is an underlying price process. For conciseness, we refer to this option as the {\\em Asian option}. Because of its average property, the Asian option is less exposed to a sudden plummet in stock prices just before maturity. In particular, for hedging purposes, the Asian option is attractive to many traders and financial institutions. For an overview of the role of the Asian option in the financial market, see \\cite{WilmottPaul2006PWoq}.\n\n\nDespite its popularity in the real market, the Asian option is mathematically challenging to price and hedge in general. Even when the underlying stock price $(S_t)_{t\\ge 0}$ follows the classical Black--Scholes model, no simple closed-form formula for the density of the random variable $\\frac{1}{T}\\int_0^T S_t\\,dt$ is known. In this paper, we analyze the Asian option for pricing and hedging purposes in the short-maturity regime.\n\n\nWe focus on the case in which the payoff function $\\Phi$ is any H\\"older continuous function and the process $(S_t)_{t\\ge 0}$ follows a local volatility model. Detailed assumptions on the model are presented in Section \\ref{sec:Approximation scheme}. This paper primarily deals with two features of the Asian option. The first feature is the short-maturity behavior of the option price. The short-maturity Asian option price is shown to be determined by the {\\em Asian volatility}, which is defined by\n\\begin{equation}\n    \\sigma_A(T):=\\sqrt{\\frac{1}{T^3}\\int_0^T \\sigma^2(t,S_0)(T-t)^2\\,dt}\\,,\n\\end{equation}\nwhere $\\sigma(\\cdot,\\cdot)$ is a local volatility function. The second feature is the initial-value sensitivity of the Asian option. This type of sensitivity is widely referred to as {\\em delta} in the finance literature. In the modern theory of finance, the delta value is used to hedge financial derivatives. This paper shows that the delta value can be expressed in terms of the Asian volatility for small $T$, as with the option price. In summary, the Asian option price $P_A(T)$ and delta value $\\Delta_A(T)$ are expressed as\n\\begin{align}\n    &P_A(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)]+\\mathcal{O}(T^{\\gamma})\\,, \\\\ &\\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,,\n\\end{align}\nfor a standard normal random variable $Z$ and the H\\"older exponent $\\gamma$ of the payoff function $\\Phi\\,.$ The asymptotic estimates established in this paper are particularly meaningful with regard to the non-linear payoff function $\\Phi\\,.$ For example, let us consider a payoff function $\\Phi$ that equals $x\\mapsto(x-K)_{+}^{\\gamma}$ in some neighborhood of $K$ for some $\\frac{1}{2}<\\gamma<2\\,.$ If $K$ equals the initial value $S_0\\,,$ we prove that the leading order of delta is $T^{\\frac{\\gamma-1}{2}}$ and we provide its coefficient in a rigorous manner.\n\n\n\n\nAs a special case, estimates for the Asian call and put option delta values are enhanced. This paper supplements the asymptotic result in \\cite{PirjolDan2016SMAO} in two ways. First, we prove that the rate function of the out-of-the-money (OTM) Asian option delta value is the same as that of the OTM Asian option price. Second, a precise Taylor expansion of the in-the-money (ITM) Asian option delta value is provided.\n\n\n\nEstimates for the price and delta of the {\\em European option} having the terminal payoff $\\Phi(S_T)$ are also investigated. Short-maturity formulas for the European option prices and delta values are obtained if the Asian volatility is replaced by\n\\begin{equation}\n    \\sigma_E(T):=\\sqrt{\\frac{1}{T}\\int_0^T \\sigma^2(t,S_0)\\,dt}\\,,\n\\end{equation}\nwhich we refer to as the {\\em European volatility}, in the formulas for the Asian option prices and delta values. With regard to $\\sigma_A(T)$ and $\\sigma_E(T)\\,,$ we compare the Asian option with the European option in Section \\ref{sec:Comparison between volatilities at short maturity}. In addition to the European option, the {\\em geometric average Asian option} having the terminal payoff\n\\begin{equation}\n    \\Phi\\left(e^{\\frac{1}{T}\\int_0^T \\log S_t\\,dt}\\right)\n\\end{equation}\nis also compared in the Black--Scholes model.\n\n\n\n\nTo obtain these estimates, we incorporate many well-known mathematical techniques with the approximation scheme. The main technique is $L^p$-approximation of the underlying stock price $(S_t)_{0\\le t\\le T}$ by some Gaussian process $(\\hat{X}_t)_{0\\le t\\le T}.$ Precise arguments are presented in Section \\ref{sec:Approximation scheme}. We adopt the method used in \\cite{PirjolDan2016SMAO, PirjolDan2019SMAO,PirjolDan2019SMFS}, where the same idea was used to compute the short-maturity asymptotics of at-the-money(ATM) Asian call and put option prices. On the basis of this idea, our research focus shifts from the random variable $\\frac{1}{T}\\int_0^T S_t\\,dt\\,$ having a sophisticated density to the Gaussian random variable $\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\,.$ This is the key strategy that we adopt to approximate the Asian option throughout in Sections \\ref{sec:Approximation scheme}--\\ref{sec:Short maturity asymptotic for a sensitivity}. In addition, we use Malliavin calculus theory to analyze the Asian option delta value. In \\cite{benhamou2000application,PirjolDan2018SOAO}, the authors used Malliavin calculus for their sensitivity analysis of the Asian call and put option. We use their methods to express the Asian option delta value. Furthermore, we use the large deviation principle to examine both OTM and ITM Asian call and put options. The large deviation principle was first used to investigate the short-maturity Asian option in \\cite{PirjolDan2016SMAO,PirjolDan2019SMAO}. \n\n\n\n\nOur study is of practical interest because existing numerical methods have proven to be less efficient in the case of short maturity or low volatility. Numerical analysis of the Asian option was conducted in \\cite{Geman,2004SEfA,BroadieMark1999Cdac,BoylePhelim2008Paso}. However, as pointed out in \\cite{fu1999pricing,vecer2002unified}, such methods are either problematic in the short-maturity regime or computationally expensive. We expect our analysis to help overcome the numerical inefficiency in the short-maturity regime.\n\n\n\n\nRecently, the short-maturity Asian option has been studied by many researchers. Under a local volatility model, the asymptotics of Asian option price have been investigated in \\cite{PirjolDan2016SMAO,PirjolDan2019SMFS}. In \\cite{PirjolDan2019SMAO}, asymptotic analysis was conducted under the constant elasticity of the variance model. The above-mentioned studies have used the large deviation principle. They have analytically solved the rate function of the law of $\\frac{1}{T}\\int_0^T S_t\\,dt$ for approximation. Sensitivity analysis was conducted in \\cite{PirjolDan2018SOAO} as a follow-up study under the Black--Scholes model. On the basis of the approximated option price established in \\cite{PirjolDan2016SMAO}, the sensitivities have been examined in \\cite{PirjolDan2018SOAO}.\n\n\n\nCompared to the above-mentioned studies, the contributions of our study are threefold. First, our paper focuses on a model having a time-dependent diffusion term. The analysis performed in \\cite{PirjolDan2016SMAO,PirjolDan2019SMFS} was based on the assumption that the diffusion is time-independent. The obtained rate function was strongly dependent on this time-independent assumption. Second, we provide the leading order and its exact coefficient for an arbitrary H\\"older continuous payoff function $\\Phi\\,.$ This generalizes the results in \\cite{PirjolDan2016SMAO,PirjolDan2019SMAO}, where vanilla options(call and put) were mainly considered. Finally, in contrast to \\cite{PirjolDan2018SOAO}, our estimates for delta do not build upon the approximated option price. Thus, our estimates are free from controlling nested errors.\n\n\n\nMalliavin calculus theory has long been applied to the Asian option. The necessary and sufficient conditions on the Malliavin weights of sensitivities have been studied in \\cite{benhamou2000application}. The Malliavin weights under the Black--Scholes model have been computed in \\cite{BoylePhelim2008Paso,grasselli2005malliavin}. Relevant theories have been presented in \\cite{NualartDavid1995TMca,nualart2018introduction}. Furthermore, under a general diffusion process, the prices of continuously sampled Asian options have been estimated using Malliavin calculus in \\cite{GobetEmmanuel2014Waoa}. In \\cite{ShirayaKenichiro2011Pbaa}, price bounds for discretely sampled options have been presented for a stochastic volatility model.\n\n \n\n\nThe remainder of this paper is organized as follows. Section \\ref{sec:Approximation scheme} outlines the model setup and introduces six auxiliary processes that are used to approximate $(S_t)_{t\\ge 0}$ in the $L^p(\\mathbb{Q})$ norm. Section \\ref{sec:Short maturity limit of an option price} examines the Asian option price for small $T$ when the payoff $\\Phi$ is Lipschitz continuous. Under the same assumption on $\\Phi\\,,$ Section \\ref{sec:Short maturity asymptotic for a sensitivity} investigates the Asian option delta value. Section \\ref{sec:Holder continuous} generalizes the results from Sections \\ref{sec:Short maturity limit of an option price} and \\ref{sec:Short maturity asymptotic for a sensitivity} to the H\\"older continuous payoff $\\Phi\\,.$ Section \\ref{sec:Comparison between volatilities at short maturity} concatenates the asymptotic results from Sections \\ref{sec:Short maturity limit of an option price} and \\ref{sec:Short maturity asymptotic for a sensitivity}, and compares them with their European counterparts. Section \\ref{sec:Special case} uses the large deviation principle to study the Asian call and put option. Finally, Section \\ref{sec:conclusion} concludes the paper.\n\n\n\n\\section{Approximation scheme}\n\\label{sec:Approximation scheme}\n\n\nWe analyze the short-maturity asymptotic behavior of Asian options under local volatility\nmodels. Assume that  the stock price process $(S_t)_{t\\ge0}$ follows a local volatility model,\n\\begin{align}\\label{eq:S_t}\ndS_t=(r-q)S_t\\,dt+\\sigma(t,S_t)S_t\\,dW_t\\,, \\quad S_0>0\\,,\n\\end{align}\nunder risk-neutral measure $\\mathbb{Q}$, where $r$ is the short rate, $q$ is the dividend rate, and $(W_t)_{t\\ge0}$ is a $\\mathbb{Q}$-Brownian motion. From Assumption \\ref{classical assumption} below, there exists a unique strong solution for this stochastic differential equation.\n\n\\begin{assume} \\label{classical assumption}\n\tLet us consider the following assumptions for the diffusion function.\n\t\\begin{enumerate}[label=(\\roman*)]\n\t\t\\item The function $\\sigma(t,x)$ is measurable in $[0,\\infty)\\times\\mathbb{R}$ and is bounded, i.e., there are two constants $\\underline{\\sigma}$ and $\\overline{\\sigma}$ such that $0<\\underline{\\sigma}\\le\\sigma(t,x) \\le\\overline{\\sigma}<\\infty$ for all $t$ and $x.$\n\t\t\\item For each $t\\,,$ the function $\\sigma(t,\\cdot)$ is twice differentiable in $\\mathbb{R}\\,.$\n\t\t\\item Define $\\nu(t,x):=\\frac{\\partial}{\\partial x}\\sigma(t,x)x$ and $\\rho(t,x):=\\frac{\\partial^2}{\\partial x^2}\\sigma(t,x)x\\,.$ Then, for each $t,$ functions $\\sigma(t,\\cdot),\\sigma(t,\\cdot)\\cdot,\\nu(t,\\cdot),\\rho(t,\\cdot)$ are Lipschitz continuous with a Lipschitz coefficient $\\alpha>0$. More precisely, there is a constant $\\alpha>0$ such that for any $x,y\\in\\mathbb{R},$\n\t\t\\begin{align}\n\t\t&\\underset{t\\ge 0}{\\sup}\\left\\vert \\sigma(t,x)-\\sigma(t,y) \\right\\vert \\le \\alpha \\left\\vert x-y \\right\\vert, \\ \\underset{t\\ge 0}{\\sup}\\left\\vert \\sigma(t,x)x-\\sigma(t,y)y \\right\\vert \\le \\alpha \\left\\vert x-y \\right\\vert, \\\\\n\t\t&\\underset{t\\ge 0}{\\sup}\\left\\vert \\nu(t,x)-\\nu(t,y) \\right\\vert \\le \\alpha \\left\\vert x-y \\right\\vert, \\ \\underset{t\\ge 0}{\\sup}\\left\\vert \\rho(t,x)-\\rho(t,y) \\right\\vert \\le \\alpha \\left\\vert x-y \\right\\vert\\,.\n\t\t\\end{align}\n\t\\end{enumerate}\n\\end{assume}\n\\noindent Clearly, this assumption covers the Black--Scholes model. In this paper, only H\\"older continuous payoff $\\Phi$ will be considered. Under Assumption \\ref{Holder payoff assumption}, $\\beta$ and $\\gamma$ always refer to constants with regard to $\\Phi$ throughout this paper.  \n\\begin{assume}\\label{Holder payoff assumption}\n\tThe payoff function $\\Phi:\\mathbb{R}\\rightarrow{\\mathbb{R}}$ is $\\gamma$-H\\"older continuous with coefficient $\\beta>0$. More precisely, for any $x,y\\in\\mathbb{R}$, $\\vert\\Phi(x)-\\Phi(y)\\vert \\le \\beta\\vert x-y\\vert^{\\gamma}$ with $0<\\gamma\\le 1\\,.$\n\\end{assume}\n\\noindent To clarify the arguments, we will first consider Lipschitz continuous payoff $\\Phi$ in Sections \\ref{sec:Short maturity limit of an option price} and \\ref{sec:Short maturity asymptotic for a sensitivity}. Unless stated otherwise, $\\beta$ always refers to the Lipschitz coefficient.\n\\begin{assume}\\label{payoff assumption}\nThe payoff function $\\Phi:\\mathbb{R}\\rightarrow{\\mathbb{R}}$ is Lipschitz continuous with coefficient $\\beta>0$. More precisely, for any $x,y\\in\\mathbb{R}$, $\\vert\\Phi(x)-\\Phi(y)\\vert \\le \\beta\\vert x-y\\vert\\,.$\n\\end{assume}\n\n\n\n\n\nNow, we introduce six processes that are used to approximate $(S_t)_{t\\ge0}$ in $L^p(\\mathbb{Q})$:\n$$X,\\,Y,\\,\\tilde{X},\\,\\tilde{Y},\\,\\hat{X},\\,\\hat{Y}\\,.$$\nDefine a process $(X_t)_{t\\ge0}$ as\n\\begin{equation}\\label{eq:X_t}\ndX_t=\\sigma(t,X_t)X_t\\,dW_t\\,,\\quad X_0=S_0>0\\,\n\\end{equation}\nand its first variation process as \n\\begin{equation}\\label{eq:Y_t}\ndY_t=\\nu(t,X_t)Y_t\\,dW_t\\,,\\quad Y_0=1\\,.\n\\end{equation}\nThese two processes will be used to\napproximate the underlying process $(S_t)_{t\\ge0}$  in  Sections \\ref{sec:Short maturity limit of an option price} and \\ref{sec:Short maturity asymptotic for a sensitivity}.\nWe also define  two \ngeometric Gaussian processes $(\\tilde{X}_t)_{t\\ge0}$ and $(\\tilde{Y}_t)_{t\\ge0}$ as\n\\begin{equation}\\label{eq:tilde x,y}\n    d\\tilde{X}_t=\\sigma(t,S_0)\\tilde{X}_t\\,dW_t\\,,\\quad \\tilde{X}_0=S_0\\,, \\qquad\nd\\tilde{Y}_t=\\nu(t,S_0)\\tilde{Y}_t\\,dW_t\\,, \\quad \\tilde{Y}_0=1\\,.\n\\end{equation}\nIn Lemma \\ref{lem:x,y close}, these two processes will be used to\napproximate   $(X_t)_{t\\ge0}$ and $(Y_t)_{t\\ge0}$ in the $L^p(\\mathbb{Q})$ norm at short time.\nFinally, we define two \nGaussian processes $(\\hat{X}_t)_{t\\ge0}$, $(\\hat{Y}_t)_{t\\ge0}$  by\n\\begin{equation}\\label{eq:hat x,y}\n    d\\hat{X}_t=\\sigma(t,S_0)S_0\\,dW_t\\,,\\quad \\hat{X}_0=S_0\\,, \\qquad d\\hat{Y}_t=\\nu(t,S_0)\\,dW_t\\,, \\quad \\hat{Y}_0=1\\,.\n\\end{equation}\nFurthermore, in Lemma \\ref{lem:x,y close}, these two processes will be used to\napproximate   $(\\tilde{X}_t)_{t\\ge0}$ and $(\\tilde{Y}_t)_{t\\ge0}$ in the $L^p(\\mathbb{Q})$ norm at short time.\nIt is easy to check that the six above-mentioned processes are all \ncontinuous martingale processes adapted to the Brownian filtration $(\\mathcal{F}_t^W)_{t\\ge 0}.$ \nNow, we introduce Lemma \\ref{lem:x,y close}. See Appendix \\ref{proof:x,y close} for the proof.\n \n \n\\begin{lemma} \\label{lem:x,y close}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $B_p$ depending only on $p$ such that the following inequalities hold.\n\\begin{enumerate}[label=(\\roman*)]\n\\item For $0\\le t\\le1,$  \n\\begin{align} \\label{eq:x close}\n    \\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t\\vert^p] \\le B_pt^p, \\quad \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t\\vert^p] \\le B_pt^p. \n\\end{align}\n\\item  For $0\\le t\\le1,$  \n\\begin{align} \\label{eq:y close}\n    \\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t\\vert^p] \\le B_pt^p, \\quad \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{Y}_t-\\hat{Y}_t\\vert^p] \\le B_pt^p.\n\\end{align}\\\n\\end{enumerate}\n\\end{lemma}\n\n\n\n\nWe now present the short-time behavior of the four processes\n$(X_t)_{t\\ge0}$, $(\\tilde{X}_t)_{t\\ge0}$, $(Y_t)_{t\\ge0}$, $(\\tilde{Y}_t)_{t\\ge0}$ in the following lemma. All the moments of the four random variables $X_T, \\tilde{X}_T, Y_T, \\tilde{Y}_T$ and their integrals over $[0,T]$ converge to constants as $T\\rightarrow{0}.$ We rephrase this argument as the following technical statement for later use. The proof is provided in Appendix \\ref{proof:dummy}.\n\n\\begin{lemma}\\label{lem:dummy} Under Assumption \\ref{classical assumption}, consider processes $(X_t)_{t\\ge0}$, $(\\tilde{X}_t)_{t\\ge0}$, $(Y_t)_{t\\ge0}$, $(\\tilde{Y}_t)_{t\\ge0}$ stated in Eqs.\\eqref{eq:X_t}, \\eqref{eq:Y_t}, and \\eqref{eq:tilde x,y}. The process $(Z_t^{p_1,p_2,p_3,p_4})_{t\\ge 0}$, which is defined by $Z_t^{p_1,p_2,p_3,p_4}:=X_t^{p_1}\\tilde{X}_t^{p_2}Y_t^{p_3}\\tilde{Y}_t^{p_4}$ for any $p_i\\in\\mathbb{R}\\,,$ $i\\in\\{1,2,3,4\\}\\,,$ satisfies following two statements.\n\\begin{enumerate}[label=(\\roman*)]\n\\item $\\begin{aligned}\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}Z_t^{p_1,p_2,p_3,p_4}\\Big]<\\infty\\,\\end{aligned}$ for any $T>0\\,.$\\,\\, Furthermore, $\\begin{aligned}\n    \\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}Z_t^{p_1,p_2,p_3,p_4}\\Big]=S_0^{p_1+p_2}\\,.\n\\end{aligned}$\n\\item Moreover, for any $q_j\\in\\mathbb{R}\\,,$ $j\\in\\{1,2,\\cdots\\,8\\}\\,,$\n\\begin{align}\n    &\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Bigg[Z_T^{q_1,q_2,q_3,q_4}\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)^{q_5}\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^{q_6}\n    \\left(\\frac{1}{T}\\int_0^TY_t\\,dt\\right)^{q_7}\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^{q_8}\\Bigg] \\\\\n    &=S_0^{q_1+q_2+q_5+q_6}\\,.\\label{eq:dummy}\n\\end{align}\n\\end{enumerate}\n\\end{lemma}\n\n\\section{Short-maturity limit of an option price with Lipschitz continuous payoffs}\n\\label{sec:Short maturity limit of an option price}\nUnder the risk-neutral measure $\\mathbb{Q}$, \nthe arbitrage-free values of the Asian and European options are \n$$ P_{A}(T):=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\right]\\,,\\quad   P_{E}(T):=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}[\\Phi(S_T)]\\,,$$\nwhere $T$ is the maturity. Throughout this section, we impose Assumption \\ref{payoff assumption} on $\\Phi\\,.$ Our objective is to find an asymptotic formula for the Asian option price up to $\\mathcal{O}(T)$; a formula for its European counterpart is also presented. \nMore precisely, we will prove in Theorem \\ref{thm:option price}\nthat \n$P_A(T)$ and $P_E(T)$ are asymptotically equal to \n$$\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+S_0\\sqrt{\\frac{1}{T^2}\\int_0^T\\sigma^2(t,S_0)(T-t)^2\\,dt}\\,Z\\right)\\right]$$\nand  $$\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+S_0\\sqrt{\\int_0^T\\sigma^2(t,S_0)\\,dt}\\,Z\\right)\\right]\\,,$$ respectively, where $Z$ is a standard normal random variable.\n\nTo achieve these results, we perform two approximation steps.\nFirst, in   Lemma \\ref{lem:price drift 0}, \nthe underlying $(S_t)_{0\\le t \\le T}$ is approximated by $(X_t)_{0\\le t \\le T}.$\nSecond, in  Theorem \\ref{thm:option price}, \nwe approximate the process\n$(X_t)_{0\\le t \\le T}$ sequentially by  $(\\tilde{X}_t)_{0\\le t \\le T}$ and $(\\hat{X}_t)_{0\\le t \\le T}$  using\nLemma \\ref{lem:x,y close}. The proof of Lemma \\ref{lem:price drift 0} is provided in Appendix \\ref{proof:price drift negligible}.\n\\begin{lemma}\\label{lem:price drift 0}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{enumerate}[label=(\\roman*)]\n    \\item $ \\begin{aligned} P_A(T)=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]+\\mathcal{O}(T)\\,,\\end{aligned}$\n    \\item $ \\begin{aligned} P_E(T)=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)]+\\mathcal{O}(T)\\,. \\end{aligned}$\n\\end{enumerate}\n\\end{lemma}\n\\noindent As can be seen in Eqs.\\eqref{eq:S_t} and \\eqref{eq:X_t}, the processes $S$ and $X$ have the same diffusion terms; however, the drift term of $X$ is zero.\nThus, this lemma implies that \nwhile estimating the Asian and European option prices,\nthe drift of the underlying stock becomes negligible at small $T>0$. This is similar to Theorem 2 of \\cite{PirjolDan2016SMAO} and Theorem 5 of \\cite{PirjolDan2019SMAO}. In \\cite{PirjolDan2016SMAO, PirjolDan2019SMAO}, the rate function that governs the short-maturity behavior of the Asian call and put option was shown to be independent of the drift term.\n\nThe main result  of this section is the following  theorem, which states asymptotic formulas for the Asian and European option prices.\n\\begin{theorem}\\label{thm:option price}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{enumerate}[label=(\\roman*)]\n    \\item $ \\begin{aligned} P_{A}(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+S_0\\sqrt{\\frac{1}{T^2}\\int_0^T\\sigma^2(t,S_0)(T-t)^2\\,dt}\\,Z\\right)\\right]+\\mathcal{O}(T)\\,,\\end{aligned}$\n    \\item $\\begin{aligned} P_{E}(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+S_0\\sqrt{\\int_0^T\\sigma^2(t,S_0)\\,dt}\\,Z\\right)\\right]+\\mathcal{O}(T)\\,,\\end{aligned}$\n\\end{enumerate}\nwhere $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)$.\n\\end{theorem}\n\\begin{proof}\nThe statement actually directly comes from Lemmas \\ref{lem:x,y close} and \\ref{lem:price drift 0}. Observe that\n\\begin{align}\n    \\left\\vert \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right]\\right\\vert\n    \\le   \\frac{\\beta}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t\\vert]\\,dt=\\frac{\\beta B_1}{2}T\\,, \\\\\n    \\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right]-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\right]\\right\\vert\n    \\le   \\frac{\\beta}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t\\vert]\\,dt=\\frac{\\beta B_1}{2}T\\,\n\\end{align}\nfor the positive constant $B_1$ in  Lemma \\ref{lem:x,y close} and $\\beta$ in Assumption \\ref{payoff assumption}.\nThus, together with Lemma \\ref{lem:price drift 0}, we get\n\\begin{align}\n    P_A(T)\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\right]+\\mathcal{O}(T)\\,.\n\\end{align}\nFrom a direct calculation using the Fubini theorem regarding a stochastic integral,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\right]\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{S_0}{T}\\int_0^T \\int_0^t\\sigma(s,S_0)\\,dW_s\\,dt\\right)\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{S_0}{T}\\int_0^T \\sigma(s,S_0)(T-s)\\,dW_s\\right)\\right]\\,.\n\\end{align}\nHence, we get the desired result for $P_A(T)$. Applying the same argument to $P_E(T)$, \n\\begin{equation}\n    P_E(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(\\hat{X}_T)]+\\mathcal{O}(T).\n\\end{equation}\nUsing\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[\\Phi(\\hat{X}_T)]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+S_0\\int_0^T\\sigma(t,S_0)\\,dW_t\\right)\\right]\\,,\n\\end{equation}\nwe get the desired result.\n\\end{proof}\n\\begin{cor}\\label{cor:option price limit same} Under Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, the prices of both the Asian option and the European option share the same limit $\\Phi(S_0)$ as $T\\rightarrow{0}$ with the convergence order $\\mathcal{O}(\\sqrt{T}).$ More precisely,\n    \\begin{equation} P_{A}(T)=\\Phi(S_0)+\\mathcal{O}(\\sqrt{T})\\,, \\quad P_{E}(T)=\\Phi(S_0)+\\mathcal{O}(\\sqrt{T})\\,.\\end{equation}\n\\end{cor}\n\nWe introduce two notions of volatilities called the {\\em Asian volatility} and the {\\em European volatility}.\n\\begin{definition} We define the Asian volatility $\\sigma_A(T)$ and the European volatility $\\sigma_E(T)$ as\n\\begin{equation}\\label{def:asian european volatility}\n    \\sigma_{A}(T):=\\sqrt{\\frac{1}{T^3}\\int_0^T\\sigma^2(t,S_0)(T-t)^2\\,dt}\\,, \\quad \\sigma_{E}(T):=\\sqrt{\\frac{1}{T}\\int_0^T\\sigma^2(t,S_0)\\,dt}\\,.\n\\end{equation}\n\\end{definition}\n\\noindent In terms of the Asian volatility and the European volatility, Theorem \\ref{thm:option price} can be rewritten as\n$$ P_A(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)]+\\mathcal{O}(T)\\,,\\quad\nP_E(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)]+\\mathcal{O}(T)\\,.$$\nIn Section \\ref{sec:Short maturity asymptotic for a sensitivity}, short-maturity asymptotic formulas for the delta values will be presented in terms of the Asian volatility and the European volatility.\n\\begin{remark}\nFrom the definition of $\\sigma_A(T)$ and $\\sigma_E(T)$, we can observe that they depend only on the first argument $t$ of the volatility function $\\sigma(t,x)\\,.$ For any $0\\le t\\le T\\,,$ the local behavior of $x\\mapsto\\sigma(t,x)$ near $x=S_0$ does not affect $\\sigma_A(T)$ and $\\sigma_E(T)\\,.$\n\\end{remark}\nWe compute asymptotic results for the call and put options as an example of Theorem \\ref{thm:option price}. This generalizes the result in Theorem 6 of \\cite{PirjolDan2016SMAO} for the ATM case.\n\\begin{ex}\\label{ex:call,put price}\nLet $P_A^{\\textnormal{call}}$ and $P_A^{\\textnormal{put}}$\nbe the Asian call and put prices with  the strike $K,$ i.e., the payoff functions are   $\\Phi(x)=(x-K)_+,$ and $\\Phi(x)=(K-x)_+,$ respectively.\n Then,  \n\\begin{align}\n    &P_A^{\\textnormal{call}}(T)=\\begin{cases}\n0+\\mathcal{O}(T), & \\mbox{if  }S_0<K\\,, \\\\\n\\frac{S_0\\sigma_A(T)}{\\sqrt{2\\pi}}\\sqrt{T}+\\mathcal{O}(T), & \\mbox{if  }S_0=K \\,,\\\\\nS_0-K+\\mathcal{O}(T), & \\mbox{if  }S_0>K\\,,\n\\end{cases}\\\\\n    &P_A^{\\textnormal{put}}(T)=\\begin{cases}\nK-S_0+\\mathcal{O}(T), & \\mbox{if  }S_0<K \\,,\\\\\n\\frac{S_0\\sigma_A(T)}{\\sqrt{2\\pi}}\\sqrt{T}+\\mathcal{O}(T), & \\mbox{if  }S_0=K\\,, \\\\\n0+\\mathcal{O}(T), & \\mbox{if  }S_0>K\\,.\n\\end{cases}\n\\end{align}\nThe prices of the European call and put option are obtained by replacing $\\sigma_A(T)$ in the above-mentioned expressions with $\\sigma_E(T)$.\n\\end{ex}\n\\begin{ex}\nGiven any $K\\,,\\delta>0$ and $0\\le \\epsilon <1\\,,$ define the payoff function $\\Phi$ by\n\\begin{align}\n    \\Phi(x)=(x-K)^{1+\\epsilon}\\mathbbm{1}_{\\{K\\le x<K+\\delta\\}}+\\delta^{1+\\epsilon}\\mathbbm{1}_{\\{K+\\delta\\le x\\}}\\,.\n\\end{align}\nSuppose that $S_0=K\\,.$ Then, we get the following asymptotic equation.\n\\begin{equation}\n    P_A(T)=\\frac{1}{2}(S_0\\sigma_A(T))^{1+\\epsilon}\\,M(1+\\epsilon)T^{\\frac{1+\\epsilon}{2}}+\\mathcal{O}(T)\\,,\n\\end{equation}\nwhere $M(1+\\epsilon):=\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^{1+\\epsilon}]$ with a standard normal variable $Z\\,.$ If we replace $\\sigma_A(T)$ with $\\sigma_E(T)\\,,$ we get the asymptotic result for the European option price $P_E(T)\\,.$\n\\end{ex}\n\n\n\\section{Short-maturity estimates for an option delta value with Lipschitz continuous payoffs}\n\\label{sec:Short maturity asymptotic for a sensitivity}\nIn this section, we present the short-maturity asymptotic for the sensitivity of the option with respect to the initial value $S_0$. In many studies, this sensitivity is referred to as {\\em delta}. We follow this convention to define the Asian delta value and the European delta value as\n$$ \\Delta_A(T):=\\frac{\\partial}{\\partial S_0}P_A(T)\\,,\\quad \\Delta_E(T):=\\frac{\\partial}{\\partial S_0}P_E(T)\\,. $$\nThroughout this section, only the Lipschitz continuous payoff $\\Phi$ will be considered. Our main objective is to obtain the short-maturity asymptotic for $\\Delta_A(T)$, $\\Delta_E(T)$. These asymptotic results are given in Theorems \\ref{thm:asian delta} and \\ref{thm:european delta}. In summary, for small $T>0$,\n$$ \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,, $$\n$$ \\Delta_E(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,, $$\nwhere $Z$ denotes a standard normal random variable. \\\\\n\\indent We derive the above-mentioned formulas as follows. In Lemma \\ref{lem:delta drift 0}, we first approximate $(S_t)_{0\\le t\\le T}$ by $(X_t)_{0\\le t\\le T}$ in line with Lemma \\ref{lem:price drift 0}. See Appendix \\ref{proof:delta drift 0} for the proof.\n\\begin{lemma}\\label{lem:delta drift 0}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{enumerate}[label=(\\roman*)]\n    \\item $\\begin{aligned}\\Delta_A(T)=e^{-rT}\\,\\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\end{aligned}$\\,,\n    \\item $\\begin{aligned}\\Delta_E(T)=e^{-rT}\\,\\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)]+\\mathcal{O}(\\sqrt{T})\\end{aligned}$\\,.\n\\end{enumerate}\n\\end{lemma}\n\\noindent Next, in Sections \\ref{subsec:asian delta mal} and \\ref{subsec:asian delta comp}, we present a short-maturity asymptotic formula for the Asian delta value $\\Delta_A(T)$. A formula for the European delta value $\\Delta_E(T)$ is presented in Section \\ref{subsec:european delta comp}.\n\n\n\\subsection{Approximation for the Malliavin  representation of the Asian delta value}\\label{subsec:asian delta mal}\nBy Malliavin calculus, the Asian delta value can be represented by the weighted sum of the payoffs. The computation under the Black--Scholes model has already been presented in \\cite{BoylePhelim2008Paso, benhamou2000application}. Under the local volatility model, we describe a possible representation in the following proposition.\n\\begin{proposition}\\label{prop:asian delta mal} \nFor the process $X$\nstated in Eq.\\eqref{eq:X_t} under Assumption \\ref{classical assumption}, we have\n\\begin{align}\n    \\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^TX_t\\,dt\\right)\\delta\\left(\\frac{2{Y_{\\cdot}}^2}{\\sigma(\\cdot,X_{\\cdot})X_{\\cdot}\\int_0^T Y_t\\,dt}\\right)\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta\\left(\\frac{2{Y_{\\cdot}}^2}{\\sigma(\\cdot,X_{\\cdot})X_{\\cdot}}\\right)\\frac{1}{\\int_0^T Y_t\\,dt}\\right] \\\\\n    &\\quad-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T \\frac{2{Y_s}^2}{\\sigma(s,X_s)X_s}D_{s}\\left(\\frac{1}{\\int_0^T Y_t\\,dt}\\right)\\,ds\\right]\\,,\n\\end{align}\nwhere $\\delta(\\cdot)$ is the Skorokhod integral in $[0,T]$ and $D_s(\\cdot)$ is the Malliavin derivative.\n\\end{proposition}\n\\begin{proof}\nSee \\cite{NualartDavid1995TMca,benhamou2000application}.\n\\end{proof}\n\\noindent With this proposition, Lemma \\ref{lem:delta drift 0} implies that for small $T>0$, the Asian delta value asymptotically behaves as\n\\begin{equation}\\label{eqn:delta_A_T}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta(u)F\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T u_{s}(D_{s}F)\\,ds\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\nHere, we define a process $(u_s)_{0\\le s\\le T}$ and a random variable $F$ by\n$$ u_{s}:=\\frac{2{Y_s}^2}{\\sigma(s,X_s)X_s}\\,,\\quad F:=\\frac{1}{\\int_0^T Y_t\\,dt}\\,.$$\n\n\n\n\nTo investigate $ \\Delta_A(T)$, we will approximate the two expectations on the right-hand side of Eq.\\eqref{eqn:delta_A_T}.\nThe approximation of the first expectation,\n\\begin{equation}\\label{eq:asian delta mal cal 1}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta(u)F\\right], \n\\end{equation}\nis described in Proposition \\ref{prop:Asian delta app 1}, \nand the approximation of the second expectation,\n\\begin{equation}\\label{eq:asian delta mal cal 2}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T u_{s}(D_{s}F)\\,ds\\right], \n\\end{equation}\nis described in Proposition \\ref{prop:Asian delta app 2}.\nTo analyze Eq.\\eqref{eq:asian delta mal cal 1}, we introduce \ntwo processes\n $(\\tilde{u}_s)_{0\\le s\\le T}$ and $(\\hat{u}_s)_{0\\le s\\le T}$ \nand two random variables\n$\\tilde{F}$ and $\\hat{F},$ defined by\n$$ \\tilde{u}_s:=\\frac{2{\\tilde{Y}_s}^2}{\\sigma(s,\\tilde{X}_s)\\tilde{X}_s}\\,, \\quad \\hat{u}_s:=\\frac{2{\\hat{Y}_s}^2}{\\sigma(s,\\hat{X}_s)\\hat{X}_s}\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}\\, $$\nand\n$$ \\tilde{F}:=\\frac{1}{\\int_0^T \\tilde{Y}_t\\,dt}\\,, \\quad \\hat{F}:=\\frac{1}{\\int_0^T \\hat{Y}_t\\,dt}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\\,, $$\nwhere $\\mathbbm{1}_A$ denotes the indicator function of set $A$.\nIn Lemma \\ref{lem:u,TF app}, \nthe process $(u_s)_{0\\le s\\le T}$ is approximated  using $(\\tilde{u}_s)_{0\\le s\\le T}$ and $(\\hat{u}_s)_{0\\le s\\le T}$. As for the random variable $F$, we use $\\tilde{F}$ and $\\hat{F}.$ \nThis procedure is similar to the approximation  based on Lemma \\ref{lem:x,y close}. The proof of Lemma \\ref{lem:u,TF app} is given in Appendix \\ref{proof: u,TF app}.\n\n\\begin{remark} The reason for introducing the indicator functions\n\t$\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}$ and $\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}$ in $\\hat{u}$ and $\\hat{F}$, respectively, is as follows.\nThe random variable \t\n$\\frac{2{\\hat{Y}_s}^2}{\\sigma(s,\\hat{X}_s)\\hat{X}_s}$  is not integrable in general, as we merely assumed that \nthe denominator \n$\\vert\\sigma(s,\\hat{X}_s)\\hat{X}_s\\vert$ is bounded above by $\\overline{\\sigma}\\vert\\hat{X}_s\\vert.$\nThis problem can be overcome by simply \nmultiplying the indicator function   $\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}$ by $\\frac{2{\\hat{Y}_s}^2}{\\sigma(s,\\hat{X}_s)\\hat{X}_s}.$\nIt is clear that  $\\mathbb{E}^\\mathbb{Q}|\\hat{u}_s|<\\infty$\nsince \n$$|\\hat{u}_s|=\\frac{2{\\hat{Y}_s}^2}{\\sigma(s,\\hat{X}_s)\\hat{X}_s}\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}\\leq \\frac{4{\\hat{Y}_s}^2}{\\underline{\\sigma}S_0}\\,$$\nand $\\hat{Y}_s$ is normal.\nThe function $\\hat{F}$ is also defined to satisfy this integrability condition.\n\\end{remark}\n\\begin{lemma}\\label{lem:u,TF app}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $D_p$ depending only on $p$ such that the following inequalities hold. \n\\begin{enumerate}[label=(\\roman*)]\n    \\item For $0\\le t\\le 1$,\n    \\begin{align}\\label{eq:u app}\n        \\mathbb{E}^\\mathbb{Q}[\\left\\vert u_t-\\tilde{u}_t \\right\\vert^p] \\le D_pt^p, \\quad \\mathbb{E}^\\mathbb{Q}[\\left\\vert \\tilde{u}_t-\\hat{u}_t \\right\\vert^p]\\le D_pt^p.\n    \\end{align}\n    \\item For $0\\le T\\le 1$,\n    \\begin{align}\\label{eq:TF app}\n        \\mathbb{E}^\\mathbb{Q}[\\vert TF-T\\tilde{F} \\vert^p] \\le D_pT^p, \\quad \\mathbb{E}^\\mathbb{Q}[\\vert T\\tilde{F}-T\\hat{F} \\vert^p] \\le D_pT^p.\n    \\end{align}\n\\end{enumerate}\n\\end{lemma}\n\\begin{proposition}\\label{prop:Asian delta app 1}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n    \\begin{equation}\\label{eq:Asian delta app 1.1}\n        \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta(u)F\\right]\n        =\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\delta(\\tilde{u})\\tilde{F}\\right]+\\mathcal{O}(\\sqrt{T})\n    \\end{equation}\nand\n    \\begin{equation}\\label{eq:Asian delta app 1.2}\n        \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\delta(\\tilde{u})\\tilde{F}\\right] =\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right]+\\mathcal{O}(\\sqrt{T}).\n    \\end{equation}\n\\end{proposition}\n\n\\begin{proof}\nWe only present the proof for Eq.\\eqref{eq:Asian delta app 1.1}. As Eq.\\eqref{eq:Asian delta app 1.2} can be proved in exactly the same manner, its proof is omitted. We may assume that $\\Phi(0)=0$. Consider a translation $\\Psi(\\cdot):=\\Phi(\\cdot)-\\Phi(0)$  otherwise. Observe that\n\\begin{align}\n    &\\left\\vert \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta(u)F\\right]-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\delta(\\tilde{u})\\tilde{F}\\right]\\right\\vert \\\\\n    &\\le \\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)-\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(u)\\vert\\,\\vert TF\\vert\\right] \\\\\n    &+\\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert  \\delta(u-\\tilde{u})\\vert\\,\\vert TF\\vert\\right] \\\\\n    &+\\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(\\tilde{u})\\vert\\,\\vert TF-T\\tilde{F}\\vert\\right] \\,.\n\\end{align}\nUsing Assumption \\ref{payoff assumption}, the H\\"older inequality, the Jensen inequality, and Lemma \\ref{lem:x,y close}, for $0\\le T\\le 1$, we obtain \n\\begin{align}\n    &\\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)-\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(u)\\vert\\,\\vert TF\\vert\\right] \\\\\n    &\\le \\frac{\\beta}{\\sqrt{T}} \\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t\\vert^4]\\,dt\\right)^{\\frac{1}{4}} \\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\delta(u)\\right)^2\\bigg]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert TF\\vert^4]\\right)^{\\frac{1}{4}} \\\\\n    &\\le \\frac{2\\beta}{\\underline{\\sigma}}\\left(\\frac{B_4}{5}\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le T}{\\max}\\left(Y_s^{4}X_s^{-2}\\right)\\Big]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert TF\\vert^4]\\right)^{\\frac{1}{4}}\\,\\sqrt{T}\\,,\\label{proof:Asian delta app 1.1}\n\\end{align}\nfor a positive constant $B_4$ defined in Lemma \\ref{lem:x,y close}.\nThe last inequality is based on the fact the Skorokhod integral of $u$ becomes the Ito integral\nsince $(u_s)_{0\\le s\\le T}$ is adapted to the Brownian filtration $(\\mathcal{F}_s^W)_{0\\le s\\le T}.$  Observe from $\\Phi(0)=0$ and Assumption \\ref{payoff assumption} that $\\vert\\Phi(x)\\vert\\le\\beta\\vert x\\vert$ holds for any $x\\in \\mathbb{R}$. Thus, by similar arguments, we obtain\n\\begin{align}\n    &\\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert  \\delta(u-\\tilde{u})\\vert\\,\\vert TF\\vert\\right] \\\\\n    &\\le\\frac{\\beta}{\\sqrt{T}}  \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^4\\right]\\right)^{\\frac{1}{4}} \\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\delta(u-\\tilde{u})\\right)^2\\bigg]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert TF\\vert^4]\\right)^{\\frac{1}{4}} \\\\\n    &\\le\\beta\\left(\\frac{D_2}{3}\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^4\\right]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert TF\\vert^4]\\right)^{\\frac{1}{4}}\\,\\sqrt{T}\\,,\n\\end{align}\nand \n\\begin{align}\n    &\\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(\\tilde{u})\\vert\\,\\vert TF-T\\tilde{F}\\vert\\right] \\\\\n    &\\le\\frac{\\beta}{\\sqrt{T}} \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^4\\right]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\delta(\\tilde{u})\\right)^2\\bigg]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert TF-T\\tilde{F}\\vert^4]\\right)^{\\frac{1}{4}} \\\\\n    &\\le\\frac{2\\beta}{\\underline{\\sigma}}(D_4)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^4\\right]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le T}{\\max}\\left(\\tilde{Y}_s^{4}\\tilde{X}_s^{-2}\\right)\\Big]\\right)^{\\frac{1}{2}}\\,\\sqrt{T}\\,,\n\\end{align}\nfor $0\\le T\\le 1$, where the constants $D_2, D_4$ are as given in Lemma \\ref{lem:u,TF app}. Therefore, from Lemma \\ref{lem:dummy}, we obtain the inequality  \n\\begin{align}\n    &\\limsup_{T\\rightarrow{0}} \\frac{1}{\\sqrt{T}}\\left\\vert \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\delta(u)F\\right]-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\delta(\\tilde{u})\\tilde{F}\\right]\\right\\vert\\\\\n    &\\le \\frac{2\\beta}{\\underline{\\sigma}S_0}\\left(\\frac{B_4}{5}\\right)^{\\frac{1}{4}}+S_0\\beta\\left(\\frac{D_2}{3}\\right)^{\\frac{1}{2}}+\\frac{2\\beta}{\\underline{\\sigma}}(D_4)^{\\frac{1}{4}}\\,<\\infty\\,.\n\\end{align}\nThis gives the desired result.\n\\end{proof}\n\\indent Now, we will approximate Eq.\\eqref{eq:asian delta mal cal 2}. To do this, we approximate $D_sF$ by $D_s\\tilde{F}$ and $D_s^{*}\\hat{F}$, where $D_s^{*}\\hat{F}$ is defined as \n$$D_s^{*}\\hat{F}:=D_s\\left(\\frac{1}{\\int_0^T\\hat{Y}_t\\,dt}\\right)\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y_t}\\,dt\\ge \\frac{1}{2}\\}}\\,.$$\nThrough some auxiliary steps, these approximations among $D_sF$, $D_s\\tilde{F}$, and $D_s^{*}\\hat{F}$ are presented in Lemma \\ref{lem:Ds close}. Before investigating them, we first show in Lemma \\ref{lem:Ds bound} that the $p$th moments of $D_sF$, $D_s\\tilde{F}$, and $D_s^{*}\\hat{F}$ are $\\mathcal{O}(\\frac{1}{T^p})$ in a short-maturity regime. As a necessary step, we also observe that the moments of $D_sX_t$, $D_sY_t$, $D_s\\tilde{Y}_t$, and $D_s\\hat{Y}_t$ are bounded. See Appendices \\ref{proof:Ds bound} and \\ref{proof:Ds close} for the proofs.\n\\begin{lemma}\\label{lem:Ds bound}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $E_p$ depending only on $p$ such that the following inequalities hold.\n\\begin{enumerate}[label=(\\roman*)]\n    \\item For $0\\le t\\le 1$,\n    \\begin{equation}\\label{eq:Dsx bound}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}X_t \\vert^p] \\le E_p.\n    \\end{equation}\n    \\item For $0\\le t\\le 1$,\n    \\begin{equation}\\label{eq:DsY bound}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}Y_t \\vert^p] \\le E_p, \\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}\\tilde{Y}_t \\vert^p] \\le E_p,\\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}\\hat{Y}_t \\vert^p] \\le E_p.\n    \\end{equation}\n    \\item For $0\\le T\\le 1$,\n    \\begin{equation}\\label{eq:TDsF bound}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert TD_{s}F\\vert^p] \\le E_p,\\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert TD_{s}\\tilde{F}\\vert^p] \\le E_p,\\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert TD_{s}^{*}\\hat{F}\\vert^p] \\le E_p.\n    \\end{equation}\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{lemma}\\label{lem:Ds close}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $F_p$ depending only on $p$ such that the following inequalities hold.\n\\begin{enumerate}[label=(\\roman*)]\n    \\item For $0\\le t\\le 1$,\n    \\begin{equation}\\label{eq:DsY close}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}Y_t-D_{s}\\tilde{Y}_t \\vert^p] \\le F_pt^{\\frac{p}{2}}, \\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}\\tilde{Y}_t-D_{s}\\hat{Y}_t \\vert^p] \\le F_pt^{\\frac{p}{2}}.\n    \\end{equation}\n    \\item For $0\\le T\\le 1$,\n    \\begin{equation}\\label{eq:TDsF close}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert TD_{s}F-TD_{s}\\tilde{F} \\vert^p] \\le F_pT^{\\frac{p}{2}}, \\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert TD_{s}\\tilde{F}-TD_{s}^{*}\\hat{F} \\vert^p] \\le F_pT^{\\frac{p}{2}}.\n    \\end{equation}\n\\end{enumerate}\n\\end{lemma}\n\\begin{proposition}\\label{prop:Asian delta app 2}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n    \\begin{equation}\\label{eq:Asian delta app 2.1}\n        \\begin{aligned}\n        &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T u_{s}(D_{s}F)\\,ds\\right] \\\\ &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\int_0^T \\tilde{u}_{s}(D_{s}\\tilde{F})\\,ds\\right] +\\mathcal{O}(\\sqrt{T})\\,\n        \\end{aligned}\n    \\end{equation}\n    and\n    \\begin{equation}\\label{eq:Asian delta app 2.2}\n        \\begin{aligned}\n        &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\int_0^T \\tilde{u}_{s}(D_{s}\\tilde{F})\\,ds\\right] \\\\ &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\n        +\\mathcal{O}(\\sqrt{T}).\n        \\end{aligned}\n    \\end{equation}\n\\end{proposition}\n\\begin{proof}\nWe only present the proof for Eq.\\eqref{eq:Asian delta app 2.1}. Similarly, Eq.\\eqref{eq:Asian delta app 2.2} follows. We may assume that $\\Phi(0)=0$.\nObserve that\n\\begin{align}\n    &\\Bigg\\vert \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T u_{s}(D_{s}F)\\,ds\\right] -\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\int_0^T \\tilde{u}_{s}(D_{s}\\tilde{F})\\,ds\\right] \\Bigg\\vert \\\\\n    &\\le \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)-\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T \\vert u_s\\vert\\vert TD_sF\\vert\\,ds\\right] \\\\\n    &+\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T \\vert u_s-\\tilde{u}_s\\vert\\vert TD_sF\\vert\\,ds\\right] \\\\\n    &+\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T\\vert \\tilde{u}_s\\vert\\vert TD_sF-TD_s\\tilde{F}\\vert\\,ds\\right]\\,.\n\\end{align}\nNote from Assumption \\ref{payoff assumption}, Lemmas \\ref{lem:x,y close} and \\ref{lem:Ds bound}, the H\\"older inequality, and the Jensen inequality that for $0\\le T\\le 1\\,,$\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)-\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T \\vert u_s\\vert\\vert TD_sF\\vert\\,ds\\right] \\\\\n    &\\le \\beta\\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t\\vert^2]\\,dt\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert u_s\\vert^2\\vert TD_sF\\vert^2]\\,ds\\right)^{\\frac{1}{2}}\\\\\n    &\\le \\beta\\left(\\frac{B_2}{3}\\right)^{\\frac{1}{2}}T\\left(\\frac{1}{T}\\int_0^T(\\mathbb{E}^\\mathbb{Q}[\\vert u_s\\vert^4])^{\\frac{1}{2}}(E_4)^{\\frac{1}{2}}\\,ds\\right)^{\\frac{1}{2}}\\\\\n    &\\le \\frac{2\\beta}{\\underline{\\sigma}}\\left(\\frac{B_2}{3}\\right)^{\\frac{1}{2}}(E_4)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le T}{\\max}Y_s^{8}X_s^{-4}\\Big]\\right)^{\\frac{1}{4}}T\\,\n\\end{align}\nholds with some positive constants $B_2$ and $E_4$. By assuming that $\\Phi(0)=0$, we obtain $|\\Phi(x)|\\le\\beta|x|$. From this inequality with Lemmas \\ref{lem:u,TF app} and \\ref{lem:Ds bound}, the H\\"older inequality, and the Jensen inequality, we observe that for $0\\le T\\le 1$,\n\\begin{align}\n   &\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T \\vert u_s-\\tilde{u}_s\\vert\\vert TD_sF\\vert\\,ds\\right] \\\\\n   &\\le\\beta\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert u_s-\\tilde{u}_s\\vert^2\\vert TD_sF\\vert^2]\\,ds\\right)^{\\frac{1}{2}}\\\\\n   &\\le\\beta\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T (D_4)^{\\frac{1}{2}}s^2(E_4)^{\\frac{1}{2}}\\,ds\\right)^{\\frac{1}{2}} \\\\\n   &\\le\\beta(D_4)^{\\frac{1}{4}}(E_4)^{\\frac{1}{4}}\\left(\\frac{1}{3}\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}T\\,\n\\end{align}\nwith some positive constants $D_4, E_4$. Similarly,\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)\\right\\vert\\frac{1}{T}\\int_0^T\\vert \\tilde{u}_s\\vert\\vert TD_sF-TD_s\\tilde{F}\\vert\\,ds\\right]\\\\\n    &\\le\\beta\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{u}_s\\vert^2\\vert TD_sF-TD_s\\tilde{F}\\vert^2]\\,ds\\right)^{\\frac{1}{2}}\\\\\n    &\\le\\beta\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T(\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{u}_s\\vert^4])^{\\frac{1}{2}}(F_4)^{\\frac{1}{2}} T\\,ds\\right)^{\\frac{1}{2}} \\\\\n    &\\le\\frac{2\\beta}{\\underline{\\sigma}}(F_4)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{X}_t\\,dt\\right)^2\\right]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le T}{\\max}\\tilde{Y}_s^{8}\\tilde{X}_s^{-4}\\Big]\\right)^{\\frac{1}{4}}\\sqrt{T}\\,\n\\end{align}\nfor a constant $F_4$ from Lemma \\ref{lem:Ds close}. Therefore, from Lemma \\ref{lem:dummy}, we can obtain\n\\begin{align}\n    &\\limsup_{T\\rightarrow{0}}\\frac{1}{\\sqrt{T}}\\Bigg\\vert \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\int_0^T u_{s}(D_{s}F)\\,ds\\right] -\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)\\int_0^T \\tilde{u}_{s}(D_{s}\\tilde{F})\\,ds\\right] \\Bigg\\vert \\\\\n    &\\le \\frac{2\\beta}{\\underline{\\sigma}}(F_4)^{\\frac{1}{4}}<\\infty\\,.\n\\end{align}\nThis gives the desired result.\n\\end{proof}\n\\subsection{Short-maturity asymptotic for the Asian delta value}\\label{subsec:asian delta comp}\nLet us concatenate the approximations established in Propositions \\ref{prop:Asian delta app 1} and \\ref{prop:Asian delta app 2} to obtain\n\\begin{equation}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\nTo deduce the short-maturity asymptotic formula presented in Theorem \\ref{thm:asian delta}, we now estimate the following two expectations.\n\\begin{equation}\\label{eq:asian delta mal cal 3}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right]\\,, \\quad\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\\,.\n\\end{equation}\nIn the first step, $\\delta(\\hat{u})$ is approximated to a normal variable $\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)$ in Lemma \\ref{lem:hat u app}. See Appendix \\ref{proof:hat u app} for the proof.\n\\begin{lemma}\\label{lem:hat u app}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $G_p$ depending only on $p$ such that the following inequalities hold for $0\\le T\\le 1$:\n\\begin{equation}\\label{eq:hat u app}\n    \\mathbb{E}^\\mathbb{Q}[\\vert\\delta(\\hat{u})\\vert^p]\\le G_pT^{\\frac{p}{2}},\\quad\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\delta(\\hat{u})-\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right) \\right\\vert^p\\right]\\le G_pT^p.\n\\end{equation}\n\\end{lemma}\n\\noindent Then, using this lemma, we can directly estimate the expectations in Eq.\\eqref{eq:asian delta mal cal 3} only in terms of multivariate normal random variables. Consequently, we propose the following asymptotic relations in Proposition \\ref{prop:Asian delta app 3}. Further details are provided in Appendix \\ref{proof:Asian delta app 3}.\n\\begin{proposition}\\label{prop:Asian delta app 3}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\\label{eq:Asian delta app 3.1}\n\\begin{aligned}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right] \\\\ &=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]\n    -2\\frac{\\Phi(S_0)}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds +\\mathcal{O}(\\sqrt{T})\\,\n\\end{aligned}\n\\end{equation}\nand\n\\begin{equation}\\label{eq:Asian delta app 3.2}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\n    =-2\\frac{\\Phi(S_0)}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds +\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)\\,.$\n\\end{proposition}\nWe now reach one of the two main results in this section. The two estimates in Proposition \\ref{prop:Asian delta app 3} directly give the short-maturity asymptotic for the Asian delta value $\\Delta_A(T)\\,.$ Recall the definition of the Asian volatility $\\sigma_A(T)$ in Eq.\\eqref{def:asian european volatility}.\n\\begin{theorem}\\label{thm:asian delta}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\\label{eq:asian delta 1}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,\n\\end{equation}\nor, equivalently,\n\\begin{equation}\\label{eq:asian delta 2}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)-\\Phi(S_0)}{S_0\\sigma_A(T)\\sqrt{T}Z}Z^2\\right]+\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)$.\n\\end{theorem}\n\n\n\\begin{cor}\\label{cor:asian delta limit}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption},\n\\begin{enumerate}[label=(\\roman*)]\n    \\item if $\\Delta_A(T)$ converges as $T\\rightarrow{0}$, then\n    \\begin{align}\n        \\lim_{T\\rightarrow{0}}\\Delta_A(T)\n        =\\lim_{\\epsilon\\searrow{0}}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+\\epsilon Z)-\\Phi(S_0)}{\\epsilon Z}Z^2\\right]\\,,\n    \\end{align}\n    where $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)$;\n    \\item if both the right derivative $D\\Phi(S_0+)$ and the left derivative $D\\Phi(S_0-)$ exist, then $\\Delta_A(T)$ converges and\n    \\begin{align}\n        \\lim_{T\\rightarrow{0}}\\Delta_A(T)\n        =\\frac{D\\Phi(S_0+)+D\\Phi(S_0-)}{2}\\,.\n    \\end{align}\n\\end{enumerate}\n\\end{cor}\nWe present some examples of Theorem \\ref{thm:asian delta}.\n\\begin{ex}\\label{ex:call,put delta}\nLet $\\Delta_A^{\\textnormal{call}}$ and $\\Delta_A^{\\textnormal{put}}$\nbe the Asian call and put delta value with  the strike $K,$ i.e., the payoff functions are $\\Phi(x)=(x-K)_+$ and $\\Phi(x)=(K-x)_+,$ respectively.\n Then,  \n\\begin{align}\n    \\Delta_A^{\\textnormal{call}}(T)=\\begin{cases}\n0+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0<K\\,, \\\\\n\\frac{1}{2}+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0=K\\,, \\\\\n1+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0>K\\,,\n\\end{cases}\\quad\n    \\Delta_A^{\\textnormal{put}}(T)=\\begin{cases}\n-1+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0<K\\,, \\\\\n\\frac{1}{2}+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0=K\\,, \\\\\n0+\\mathcal{O}(\\sqrt{T}), & \\mbox{if  }S_0>K\\,.\n\\end{cases}\n\\end{align}\n\\end{ex}\n\\begin{ex}\\label{ex:convex payoff}\nGiven any $K\\,,\\delta>0$, and $0\\le \\epsilon <1\\,,$ define the payoff function $\\Phi$ by\n\\begin{align}\n    \\Phi(x)=(x-K)^{1+\\epsilon}\\mathbbm{1}_{\\{K\\le x<K+\\delta\\}}+\\delta^{1+\\epsilon}\\mathbbm{1}_{\\{K+\\delta\\le x\\}}\\,.\n\\end{align}\nSuppose that $S_0=K\\,.$ Then, we get the following asymptotic equation.\n\\begin{equation}\n    \\Delta_A(T)=\\frac{1}{2}(S_0\\sigma_A(T))^{\\epsilon}\\,M(2+\\epsilon)T^{\\frac{\\epsilon}{2}}+\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere $M(2+\\epsilon):=\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^{2+\\epsilon}]$ with a standard normal variable $Z\\,.$ In this example, the leading order of $\\Delta_A(T)$ is $T^{\\frac{\\epsilon}{2}}\\,$ as $T\\rightarrow{0}\\,.$\n\\end{ex}\n\n\\subsection{Short-maturity asymptotic for the European delta value} \\label{subsec:european delta comp}\nIn the remainder of this section, we will investigate the short-maturity behavior of the European delta value $\\Delta_E(T)$. The desired asymptotic formula is presented in Theorem \\ref{thm:european delta}. To prove this, we follow the same approximation steps used to derive the asymptotic formula for the Asian delta value $\\Delta_A(T)$ with a slight modification. First, we use Malliavin calculus to rewrite the European delta value as the weighted sum of the payoffs.\n\\begin{proposition} For the process $X$\nstated in Eq.\\eqref{eq:X_t} under Assumption \\ref{classical assumption}, we have\n\\begin{align}\n    \\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)]&=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(X_T\\right)\\delta\\left(\\frac{Y_{\\cdot}}{\\sigma(\\cdot,X_{\\cdot})X_{\\cdot}}\\right)\\frac{X_T}{Y_T}\\right] -\\frac{1}{S_0}\\,\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)] \\\\\n    &+\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(X_T)\\int_0^T \\frac{Y_s}{\\sigma(s,X_s)X_s}\\frac{X_T(D_{s}Y_T)}{{Y_T}^2}ds\\right],\n\\end{align}\nwhere $\\delta(\\cdot)$ denotes the Skorokhod integral in $[0,T]$ and $D_s(\\cdot)$ is the Malliavin derivative.\n\\end{proposition}\n\\begin{proof}\nSee \\cite{NualartDavid1995TMca,benhamou2000application}.\n\\end{proof}\n\\noindent Thus, we can observe from Corollary \\ref{cor:option price limit same} and Lemma \\ref{lem:delta drift 0} that for small $T>0\\,,$\n\\begin{equation}\\label{eqn:delta_E_T}\n    \\Delta_E(T)=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\Big[\\Phi(X_T)\\delta(h)G\\Big]-\\frac{\\Phi(S_0)}{S_0} +\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(X_T)\\int_0^T h_s H_s\\,ds\\right]+\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere the processes $(h_s)_{0\\le s\\le T}\\,, (H_s)_{0\\le s\\le T}$ and a random variable $G$ are defined by\n\\begin{align}\n    h_s:=\\frac{Y_s}{\\sigma(s,X_s)X_s}\\,, \\quad\n    H_s:=\\frac{X_T(D_{s}Y_T)}{{Y_T}^2}\\,, \\quad\n    G:=\\frac{X_T}{Y_T}\\,.\n\\end{align}\n\\indent In the second step, we approximate the two expectations on the right-hand side of Eq.\\eqref{eqn:delta_E_T}, i.e.,\n\\begin{align}\\label{eq:european delta app 1}\n    \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\Big[\\Phi(X_T)\\delta(h)G\\Big]\\,,\\quad \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(X_T)\\int_0^T h_s H_s\\,ds\\right]\\,.\n\\end{align}\nFor the approximation, we define four auxiliary processes $(\\tilde{h}_s)_{0\\le s\\le T}\\,,$ $(\\hat{h}_s)_{0\\le s\\le T}\\,,$ $(\\tilde{H}_s)_{0\\le s\\le T}\\,,$ $(\\hat{H}_s)_{0\\le s\\le T}$ by\n\\begin{align}\n    \\tilde{h}_s:=\\frac{\\tilde{Y}_s}{\\sigma(s,\\tilde{X}_s)\\tilde{X}_s}\\,, \\quad\n    \\hat{h}_s:=\\frac{\\hat{Y}_s}{\\sigma(s,\\hat{X}_s)\\hat{X}_s}\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}\\,\n\\end{align}\nand\n\\begin{align}\n    \\tilde{H}_s:=\\frac{\\tilde{X}_T(D_{s}\\tilde{Y}_T)}{{\\tilde{Y}_T}^2}\\,, \\quad\n    \\hat{H}_s:=\\frac{\\hat{X}_T(D_{s}\\hat{Y}_T)}{{\\hat{Y}_T}^2}\\mathbbm{1}_{\\{\\hat{Y}_T\\ge\\frac{1}{2}\\}}\\,.\n\\end{align}\nIn Lemma \\ref{lem:h,G,H app}, $(\\tilde{h}_s)_{0\\le s\\le T}\\,,$ $(\\hat{h}_s)_{0\\le s\\le T}$ are used to approximate $(h_s)_{0\\le s\\le T}$. Similarly, $(\\tilde{H}_s)_{0\\le s\\le T}\\,,$ $(\\hat{H}_s)_{0\\le s\\le T}$ are used to approximate $(H_s)_{0\\le s\\le T}\\,.$\nWe also define two new random variables $\\tilde{G}\\,,$ $\\hat{G}$ by\n\\begin{align}\n    \\tilde{G}:=\\frac{\\tilde{X}_T}{\\tilde{Y}_T}\\,, \\quad\n    \\hat{G}:=\\frac{\\hat{X}_T}{\\hat{Y}_T}\\mathbbm{1}_{\\{\\hat{Y}_T\\ge\\frac{1}{2}\\}}\\,\n\\end{align}\nto approximate $G$.\n\\begin{lemma}\\label{lem:h,G,H app}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $I_p$ depending only on $p$ such that the following inequalities hold. \n\\begin{enumerate}[label=(\\roman*)]\n    \\item For $0\\le t\\le 1$,\n    \\begin{equation}\n        \\mathbb{E}^\\mathbb{Q}[\\vert h_t-\\tilde{h}_t\\vert^p] \\le I_pt^p\\,, \\quad\n        \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{h}_t-\\hat{h}_t\\vert^p] \\le I_pt^p\\,.\n    \\end{equation}\n    \\item For $0\\le T\\le 1$,\n    \\begin{equation}\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert H_s-\\tilde{H}_s\\vert^p] \\le I_pT^{\\frac{p}{2}}\\,, \\quad\n        \\underset{s\\ge0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{H}_s-\\hat{H}_s\\vert^p] \\le I_pT^{\\frac{p}{2}}\\,.\n    \\end{equation}\n    \\item For $0\\le T\\le 1$,\n    \\begin{equation}\n        \\mathbb{E}^\\mathbb{Q}[\\vert G-\\tilde{G}\\vert^p] \\le I_pT^p\\,, \\quad\n        \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{G}-\\hat{G}\\vert^p] \\le I_pT^p\\,.\n    \\end{equation}\n\\end{enumerate}\n\\end{lemma}\n\\noindent Using this lemma, we can approximate the two expectations in Eq.\\eqref{eq:european delta app 1}. The approximation results are given in the following proposition.\n\\begin{proposition}\\label{prop:european delta app 1}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{enumerate}[label=(\\roman*)]\n    \\item $\\begin{aligned}\n        \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\Big[\\Phi(X_T)\\delta(h)G\\Big]=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\tilde{X}_T)\\delta(\\tilde{h})\\tilde{G}\\right]+\\mathcal{O}(\\sqrt{T}).\n    \\end{aligned}$\n    \\item $\\begin{aligned}\n        \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\tilde{X}_T)\\delta(\\tilde{h})\\tilde{G}\\right]=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\delta(\\hat{h})\\hat{G}\\right]+\\mathcal{O}(\\sqrt{T}).\n    \\end{aligned}$\n    \\item $\\begin{aligned}\n        \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(X_T)\\int_0^T h_s H_s\\,ds\\right]=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\tilde{X}_T)\\int_0^T \\tilde{h}_s \\tilde{H}_s\\,ds\\right]+\\mathcal{O}(\\sqrt{T}).\n    \\end{aligned}$\n    \\item $\\begin{aligned}\n        \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\tilde{X}_T)\\int_0^T \\tilde{h}_s \\tilde{H}_s\\,ds\\right]=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\hat{h}_s \\hat{H}_s\\,ds\\right]+\\mathcal{O}(\\sqrt{T}).\n    \\end{aligned}$\n\\end{enumerate}\n\\end{proposition}\n\\noindent Since the proofs for Lemma \\ref{lem:h,G,H app} and Proposition \\ref{prop:european delta app 1} are obtained by merely duplicating those for Lemma \\ref{lem:u,TF app} and Propositions \\ref{prop:Asian delta app 1}, \\ref{prop:Asian delta app 2}, we omit them. \\\\\n\\indent Then, it is easy to observe from Proposition \\ref{prop:european delta app 1} that for small $T>0\\,,$\n\\begin{equation}\\label{eq:european delta mal cal}\n    \\Delta_E(T)=\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\delta(\\hat{h})\\hat{G}\\right]-\\frac{\\Phi(S_0)}{S_0}+\\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\hat{h}_s \\hat{H}_s\\,ds\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\nTo obtain the asymptotic formula for $\\Delta_E(T)$, we need to estimate the following two expectations for the last step.\n\\begin{equation}\\label{eq:european delta app 2}\n    \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\delta(\\hat{h})\\hat{G}\\right]\\,, \\quad \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\hat{h}_s \\hat{H}_s\\,ds\\right]\\,.\n\\end{equation}\nAs a necessary lemma, we approximate $\\delta(\\hat{h})$ to a normal random variable $\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)$ in the following. The proof is similar to the proof for Lemma \\ref{lem:hat u app}; hence, it is omitted.\n\\begin{lemma}\\label{lem:hat h app}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ there exists a positive constant $J_p$ depending only on $p$ such that the following inequalities hold for $0\\le T\\le 1$:\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[\\vert\\delta(\\hat{h})\\vert^p]\\le J_pT^{\\frac{p}{2}}, \\quad \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\delta(\\hat{h})-\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)\\right\\vert^p\\right]\\le J_pT^p\\,.\n\\end{equation}\n\\end{lemma}\n\\noindent As Lemma \\ref{lem:hat u app} helps estimate the expectations in Eq.\\eqref{eq:asian delta mal cal 3}, this lemma enables us to estimate the expectations in Eq.\\eqref{eq:european delta app 2} in relation to multivariate normal random variables. Direct calculations with regard to normal random variables yield the following two estimates. See Appendix \\ref{proof:european delta app 2} for details.\n\\begin{proposition}\\label{prop:european delta app 2}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\\label{eq:european delta app 2.1}\n    \\begin{aligned}\n    &\\frac{1}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\delta(\\hat{h})\\hat{G}\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]\n    +\\frac{\\Phi(S_0)}{S_0T}\\int_0^T\\frac{\\sigma(s,S_0)-\\nu(s,S_0)}{\\sigma(s,S_0)}\\,ds+\\mathcal{O}(\\sqrt{T})\\,\n\\end{aligned}\n\\end{equation}\nand\n\\begin{equation}\\label{eq:european delta app 2.2}\n    \\frac{1}{S_0T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\hat{h}_s \\hat{H}_s\\,ds\\right]\n    =\\frac{\\Phi(S_0)}{S_0T}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}\\,ds+\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)$.\n\\end{proposition}\n\\noindent Therefore, we obtain the desired asymptotic formula for $\\Delta_E(T)$ by straightforward use of Proposition \\ref{prop:european delta app 2} in Eq.\\eqref{eq:european delta mal cal}.\n\n\\begin{theorem}\\label{thm:european delta}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\n    \\Delta_E(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,\n\\end{equation}\nor, equivalently,\n\\begin{equation}\n    \\Delta_E(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)-\\Phi(S_0)}{S_0\\sigma_E(T)\\sqrt{T}Z}Z^2\\right]+\\mathcal{O}(\\sqrt{T})\\,,\n\\end{equation}\nwhere $Z$ is a standard normal random variable, i.e., $Z\\sim N(0,1)$.\n\\end{theorem}\n\\noindent By comparing Theorem \\ref{thm:asian delta} with Theorem \\ref{thm:european delta}, we can observe that the limits of $\\Delta_A(T)$ and $\\Delta_E(T)$ are the same.\n\\begin{cor} Under Assumptions \\ref{classical assumption}\nand \\ref{payoff assumption}, if $\\Delta_E(T)$ converges as $T\\rightarrow{0}$, then $\\Delta_A(T)$ also converges and vice versa. Moreover, \n    \\begin{align}\n        \\lim_{T\\rightarrow{0}}\\Delta_A(T)\n        =\\lim_{T\\rightarrow{0}}\\Delta_E(T)\\,.\n    \\end{align}\n\\end{cor}\n\n\n\n\n\\section{Short-maturity options with H\\"older continuous payoffs}\\label{sec:Holder continuous}\nIn this section, we generalize Theorems \\ref{thm:option price}, \\ref{thm:asian delta}, and \\ref{thm:european delta} to the H\\"older continuous function $\\Phi\\,.$ Under Assumption \\ref{classical assumption}, the first variation process of $S$ is the unique solution of Eq.\\eqref{eq:process z}:\n\\begin{equation}\ndZ_t=(r-q)Z_t\\,dt+\\nu(t,S_t)Z_t\\,dW_t\\,, \\quad Z_0=1\\,.\n\\end{equation}\nAnalogous to Lemma \\ref{lem:x,y close}, the lemma below is crucial in the following.\n\\begin{lemma}\\label{lem:s,x close}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ as $t\\rightarrow{0}\\,,$ we have\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert^p]=\\mathcal{O}(t^p)\\,, \\quad \\mathbb{E}^\\mathbb{Q}[\\vert Z_t-Y_t\\vert^p]=\\mathcal{O}(t^p)\\,.\n\\end{equation}\n\\end{lemma}\n\\begin{proof}\nThe first equality with $p=2$ has already been proved in the proof of Lemma \\ref{lem:price drift 0} in Appendix \\ref{proof:price drift negligible}. The remainder of the proof is the same as that of Lemma \\ref{lem:x,y close} in Appendix \\ref{proof:x,y close}.\n\\end{proof}\n\\noindent This section considers H\\"older continuous payoffs in the following order. In Section \\ref{subsec:holder price}, the asymptotic for option prices in Theorem \\ref{thm:option price} will be generalized. Estimates for the option delta value in Theorems \\ref{thm:asian delta} and \\ref{thm:european delta} are generalized to H\\"older continuous payoffs in Section \\ref{subsec:holder delta}.\n\\subsection{Estimates for option prices}\\label{subsec:holder price}\nThe following lemma is a generalization of Lemma \\ref{lem:price drift 0}.\n\\begin{lemma}\\label{lem:holder price drift 0}\nUnder Assumptions \\ref{classical assumption}\nand \\ref{Holder payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{enumerate}[label=(\\roman*)]\n    \\item $ \\begin{aligned} P_A(T)=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]+\\mathcal{O}(T^{\\gamma})\\,,\\end{aligned}$\n    \\item $ \\begin{aligned} P_E(T)=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)]+\\mathcal{O}(T^{\\gamma})\\,. \\end{aligned}$\n\\end{enumerate}\nHere, $\\gamma$ is the H\\"older exponent in Assumption \\ref{Holder payoff assumption}.\n\\end{lemma}\n\\begin{proof}\nChoose $q>1$ such that $\\gamma q>1\\,.$ Then, by the Jensen inequality and Assumption \\ref{Holder payoff assumption},\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)-\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right] \\\\\n    &\\le \\beta\\,\\,\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\vert S_t-X_t \\vert\\,dt\\right)^{\\gamma}\\right]\n    \\le \\beta\\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert^{\\gamma q}]\\,dt\\right)^{\\frac{1}{q}}\\,.\n\\end{align}\nThen, the remainder of the proof comes from Lemma \\ref{lem:s,x close}.\n\\end{proof}\n\\noindent Now, we get the asymptotic estimates for the prices of options having H\\"older continuous payoffs.\n\\begin{theorem}\\label{thm:option price holder}\nUnder Assumptions \\ref{classical assumption} and \\ref{Holder payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\n    P_A(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)]+\\mathcal{O}(T^{\\gamma})\\,,\\quad\nP_E(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)]+\\mathcal{O}(T^{\\gamma})\\,.\n\\end{equation}\nHere, $\\gamma$ is the H\\"older exponent in Assumption \\ref{Holder payoff assumption}.\n\\end{theorem}\n\\begin{proof}\nThe proof is straightforward from the proofs of Theorems \\ref{thm:option price} and \\ref{lem:holder price drift 0}.\n\\end{proof}\nCompared to the Lipschitz continuous case $(\\gamma=1)\\,,$ the convergence order is degraded for $\\gamma< 1\\,.$ However, the following example shows that this asymptotic relation is optimal in general.\n\\begin{ex}\nGiven any $K$ and $0<\\gamma \\le 1\\,,$ define the payoff function $\\Phi$ by\n\\begin{equation}\n    \\Phi(x)=(x-K)_{+}^{\\gamma}\\,.\n\\end{equation}\nSuppose that $S_0=K\\,.$ Then, we get the following asymptotic equation.\n\\begin{equation}\n    P_A(T)=\\frac{1}{2}(S_0\\sigma_A(T))^{\\gamma}M(\\gamma)T^{\\frac{\\gamma}{2}}+\\mathcal{O}(T^{\\gamma})\\,,\n\\end{equation}\nwhere $M(\\gamma):=\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^{\\gamma}]$ with a standard normal variable $Z\\,.$ If we replace $\\sigma_A(T)$ with $\\sigma_E(T)\\,,$ we get the asymptotic result for the European option price $P_E(T)\\,.$\n\\end{ex}\n\\subsection{Estimates for option delta values}\\label{subsec:holder delta}\nIn this section, we investigate the short-maturity option delta values when $\\Phi$ is any H\\"older continuous function. The main results are as follows:\n\\begin{equation}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,\n\\end{equation}\nand\n\\begin{equation}\n    \\Delta_E(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,,\n\\end{equation}\nwhere $Z$ denotes a standard normal variable and $\\gamma$ is the H\\"older exponent in Assumption \\ref{Holder payoff assumption}. \\\\\n\\indent The proof begins by recognizing the changes in Lemma \\ref{lem:delta drift 0} from Section \\ref{sec:Short maturity asymptotic for a sensitivity}. In the proof of Lemma \\ref{lem:delta drift 0}, we make use of the fact that any Lipschitz continuous function is almost everywhere differentiable with respect to the Lebesgue measure. However, this condition fails for arbitrary $\\gamma$-H\\"older continuous functions in general. Therefore, we should rely only on the Malliavin representation of the option delta (see Proposition \\ref{prop:asian delta mal}) for the approximation. First, we examine the following Malliavin representation for $\\Delta_A(T)\\,.$\n\\begin{proposition}\\label{prop:asian delta mal holder}\nFor the process $S$ stated in Eq.\\eqref{eq:S_t}, under Assumption \\ref{classical assumption}, we have\n\\begin{align}\n    \\Delta_A(T)\n    &=e^{-rT}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\delta\\left(\\frac{2{Z_{\\cdot}}^2}{\\sigma(\\cdot,S_{\\cdot})S_{\\cdot}}\\right)\\frac{1}{\\int_0^T Z_t\\,dt}\\right] \\\\\n    &\\quad-e^{-rT}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\int_0^T \\frac{2{Z_u}^2}{\\sigma(u,S_u)S_u}D_{u}\\left(\\frac{1}{\\int_0^T Z_t\\,dt}\\right)\\,du\\right]\\,,\n\\end{align}\nwhere $\\delta(\\cdot)$ is the Skorokhod integral in $[0,T]$ and $D_s(\\cdot)$ is the Malliavin derivative.\n\\end{proposition}\n\\begin{proof}\nSee \\cite{NualartDavid1995TMca,benhamou2000application}.\n\\end{proof}\n\\noindent As we approximated $u$ by $\\tilde{u}$ and $\\hat{u}$ in Lemma \\ref{lem:u,TF app}, we would approximate a process $\\frac{2Z_{\\cdot}^2}{\\sigma(\\cdot, S_{\\cdot})S_{\\cdot}}$ by $u\\,.$ Likewise, a random variable $\\frac{1}{\\int_0^T Z_t\\,dt}$ is approximated to $F\\,.$ Examine following Lemmas  \\ref{lem:u app holder} and \\ref{lem:TF app holder} in comparison to Lemmas \\ref{lem:u,TF app}, \\ref{lem:Ds bound}, and \\ref{lem:Ds close}.\n\\begin{lemma}\\label{lem:u app holder}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ as $t\\rightarrow{0}$, we have\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2Z^2_t}{\\sigma(t,S_t)S_t}-u_t\\right\\vert^p\\right]=\\mathcal{O}(t^p).\n\\end{equation}\n\\end{lemma}\n\\begin{proof}\nSame as the proof of Lemma \\ref{lem:u,TF app}. Use Lemma \\ref{lem:s,x close} instead of Lemma \\ref{lem:x,y close}.\n\\end{proof}\n\\begin{lemma}\\label{lem:TF app holder}\nUnder Assumption \\ref{classical assumption}, for any $p>0\\,,$ as $T\\rightarrow{0}$, we have\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{1}{\\frac{1}{T}\\int_0^T Z_t\\,dt}-TF\\right\\vert^p\\right]=\\mathcal{O}(T^p)\\, \\quad \\sup_{s\\ge 0}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert D_s\\left(\\frac{1}{\\frac{1}{T}\\int_0^T Z_t\\,dt}\\right)-TD_sF\\right\\vert^p\\right]=\\mathcal{O}(T^p)\n\\end{equation}\n\\end{lemma}\n\\begin{proof}\nExamine the following Malliavin derivatives:\n\\begin{equation}\n    D_uS_l=\\frac{Z_l}{Z_u}\\sigma(u,S_u)S_u\\mathbbm{1}_{\\{u\\le l\\}}\\,,\n\\end{equation}\n\\begin{equation}\n    D_uZ_t=Z_t\\left[\\nu(u,S_u)-\\int_0^t\\nu(l,S_l)\\rho(l,S_l)D_uS_l\\,dl+\\int_0^t\\rho(l,S_l)D_uS_l\\,dW_l\\right]\\mathbbm{1}_{\\{u\\le t\\}}\\,.\n\\end{equation}\nThe remainder of the proof is similar to the proofs of Lemmas \\ref{lem:u,TF app}--\\ref{lem:Ds close}. Use Lemma \\ref{lem:s,x close} instead of Lemma \\ref{lem:x,y close}. \n\\end{proof}\n\\noindent Through minor changes in Propositions \\ref{prop:Asian delta app 1} and \\ref{prop:Asian delta app 2}, we obtain the generalized version of Lemma \\ref{lem:delta drift 0}.\n\\begin{lemma}\nUnder Assumptions \\ref{classical assumption} and \\ref{Holder payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\n    \\Delta_A(T)=e^{-rT}\\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,.\n\\end{equation}\n\\end{lemma}\n\\begin{proof}\nApply Lemmas \\ref{lem:u app holder} and \\ref{lem:TF app holder} to the proofs of Propositions \\ref{prop:Asian delta app 1} and \\ref{prop:Asian delta app 2} instead of Lemmas \\ref{lem:u,TF app}--\\ref{lem:Ds close}.\n\\end{proof}\nThis section is devoted to the following result, which is the generalization of Theorems \\ref{thm:asian delta} and \\ref{thm:european delta}.\n\\begin{theorem}\\label{thm:delta formula holder}\nUnder Assumptions \\ref{classical assumption} and \\ref{Holder payoff assumption}, as $T\\rightarrow{0}$, we have\n\\begin{equation}\n    \\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,\n\\end{equation}\nand\n\\begin{equation}\n    \\Delta_E(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,.\n\\end{equation}\n\\end{theorem}\n\\begin{proof}\nMinor changes to the convergence rates in Propositions \\ref{prop:Asian delta app 1}--\\ref{prop:Asian delta app 3} give the first asymptotic. For the European delta, we duplicate the arguments in this section.\n\\end{proof}\n\\begin{remark}\nThe borderline is $\\gamma=\\frac{1}{2}$ in these formulas. If $\\gamma<\\frac{1}{2}\\,,$ the estimates in Theorem \\ref{thm:delta formula holder} are meaningless; however, for $\\frac{1}{2}<\\gamma\\le 1\\,,$ they provide us with the short-maturity estimate with the convergence rate $\\gamma-\\frac{1}{2}\\le \\frac{1}{2}\\,.$\n\\end{remark}\n\\begin{ex}\nGiven any $K$ and $\\frac{1}{2}<\\gamma<1\\,,$ define the payoff function $\\Phi$ by\n\\begin{equation}\n    \\Phi(x)=(x-K)_{+}^{\\gamma}\\,.\n\\end{equation}\nSuppose that $S_0=K\\,.$ Then, we get the following asymptotic equation.\n\\begin{equation}\n    \\Delta_A(T)=\\frac{M(1+\\gamma)}{2(S_0\\sigma_A(T))^{1-\\gamma}}\\times\\frac{1}{T^{\\frac{1-\\gamma}{2}}}+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,,\n\\end{equation}\nwhere $M(1+\\gamma):=\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^{1+\\gamma}]$ with a standard normal variable $Z\\,.$\n\\end{ex}\n\\section{Comparison between volatilities at short maturity}\n\\label{sec:Comparison between volatilities at short maturity}\nTo emphasize the dependence on the payoff function $\\Phi$ and the volatility function $\\sigma(t,x)\\,,$ we denote the option price $P_A(T)\\,,$ $P_E(T)$ and the option delta value $\\Delta_A(T)\\,,$ $\\Delta_E(T)$ by $P_A(T;\\Phi\\,, \\sigma)\\,,$ $P_E(T;\\Phi\\,, \\sigma)$ and $\\Delta_A(T;\\Phi\\,, \\sigma)\\,,$ $\\Delta_E(T;\\Phi\\,, \\sigma)\\,,$ respectively. Since the Asian and European volatilities $\\sigma_A(T)\\,,$ $\\sigma_E(T)$ defined in Eq.\\eqref{def:asian european volatility} also depend on the volatility function $\\sigma(t,x)$ , we denote them by $\\sigma_A(T;\\sigma)\\,,$ $\\sigma_E(T;\\sigma)\\,.$\n\n\\subsection{Comparison under the general local volatility model}\nIn practice, Asian-style options are mainly quoted by their prices, not by their implied volatilities. This is due to the lack of a simple closed-form formula for the density of $\\frac{1}{T}\\int_0^T S_t\\,dt\\,.$ Instead, many practitioners estimate the implied volatilities of European-style options for pricing and hedging purposes. \\\\\n\\indent Let us focus on the situation in which the volatility function $\\sigma(t,x)$ of the underlying process Eq.\\eqref{eq:S_t} is calibrated to match market data on European-style options. Denote this calibrated volatility by $\\sigma_{\\textnormal{Implied}}(t,x)\\,.$ In this section, we aim to price and hedge the Asian option under the volatility function $\\sigma_{\\textnormal{Implied}}(t,x)\\,.$ Note from the asymptotic formulas established in Theorems \\ref{thm:option price}, \\ref{thm:asian delta}, and \\ref{thm:european delta} that if a function $\\tau(t,x)$ satisfies Assumption \\ref{classical assumption} and the equality\n\\begin{equation}\\label{eq:implied match}\n    \\sigma_A(T;\\sigma_{\\textnormal{Implied}})=\\sigma_E(T;\\tau)\n\\end{equation}\nfor sufficiently small $T>0\\,,$ then for any Lipschitz continuous function $\\Phi(\\cdot)\\,,$\n\\begin{equation}\\label{eq:when tau matched}\n    P_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})=P_E(T;\\Phi, \\tau)+\\mathcal{O}(T)\\,, \\quad\n    \\Delta_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})=\\Delta_E(T;\\Phi, \\tau)+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\nMeanwhile, if the equality Eq.\\eqref{eq:implied match} fails in any neighborhood of $T=0$ and, heuristically speaking, $\\tau$ deviates excessively from $\\sigma_{\\textnormal{Implied}}\\,,$ we will see in Propositions \\ref{prop:price optimal general} and \\ref{prop:delta optimal general} that the convergence rates in Eq.\\eqref{eq:when tau matched} are degraded in general. In this sense, we may regard the European option under the volatility $\\tau(t,x)$ satisfying Eq.\\eqref{eq:implied match} as a short-maturity proxy for the Asian option under the volatility $\\sigma_{\\textnormal{Implied}}(t,x).$\n\n\\begin{proposition}\\label{prop:price optimal general} Consider any volatility function $\\tau$ that satisfies Assumption \\ref{classical assumption}.\n\\begin{enumerate}[label=(\\roman*)]\n\\item For any payoff function $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n\\begin{equation}\n        P_A(T;\\Phi,\\sigma_{\\textnormal{Implied}})=P_E(T;\\Phi,\\tau)\\,+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\n\\item Suppose that $\\limsup_{T\\rightarrow{0}}\\vert \\sigma_A(T;\\sigma_{\\textnormal{Implied}})-\\sigma_E(T;\\tau) \\vert\\ne 0\\,.$ Then, for any $0<\\epsilon\\le \\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n\\begin{equation}\\label{eq:phi epsilon}\n    \\vert P_A(T;\\Phi_{\\epsilon},\\sigma_{\\textnormal{Implied}})\n    -P_E(T;\\Phi_{\\epsilon},\\tau)\\vert\\ne \\mathcal{O}(T^{\\frac{1}{2}+\\epsilon})\\,.\n\\end{equation}\n\\item By contrast, suppose that $\\sigma_A(T;\\sigma_{\\textnormal{Implied}})=\\sigma_E(T;\\tau)$ for small $T>0\\,.$ Then, for any $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n\\begin{equation}\n    P_A(T;\\Phi,\\sigma_{\\textnormal{Implied}})=P_E(T;\\Phi,\\tau)+\\mathcal{O}(T)\\,.\n\\end{equation}\n\\end{enumerate}\n\\end{proposition}\n\\begin{proof}\nExcept for Eq.\\eqref{eq:phi epsilon}, the remainder of the proof is obvious from Theorem \\ref{thm:option price}. Take $\\Phi_{\\epsilon}$ as\n\\begin{equation}\n    \\Phi_{\\epsilon}(x):=(x-K)^{1+\\epsilon}\\mathbbm{1}_{\\{K\\le x< 2K\\}}+(2K)^{1+\\epsilon}\\mathbbm{1}_{\\{2k\\le x\\}}\\,,\n\\end{equation}\nwith $K=S_0\\,.$ Then, Eq.\\eqref{eq:phi epsilon} easily follows. \n\\end{proof}\n\n\\begin{proposition}\\label{prop:delta optimal general} Consider any volatility function $\\tau$ that satisfies Assumption \\ref{classical assumption}.\n\\begin{enumerate}[label=(\\roman*)]\n\\item For any payoff function $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n\\begin{equation}\n        \\Delta_A(T;\\Phi,\\sigma_{\\textnormal{Implied}})=\\Delta_E(T;\\Phi,\\tau)\\,+\\mathcal{O}(1)\\,.\n\\end{equation}\n\\item Suppose that $\\limsup_{T\\rightarrow{0}}\\vert \\sigma_A(T;\\sigma_{\\textnormal{Implied}})-\\sigma_E(T;\\tau) \\vert\\ne 0\\,.$ Then, for any $0<\\epsilon\\le\\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n\\begin{equation}\n    \\vert \\Delta_A(T;\\Phi_{\\epsilon},\\sigma_{\\textnormal{Implied}})\n    -\\Delta_E(T;\\Phi_{\\epsilon},\\tau)\\vert\\ne \\mathcal{O}(T^{\\epsilon})\\,.\n\\end{equation}\n\\item By contrast, suppose that $\\sigma_A(T;\\sigma_{\\textnormal{Implied}})=\\sigma_E(T;\\tau)$ for small $T>0\\,.$ Then, for any $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n\\begin{equation}\n    \\Delta_A(T;\\Phi,\\sigma_{\\textnormal{Implied}})=\\Delta_E(T;\\Phi,\\tau)+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\n\\end{enumerate}\n\\end{proposition}\n\\begin{proof}\nSame as that of Proposition \\ref{prop:price optimal general}.\n\\end{proof}\n\\begin{remark}\nSuppose that $s\\mapsto\\sigma_{\\textnormal{Implied}}(s,S_0)$ and $s\\mapsto\\tau(s,S_0)$ are both continuous at $s=0.$ Then, the condition $\\limsup_{T\\rightarrow{0}}\\vert \\sigma_A(T;\\sigma_{\\textnormal{Implied}})-\\sigma_E(T;\\tau) \\vert\\ne 0$ is equivalent to $$\\tau(0,S_0)\\ne \\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}(0,S_0)\\,,$$\nbecause $\\lim_{T\\rightarrow{0}}\\sigma_A(T;\\sigma_{\\textnormal{Implied}})=\\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}(0,S_0)$ and $\\lim_{T\\rightarrow{0}}\\sigma_E(T;\\tau)=\\tau(0,S_0)\\,.$ These limits coincide with the well-known result that ATM Asian vol $=\\frac{1}{\\sqrt{3}}\\cdot$ ATM European vol. See \\cite{PirjolDan2016SMAO}.\n\\end{remark}\nIf some suitable technical condition is satisfied for $s\\mapsto\\sigma_{\\textnormal{Implied}}(s,S_0)\\,,$ then Eq.\\eqref{eq:implied match} forces $\\tau$ to be determined uniquely. Hence, whenever $\\sigma_{\\textnormal{Implied}}$ is given, we can always approximate the Asian option having volatility $\\sigma_{\\textnormal{Implied}}$ by the European option having volatility $\\tau\\,.$\n\n\\begin{proposition}\nSuppose that $\\tau$ satisfies Assumption \\ref{classical assumption} and Eq.\\eqref{eq:implied match}. If $s\\mapsto\\sigma_{\\textnormal{Implied}}(s,S_0)$ is continuous in some neighborhood of $s=0\\,,$ say $[0,\\delta]\\,,$ then $s\\mapsto\\tau(s,S_0)$ is uniquely determined in $[0,\\delta]$ by\n\\begin{align}\n    \\tau(s,S_0)=\\left[\\frac{2}{s^3}\\int_0^s \\sigma_{\\textnormal{Implied}}^2(u,S_0)(us-u^2)\\,du\\right]^{\\frac{1}{2}}\\,.\n\\end{align}\nConversely, if $s\\mapsto\\tau(s,S_0)$ is of $C^2[0,\\delta]$, then the only choice of a continuous function $s\\mapsto\\sigma_{\\textnormal{Implied}}(s,S_0)$ in $[0,\\delta]$ is\n\\begin{align}\n    \\sigma_{\\textnormal{Implied}}(s,S_0)=\\tau(s,S_0)\\left[3+6s\\frac{\\tau^{\'}(s,S_0)}{\\tau(s,S_0)}+s^2\\left(\\frac{\\tau^{\'}(s,S_0)}{\\tau(s,S_0)}\\right)^2+s^2\\frac{\\tau^{\'\'}(s,S_0)}{\\tau(s,S_0)}\\right]^{\\frac{1}{2}}\\,.\n\\end{align}\n\\end{proposition}\n\\begin{proof}\nDifferentiate both sides of Eq.\\eqref{eq:implied match} by $T\\,.$\n\\end{proof}\n\n\\subsection{Comparison under the Black--Scholes model}\nIn this section, we focus on the Black--Scholes model, i.e., $\\sigma(t,x)\\equiv\\sigma$. Observe that under the Black--Scholes model, $\\sigma_A(T;\\sigma)=\\frac{\\sigma}{\\sqrt{3}}$ and $\\sigma_E(T;\\sigma)=\\sigma$. Now, consider a new option, i.e., the so-called {\\em geometric average Asian option}, whose price is given by\n\\begin{align}\n    P_G^{\\textnormal{BS}}(T;\\Phi,\\sigma):=e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(e^{\\frac{1}{T}\\int_0^T\\log S_t\\,dt}\\right)\\right]\\,.\n\\end{align}\nHere, the superscript $\\textnormal{BS}$ is used to emphasize the Black--Scholes model. We denote its delta value by $\\Delta_G^{\\textnormal{BS}}(T;\\Phi,\\sigma)\\,.$ \\\\\n\\indent Let us confine ourselves to the situation in which a constant $\\sigma_{\\textnormal{Implied}}$ is obtained from the European option. We want to approximate the Asian option price $P^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})$ and its delta value $\\Delta^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})$ by their European and geometric average Asian counterparts. In Propositions \\ref{prop:price optimal} and \\ref{prop:delta optimal}, we observe that the European option having volatility $\\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}$ and the geometric average Asian option having volatility $\\sigma_{\\textnormal{Implied}}$ are optimal choices for the asymptotic approximation.\n\\begin{proposition}\\label{prop:price optimal}\nConsider any constant volatility $\\tau>0\\,.$\n\\begin{enumerate}[label=(\\roman*)]\n    \\item For any payoff function $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n    \\begin{align}\n    &P^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})\n    =P^{\\textnormal{BS}}_E(T;\\Phi, \\tau)+\\mathcal{O}(\\sqrt{T})\\,, \\\\\n    &P^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})\n    =P^{\\textnormal{BS}}_G(T;\\Phi, \\tau)+\\mathcal{O}(\\sqrt{T})\\,.\n    \\end{align}\n    \\item Suppose that $\\tau\\ne\\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}\\,.$ Then, for any $0<\\epsilon\\le\\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n    \\begin{equation}\n        \\vert P^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})-P^{\\textnormal{BS}}_E(T;\\Phi_{\\epsilon}, \\tau) \\vert\\ne\\mathcal{O}(T^{\\frac{1}{2}+\\epsilon})\\,.\n    \\end{equation}\n    Likewise, if $\\tau\\ne\\sigma_{\\textnormal{Implied}}$ and $0<\\epsilon\\le\\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n    \\begin{equation}\n        \\vert P^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})-P^{\\textnormal{BS}}_G(T;\\Phi_{\\epsilon}, \\tau) \\vert\\ne\\mathcal{O}(T^{\\frac{1}{2}+\\epsilon})\\,.\n    \\end{equation}\n    \\item By contrast, for any $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n    \\begin{align}\n       &P^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})=P^{\\textnormal{BS}}_E\\left(T;\\Phi_{\\epsilon}, \\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}\\right)+\\mathcal{O}(T)\\,, \\\\\n      &P^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})=P^{\\textnormal{BS}}_G(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})+\\mathcal{O}(T)\\,.\n    \\end{align}\n\\end{enumerate}\n\\end{proposition}\n\\begin{proof}\nUnder the Black--Scholes model, $e^{\\frac{1}{T}\\int_0^T\\log S_t\\,dt}$ is a log-normal random variable. Hence, it is easy to check from Lemma \\ref{lem:price drift 0} that\n\\begin{equation}\n    P^{\\textnormal{BS}}_G(T;\\Phi, \\tau)=P^{\\textnormal{BS}}_E\\left(T;\\Phi, \\frac{1}{\\sqrt{3}}\\tau\\right)+\\mathcal{O}(T)\\,\n\\end{equation}\nfor any positive constant $\\tau$ and $\\Phi$ satisfying Assumption \\ref{payoff assumption}. The remainder of the proof is the same as that of Proposition \\ref{prop:price optimal general}.\n\\end{proof}\n\\begin{proposition}\\label{prop:delta optimal}\nConsider any constant volatility $\\tau>0\\,.$\n\\begin{enumerate}[label=(\\roman*)]\n    \\item For any payoff function $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n    \\begin{align}\n    &\\Delta^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})\n    =\\Delta^{\\textnormal{BS}}_E(T;\\Phi, \\tau)+\\mathcal{O}(1)\\,, \\\\\n    &\\Delta^{\\textnormal{BS}}_A(T;\\Phi, \\sigma_{\\textnormal{Implied}})\n    =\\Delta^{\\textnormal{BS}}_G(T;\\Phi, \\tau)+\\mathcal{O}(1)\\,.\n    \\end{align}\n    \\item Suppose that $\\tau\\ne\\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}\\,.$ Then, for any $0<\\epsilon\\le\\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n    \\begin{equation}\n        \\vert \\Delta^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})-\\Delta^{\\textnormal{BS}}_E(T;\\Phi_{\\epsilon}, \\tau) \\vert\\ne\\mathcal{O}(T^{\\epsilon})\\,.\n    \\end{equation}\n    Likewise, if $\\tau\\ne\\sigma_{\\textnormal{Implied}}$ and $0<\\epsilon\\le\\frac{1}{2}\\,,$ there exists a payoff function $\\Phi_{\\epsilon}$ that satisfies Assumption \\ref{payoff assumption} such that\n    \\begin{equation}\n        \\vert \\Delta^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})-\\Delta^{\\textnormal{BS}}_G(T;\\Phi_{\\epsilon}, \\tau) \\vert\\ne\\mathcal{O}(T^{\\epsilon})\\,.\n    \\end{equation}\n    \\item By contrast, for any $\\Phi$ that satisfies Assumption \\ref{payoff assumption},\n    \\begin{align}\n       &\\Delta^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})=\\Delta^{\\textnormal{BS}}_E\\left(T;\\Phi_{\\epsilon}, \\frac{1}{\\sqrt{3}}\\sigma_{\\textnormal{Implied}}\\right)+\\mathcal{O}(\\sqrt{T})\\,, \\\\\n      &\\Delta^{\\textnormal{BS}}_A(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})=\\Delta^{\\textnormal{BS}}_G(T;\\Phi_{\\epsilon}, \\sigma_{\\textnormal{Implied}})+\\mathcal{O}(\\sqrt{T})\\,.\n    \\end{align}\n\\end{enumerate}\n\\end{proposition}\n\\begin{proof}\nFrom Lemma \\ref{lem:delta drift 0} and Lemma \\ref{lem:another repre delta} in its proof, it is easy to check that\n\\begin{equation}\n    \\Delta^{\\textnormal{BS}}_G(T;\\Phi, \\tau)=\\Delta^{\\textnormal{BS}}_E\\left(T;\\Phi, \\frac{1}{\\sqrt{3}}\\tau\\right)+\\mathcal{O}(\\sqrt{T})\\,\n\\end{equation}\nfor any positive constant $\\tau$ and $\\Phi$ satisfying Assumption \\ref{payoff assumption}. The remainder of the proof is the same as that of Proposition \\ref{prop:delta optimal general}.\n\\end{proof}\n\\begin{remark}\nConsider any positive constant $\\tau\\,.$ Even though $P^{\\textnormal{BS}}_A(T;\\Phi,\\tau)\\,,$ $P^{\\textnormal{BS}}_G(T;\\Phi,\\tau)\\,,$ $P^{\\textnormal{BS}}_E(T;\\Phi,\\tau)\\,$ as well as $\\Delta^{\\textnormal{BS}}_A(T;\\Phi,\\tau)\\,,$ $\\Delta^{\\textnormal{BS}}_G(T;\\Phi,\\tau)\\,,$ $\\Delta^{\\textnormal{BS}}_E(T;\\Phi,\\tau)\\,$ share the same limit as $T\\rightarrow{0}$ from Corollaries \\ref{cor:option price limit same} and \\ref{cor:asian delta limit}, Propositions \\ref{prop:price optimal} and \\ref{prop:delta optimal} argue that the Asian option is more ``close\'\' to the geometric average Asian option than to the European option. \n\\end{remark}\n\n\n\n\\section{Special case: Approximation for call and put options}\n\\label{sec:Special case}\nThis section only considers the Asian call option, i.e., $\\Phi(x)=(x-K)_{+}$, and the Asian put option, i.e., $\\Phi(x)=(K-x)_{+}\\,.$ The meanings of the following notations are self-explanatory:\n\\begin{equation}\n    P^{\\textnormal{call}}_A(T)\\,,\\,\\, P^{\\textnormal{put}}_A(T)\\,,\\,\\, \\Delta^{\\textnormal{call}}_A(T)\\,,\\,\\, \\Delta^{\\textnormal{put}}_A(T)\\,.\n\\end{equation}\nThe short-maturity behaviors of these four quantities have already been examined in Examples \\ref{ex:call,put price} and \\ref{ex:call,put delta}. However, this section uses the large deviation principle to provide additional information about their behaviors.\n\\subsection{Application of the large deviation principle}\nConsider the model where the volatility function $\\sigma(t,x)$ is independent of $t$. In other words, $\\sigma(t,x)\\equiv\\sigma(x)$. Besides Assumption \\ref{classical assumption}, let us impose the following assumption on the volatility function $\\sigma(x)\\,.$\n\\begin{assume}\\label{otm assumption}\nThere are constants $M>0$ and $\\gamma>0$ such that for any $x,y\\in \\mathbb{R}$,\n\\begin{equation}\n    \\vert\\sigma(e^x)-\\sigma(e^y)\\vert\\le M\\vert x-y\\vert^{\\gamma}\\,.\n\\end{equation}\n\\end{assume}\n\\indent Under Assumptions \\ref{classical assumption} and \\ref{otm assumption}, the following short-maturity asymptotic results for $P^{\\textnormal{call}}_A(T)$ and $P^{\\textnormal{put}}_E(T)$ were first proved in \\cite{PirjolDan2016SMAO}.\n\\begin{theorem}[Pirjol, D. and L. Zhu \\cite{PirjolDan2016SMAO}]\\label{thm:ldp price}\nUnder Assumptions \\ref{classical assumption} and \\ref{otm assumption}, the following hold.\n\\begin{enumerate}[label=(\\roman*)]\n    \\item For an OTM Asian call option, i.e., $K>S_0$,\n    \\begin{equation}\n        \\lim_{T\\rightarrow0}T\\log(P_A^{\\textnormal{call}}(T))=-\\mathcal{I}(K,S_0)\\,.\n    \\end{equation}\n    \\item For an OTM Asian put option, i.e., $S_0>K$,\n    \\begin{equation}\n        \\lim_{T\\rightarrow0}T\\log(P_A^{\\textnormal{put}}(T))=-\\mathcal{I}(K,S_0)\\,.\n    \\end{equation}\n\\end{enumerate}\nHere, for any $x\\,, y>0$, the rate function $\\mathcal{I}$ is defined by\n\\begin{equation}\\label{eq:rate function}\n    \\mathcal{I}(x\\,, y):=\\underset{\\substack{\\int_0^1e^{g(t)}\\,dt=x, \\\\ g(0)=\\log y,\\, g\\in\\mathcal{AC}[0,1]}}{\\inf}\\frac{1}{2}\\int_0^1\\,\\left(\\frac{g^{\'}(t)}{\\sigma(e^{g(t)})}\\right)^2dt\\,,\n\\end{equation}\nwhere $\\mathcal{AC}[0,1]$ is the space of absolutely continuous functions on $[0,1]$.\n\\end{theorem}\n\\begin{remark}\\label{rmk:ldp} Note that the rate function $\\mathcal{I}$ in \\cite{PirjolDan2016SMAO} comes from the large deviation principle. According to the large deviation principle, for any Borel set $A$ in $\\mathbb{R}^{+}\\,,$\n\\begin{align}\n    -\\inf_{x\\in A^{\\circ}}\\mathcal{I}(x,S_0)\n    &\\le \\liminf_{T\\rightarrow{0}}T\\log\\left(\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T S_t\\,dt\\in A\\right\\}\\right) \\\\\n    &\\quad \\le \\limsup_{T\\rightarrow{0}}T\\log\\left(\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T S_t\\,dt\\in A\\right\\}\\right)\n    \\le -\\inf_{x\\in \\overline{A}}\\mathcal{I}(x,S_0)\\,,\n\\end{align}\nwhere $A^{\\circ}$ is the interior of $A$ and $\\overline{A}$ is the closure of $A\\,.$ See \\cite{DemboAmir1998Ldta, PirjolDan2016SMAO} for details.\n\\end{remark}\n\\noindent By solving the variational problem on the right-hand side of Eq.\\eqref{eq:rate function}, the following property of the rate function $\\mathcal{I}$ was proposed in \\cite{PirjolDan2016SMAO}.\n\\begin{proposition}[Pirjol, D. and L. Zhu \\cite{PirjolDan2016SMAO}]\\label{prop:rate function}\nGiven any $y>0$, $x\\mapsto \\mathcal{I}(x,y)$ is a continuous map that is monotone decreasing in $(-\\infty, y]$ and monotone increasing in $[y,\\infty)$.\n\\end{proposition}\n\\subsection{Short-maturity asymptotic for the Asian call and put option delta value}\nSimilarly to \\cite{PirjolDan2016SMAO}, we use the large deviation theory to examine the short-maturity asymptotic for $\\Delta^{\\textnormal{call}}_A(T)$ and $\\Delta^{\\textnormal{put}}_E(T)\\,.$ See Appendix \\ref{proof:otm delta} for the proof.\n\\begin{theorem}\\label{thm:otm delta}\nUnder Assumptions \\ref{classical assumption} and \\ref{otm assumption}, the following hold for the rate function $\\mathcal{I}$ defined by Eq.\\eqref{eq:rate function}.\n\\begin{enumerate}[label=(\\roman*)]\n\\item For an OTM Asian call option, i.e., $K>S_0$,\n\\begin{equation}\\label{eq:otm cdelta}\n \\lim_{T\\rightarrow0}T\\log(\\Delta_A^{\\textnormal{call}}(T))=-\\mathcal{I}(K,S_0)\\,.\n\\end{equation}\n\\item For an OTM Asian put option, i.e., $S_0>K$,\n\\begin{equation}\\label{eq:otm pdelta}\n \\lim_{T\\rightarrow0}T\\log(-\\Delta_A^{\\textnormal{put}}(T))=-\\mathcal{I}(K,S_0)\\,.\n\\end{equation}\n\\end{enumerate}\n\\end{theorem}\n\\noindent As a corollary of Theorem \\ref{thm:otm delta}, we can approximate ITM Asian call and put option delta values.\n\\begin{cor}\\label{cor:itm delta}\nUnder Assumptions \\ref{classical assumption} and \\ref{otm assumption}, the following asymptotic relations hold as $T\\rightarrow{0}\\,.$\n\\begin{enumerate}[label=(\\roman*)]\n\\item For an ITM Asian call option, i.e., $S_0>K$,\n\\begin{align}\n    \\Delta_A^{\\textnormal{call}}(T)=1-\\frac{1}{2}(r+q)T+\\left(\\frac{r^2+rq+q^2}{6}\\right)T^2+\\mathcal{O}(T^3)\\,.\n\\end{align}\n\\item For an ITM Asian put option, i.e., $K>S_0$,\n\\begin{align}\n    \\Delta_A^{\\textnormal{put}}(T)=-1+\\frac{1}{2}(r+q)T-\\left(\\frac{r^2+rq+q^2}{6}\\right)T^2+\\mathcal{O}(T^3)\\,.\n\\end{align}\n\\end{enumerate}\n\\end{cor}\n\\begin{proof}\nFrom Lemma \\ref{lem:another repre delta} in Appendix \\ref{proof:delta drift 0}, we can get the put-call parity for the Asian option delta value:\n\\begin{equation}\n    \\Delta_A^{\\textnormal{call}}(T)-\\Delta_A^{\\textnormal{put}}(T)\n    =\\frac{e^{-rT}}{S_0}\\frac{1}{T}\\int_{0}^{T} \\mathbb{E}^\\mathbb{Q}[S_t]\\,dt\n    =\\frac{e^{-rT}}{S_0}\\frac{1}{T}\\int_{0}^{T} S_0e^{(r-q)t}\\,dt\n    =\\frac{e^{-qT}-e^{-rT}}{(r-q)T}\\,.\n\\end{equation}\nFrom Theorem \\ref{thm:otm delta}, the OTM Asian delta value vanishes at an exponential rate. Therefore, the Taylor expansion\n\\begin{equation}\n     \\frac{e^{-qT}-e^{-rT}}{(r-q)T}=1-\\frac{1}{2}(r+q)T+\\left(\\frac{r^2+rq+q^2}{6}\\right)T^2+\\mathcal{O}(T^3)\\,\n\\end{equation}\ngives Corollary \\ref{cor:itm delta}.\n\\end{proof}\n\\begin{remark}\nTheorem \\ref{thm:otm delta} and its Corollary \\ref{cor:itm delta} are obtained from direct use of the large deviation theory. This is different from the method used in \\cite{PirjolDan2018SOAO}. More precisely, \\cite{PirjolDan2018SOAO} involved a sensitivity analysis of  approximated option prices, not true option prices.\n\\end{remark}\n\\begin{remark}\nObserve that Corollary \\ref{cor:itm delta} extends the result in Example \\ref{ex:call,put delta}. The drift term determines the order greater than $\\sqrt{T}\\,.$\n\\end{remark}\n\n\\section{Conclusion}\\label{sec:conclusion}\nThis paper described the short-maturity asymptotic analysis of the Asian option having an arbitrary H\\"older continuous payoff in the local volatility model. We were mainly interested in the Asian option price and the Asian option delta value. The short-maturity behaviors of the option price and the delta value were both expressed in terms of the {\\em Asian volatility}, which was defined by\n\\begin{equation}\n    \\sigma_{A}(T)=\\sqrt{\\frac{1}{T^3}\\int_0^T\\sigma^2(t,S_0)(T-t)^2\\,dt}\\,.\n\\end{equation}\nFor sufficiently small $T>0\\,,$ we proved that\n\\begin{align}\n    &P_A(T)=\\mathbb{E}^\\mathbb{Q}[\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)]+\\mathcal{O}(T^{\\gamma})\\,, \\\\ &\\Delta_A(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(T^{\\gamma-\\frac{1}{2}})\\,,\n\\end{align}\nfor a standard normal random variable $Z$ and the H\\"older exponent $\\gamma$ of the payoff function $\\Phi.$ \n\n\nThese asymptotic results were based on the idea that an underlying process $(S_t)_{t\\ge0}$ under the local volatility model can be approximated by some suitable Gaussian processes in the $L^p(\\mathbb{Q})$ norm. To implement this main idea in the approximation, we used Malliavin calculus theory to represent the delta of the Asian option. In addition, we used the large deviation principle to investigate an asymptotic for the Asian call and put option.\\\\\n\\indent For comparison with the Asian option, we examined the short-maturity behavior of the European option. In contrast to the Asian volatility, we proved that at short maturity $T\\,,$ the European option is expressed by the {\\em European volatility}. In terms of these volatilities, we observed the resemblance between Asian and European options at short maturity. \\newline\n\n\n\n\n\n\n\\noindent\\textbf{Acknowledgement.}\\\\ \nHyungbin Park was supported by the National Research Foundation of Korea (NRF) grant funded by the Ministry of Science and ICT  (No. 2018R1C1B5085491 and No. 2017R1A5A1015626) \nand the Ministry of Education   (No. 2019R1A6A1A10073437) through Basic Science Research Program.\n\n\n\n\n\n\\appendices\n\n \n\n\\section{Proofs of the results in Section \\ref{sec:Approximation scheme}}\n\n \n \n\\subsection{Proof for Lemma \\ref{lem:x,y close}} \\label{proof:x,y close}\n\\begin{proof}\nFirst, we prove the second inequality of Eq.\\eqref{eq:x close}.\nIt suffices to show this for $p\\ge2$ since once this is proven, \nfor $0<p<2,$  by the Jensen inequality, $$\\left(\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t \\vert^p] \\right)^{\\frac{2}{p}} \\le\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t \\vert^2] \\le B_2t^2\\,$$\nfor some constant $B_2>0.$\nNow, for $p\\ge2$, observe that  \n\\begin{align}\n\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t \\vert^p]\n&\\le C_p\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^t\\vert \\sigma(s,S_0)\\tilde{X}_s-\\sigma(s,S_0)S_0 \\vert^2 \\,ds\\right)^{\\frac{p}{2}}\\right] \\\\\n&\\le C_p\\overline{\\sigma}^p\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^t\\vert \\tilde{X}_s-S_0 \\vert^2 \\,ds\\right)^{\\frac{p}{2}}\\right] \\\\\n&\\le C_p\\overline{\\sigma}^pt^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[ \\vert \\tilde{X}_s-S_0 \\vert^p]\\,ds \\label{proof:x close 1.1}\n\\end{align}\nfor some constant $C_p>0.$ For these inequalities, we used the Burkholder--Davis--Gundy inequality, Assumption \\ref{classical assumption}, and the Jensen inequality. \nUsing the Jensen inequality $(\\frac{x+y}{2})^p\\le\\frac{x^p+y^p}{2}$ for $x,y\\ge0,$ it follows that for $t\\le1,$\n\\begin{align}\n&t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[ \\vert \\tilde{X}_s-S_0 \\vert^p]\\,ds \\\\\n    \\le\\,& 2^{p-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_s-\\hat{X}_s \\vert^p]\\,ds  +2^{p-1}t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[ \\vert \\hat{X}_s-S_0 \\vert^p]\\,ds \\label{proof:x close 1.2}\\\\\n    \\le\\,& 2^{p-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_s-\\hat{X}_s \\vert^p]\\,ds +\n 2^{p-1}\\overline{\\sigma}^{p}{S_0}^pM(p)\\frac{1}{\\frac{p}{2}+1}t^p\\label{proof:x close 1.3}\n\\end{align}\nwhere $M(p)$ is defined as $\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^p]$ while $Z$ denotes a standard normal distribution.\nBecause $\\hat{X}_t$ is a normal random variable for each $t>0$,\ndirect calculation gives Eq.\\eqref{proof:x close 1.3}. Hence, from Eqs.\\eqref{proof:x close 1.1} and \\eqref{proof:x close 1.3}, we get\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t \\vert^p]\n    \\le f_p(t)+A_p\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_s-\\hat{X}_s \\vert^p]\\,ds\\,,\n\\end{align}\nwhere $f_p(t):=C_p\\overline{\\sigma}^p2^{p-1}\\overline{\\sigma}^{p}{S_0}^pM(p)\\frac{1}{\\frac{p}{2}+1}t^p$ and $A_p:=C_p\\overline{\\sigma}^p2^{p-1}\\,.$ Then, by the Gronwall inequality, for any $0\\le t\\le 1\\,,$\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t \\vert^p]\n    \\le f_p(t)+A_p\\int_0^tf_p(s)e^{A_p(t-s)}\\,ds\\,.\n\\end{align}\nThus, we can find a constant $B_p$ for $\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_t-\\hat{X}_t\\vert^p] \\le B_pt^p$. \\\\\n\\indent For the first inequality of Eq.\\eqref{eq:x close}, we also present the proof for $p\\ge2$. Observe that $(X_t-\\tilde{X}_t)_{t\\ge0}$ is a continuous martingale starting at 0. By the Burkholder--Davis--Gundy inequality and the Jensen inequality, we get\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t \\vert^p]\n    &\\le C_p\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^t \\vert \\sigma(s,X_s)X_s-\\sigma(s,S_0)\\tilde{X}_s \\vert^2 \\,ds\\right)^{\\frac{p}{2}}\\right] \\\\\n    &\\le C_pt^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,X_s)X_s-\\sigma(s,S_0)\\tilde{X}_s \\vert^p]\\,ds\n\\end{align}\nfor some constant $C_p>0$. Using the Jensen inequality $(\\frac{x+y+z}{3})^p\\le\\frac{x^p+y^p+z^p}{3}$ for $x,y,z\\ge0,$\n\\begin{align}\n    &t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,X_s)X_s-\\sigma(s,S_0)\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &\\le 3^{p-1}t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,X_s)X_s-\\sigma(s,\\tilde{X}_s)\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &+3^{p-1}t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\tilde{X}_s)\\tilde{X}_s-\\sigma(s,\\hat{X}_s)\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &+3^{p-1}t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\hat{X}_s)\\tilde{X}_s-\\sigma(s,S_0)\\tilde{X}_s \\vert^p]\\,ds\\,.\n\\end{align}\nFirst, observe under Assumption \\ref{classical assumption} that if $t\\le1$,\n\\begin{align}\n    t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,X_s)X_s-\\sigma(s,\\tilde{X}_s)\\tilde{X}_s \\vert^p]\\,ds\n    \\le \\alpha^p\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert X_s-\\tilde{X}_s \\vert^p]\\,ds\\,.\n\\end{align}\nSecond, by Assumption \\ref{classical assumption} and the H\\"older inequality with $t\\le1$,\n\\begin{align}\n    &t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\tilde{X}_s)\\tilde{X}_s-\\sigma(s,\\hat{X}_s)\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &\\le t^{\\frac{p}{2}-1}\\int_0^t(\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\tilde{X}_s)-\\sigma(s,\\hat{X}_s) \\vert^{2p}])^{\\frac{1}{2}}(\\mathbb{E}^\\mathbb{Q}[\\vert\\tilde{X}_s\\vert^{2p}])^{\\frac{1}{2}} \\,ds \\\\\n    &\\le t^{\\frac{p}{2}-1}\\alpha^p\\int_0^t(\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{X}_s-\\hat{X}_s\\vert^{2p}])^{\\frac{1}{2}}{S_0}^p e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds \\label{proof:x close 1.4}\\\\\n    &\\le \\alpha^p(B_{2p})^{\\frac{1}{2}}{S_0}^p t^{\\frac{p}{2}-1}\\int_0^t s^p e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds\\,. \\label{proof:x close 1.5} \\\\\n    &\\le \\alpha^p(B_{2p})^{\\frac{1}{2}}{S_0}^p t^{p}\\int_0^1 e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds\\,. \\label{proof:x close 1.6}\n\\end{align}\nObserve that Eq.\\eqref{proof:x close 1.4} comes from direct calculation since $\\tilde{X}_s$ is a log-normal random variable for each $s>0$. The second inequality of Eq.\\eqref{eq:x close} gives Eq.\\eqref{proof:x close 1.5} whereas Eq.\\eqref{proof:x close 1.6} holds under $t\\le1$ with $p\\ge 2$. Third, by Assumption \\ref{classical assumption} and the H\\"older inequality with $t\\le 1$,\n\\begin{align}\n    &t^{\\frac{p}{2}-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\hat{X}_s)\\tilde{X}_s-\\sigma(s,S_0)\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &\\le t^{\\frac{p}{2}-1}\\int_0^t(\\mathbb{E}^\\mathbb{Q}[\\vert \\sigma(s,\\hat{X}_s)-\\sigma(s,S_0) \\vert^{2p}])^{\\frac{1}{2}}(\\mathbb{E}^\\mathbb{Q}[\\vert\\tilde{X}_s\\vert^{2p}])^{\\frac{1}{2}} \\,ds \\\\\n    &\\le t^{\\frac{p}{2}-1}\\alpha^p\\int_0^t(\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{X}_s-S_0 \\vert^{2p}])^{\\frac{1}{2}}{S_0}^p e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds \\\\\n    &\\le \\alpha^p\\overline{\\sigma}^p{S_0}^{2p}(M(2p))^{\\frac{1}{2}} t^{\\frac{p}{2}-1}\\int_0^t s^{\\frac{p}{2}} e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds\\,\\label{proof:x close 1.7} \\\\\n    &\\le \\alpha^p\\overline{\\sigma}^p{S_0}^{2p}(M(2p))^{\\frac{1}{2}}\\frac{1}{\\frac{p}{2}+1} t^{p} e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}\\vee 0}\\,\\label{proof:x close 1.8}\n\\end{align}\nwhere $M(p)$ is defined as $\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^p]$ while $Z$ denotes a standard normal distribution. Since $\\hat{X}_s$ is a normal random variable, it is easy to verify the inequality Eq.\\eqref{proof:x close 1.7}. From $t\\le 1$, Eq.\\eqref{proof:x close 1.8} follows. By combining the three inequalities, for $t\\le1$, we obtain\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t \\vert^p] \\\\\n    &\\le 3^{p-1}C_p\\alpha^p\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert X_s-\\tilde{X}_s \\vert^p]\\,ds \\\\\n    &+\\left(3^{p-1}C_p\\alpha^p(B_{2p})^{\\frac{1}{2}}{S_0}^p \\int_0^1 e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}s}\\,ds\n    +3^{p-1}C_p\\alpha^p\\overline{\\sigma}^p{S_0}^{2p}(M(2p))^{\\frac{1}{2}}\\frac{1}{\\frac{p}{2}+1} e^{\\frac{p(2p-1)\\overline{\\sigma}^2}{2}\\vee 0}\\right)t^p\\,.\n\\end{align}\nFrom straightforward use of the Gronwall inequality, we can find a positive constant $B_p\'$ such that $\\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t \\vert^p]\\le B_p\'t^p$. Without loss of generality, we can identify $B_p\'$ with $B_p$ or otherwise just take a bigger one. \\\\\n\\indent The proof for the second inequality of Eq.\\eqref{eq:y close} is nearly a repetition of the proof for the second inequality of Eq.\\eqref{eq:x close}; hence, it is omitted. Here, we examine only the first inequality of Eq.\\eqref{eq:y close}. Again, restrict $p\\ge 2$. By the Burkholder--Davis--Gundy inequality and the Jensen inequality,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t \\vert^p]\n    &\\le C_p\\mathbb{E}^\\mathbb{Q}\\left[\\left( \\int_0^t (\\nu(s,X_s)Y_s-\\nu(s,S_0)\\tilde{Y}_s)^2\\,ds \\right)^{\\frac{p}{2}}\\right] \\\\\n    &\\le 3^{p-1}C_p t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,X_s)Y_s-\\nu(s,X_s)\\tilde{Y}_s \\vert^p]\\,ds \\\\\n    &+3^{p-1}C_p t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,X_s)\\tilde{Y}_s-\\nu(s,\\hat{X}_s)\\tilde{Y}_s \\vert^p]\\,ds \\\\\n    &+3^{p-1}C_p t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,\\hat{X}_s)\\tilde{Y}_s-\\nu(s,S_0)\\tilde{Y}_s \\vert^p]\\,ds\n\\end{align}\nfor some constant $C_p>0$. The remainder of the proof is similar to the proof for Eq.\\eqref{eq:x close}. First,\n\\begin{align}\n    t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,X_s)Y_s-\\nu(s,X_s)\\tilde{Y}_s \\vert^p]\\,ds\n    \\le \\alpha^p\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert Y_s-\\tilde{Y}_s \\vert^p]\\,ds\n\\end{align}\nfor $t\\le 1$. For this inequality, we use $\\vert \\nu(t,x)\\vert \\le \\alpha$ for all $t>0$ and $x\\in \\mathbb{R}$, which comes from Assumption \\ref{classical assumption}. Second, observe from Assumption \\ref{classical assumption}, the H\\"older inequality, Eq.\\eqref{eq:x close}, and direct computation with regard to a log-normal random variable $\\tilde{Y}_s$ that\n\\begin{align}\n    &t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,X_s)\\tilde{Y}_s-\\nu(s,\\hat{X}_s)\\tilde{Y}_s \\vert^p]\\,ds \\\\\n    &\\le \\alpha^p t^{\\frac{p}{2}-1}\\int_0^t (\\mathbb{E}^\\mathbb{Q}[\\vert X_s-\\hat{X}_s \\vert^{2p}])^{\\frac{1}{2}}(\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{Y}_s \\vert^{2p}])^{\\frac{1}{2}} \\,ds \\\\\n    &\\le \\alpha^p 2^p (B_{2p})^{\\frac{1}{2}} t^{\\frac{p}{2}-1}\\int_0^t s^p e^{\\frac{p(2p-1)\\alpha^2}{2}s}\\,ds \\\\\n    &\\le \\alpha^p 2^p (B_{2p})^{\\frac{1}{2}}t^{p}\\int_0^1 e^{\\frac{p(2p-1)\\alpha^2}{2}s}\\,ds \\label{proof:y close 1.1}\n\\end{align}\nwhere Eq.\\eqref{proof:y close 1.1} holds under $t\\le 1$. Third, use Assumption \\ref{classical assumption}, the H\\"older inequality, and direct computation with regard to a normal random variable $\\hat{X}_s$ and a log-normal random variable $\\tilde{Y}_s$ to obtain\n\\begin{align}\n    &t^{\\frac{p}{2}-1}\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,\\hat{X}_s)\\tilde{Y}_s-\\nu(s,S_0)\\tilde{Y}_s \\vert^p]\\,ds \\\\\n    &\\le \\alpha^p t^{\\frac{p}{2}-1}\\int_0^t (\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{X}_s-S_0 \\vert^{2p}])^{\\frac{1}{2}}(\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{Y}_s \\vert^{2p}])^{\\frac{1}{2}} \\,ds \\\\\n    &\\le \\alpha^p\\overline{\\sigma}^p{S_0}^p(M(2p))^{\\frac{1}{2}} t^{\\frac{p}{2}-1}\\int_0^t s^{\\frac{p}{2}} e^{\\frac{p(2p-1)\\alpha^2}{2}s}\\,ds \\\\\n    &\\le \\alpha^p\\overline{\\sigma}^p{S_0}^p(M(2p))^{\\frac{1}{2}}\\frac{1}{\\frac{p}{2}+1} t^{p} e^{\\frac{p(2p-1)\\alpha^2}{2}\\vee 0} \\label{proof:y close 1.2}.\n\\end{align}\nThe last inequality Eq.\\eqref{proof:y close 1.2} holds under $t\\le 1$. Combine the three inequalities to get\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t \\vert^p] \\\\\n    &\\le 3^{p-1}C_p\\alpha^p\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert Y_s-\\tilde{Y}_s \\vert^p]\\,ds \\\\\n    &+\\left(3^{p-1}C_p\\alpha^p 2^p (B_{2p})^{\\frac{1}{2}}\\int_0^1 e^{\\frac{p(2p-1)\\alpha^2}{2}s}\\,ds\n    +3^{p-1}C_p\\alpha^p\\overline{\\sigma}^p{S_0}^p(M(2p))^{\\frac{1}{2}}\\frac{1}{\\frac{p}{2}+1} e^{\\frac{p(2p-1)\\alpha^2}{2}\\vee 0}\\right)t^p\\,.\n\\end{align}\nApply the Gronwall inequality to get the bound $B_p$.\n\\end{proof}\n\n\\subsection{Proof for Lemma \\ref{lem:dummy}} \\label{proof:dummy}\nBefore we prove Lemma \\ref{lem:dummy}, we state and prove the generalized version of it. Later, we will show that the following lemma is actually a sufficient condition.\n\\begin{lemma}\\label{lem:dummy general}\n\tGiven a measure space $(\\Omega, \\mathcal{F}, \\mathbb{Q})$ and a Brownian motion $(W_t)_{t\\ge0}$, suppose that a process $(\\theta_t)_{t\\ge0}$ is adapted to the Brownian filtration $(\\mathcal{F}_t^W)_{t\\ge 0}$ and is uniformly bounded. More precisely, there is a constant $C>0$ such that $\\vert \\theta_t(\\omega) \\vert\\le C$ for any $t\\ge0$ and $\\omega\\in \\Omega$. Define a continuous martingale process $(M_t)_{t\\ge0}$ as\n\\begin{equation}\n\tM_t:=M_0e^{-\\frac{1}{2}\\int_0^t \\theta_s^2\\,ds+\\int_0^t\\theta_s\\,dW_s}\\,, \\quad M_0>0\\,.\n\\end{equation}\nThen, for any $\\xi\\in\\mathbb{R}$, the following three statements hold.\n\\begin{enumerate}[label=(\\roman*)]\n\\item $\\begin{aligned}\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}[M_T^{\\xi}]=M_0^{\\xi}\\,.\\end{aligned}$\n\\item $\\begin{aligned} \\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}M_t^{\\xi}\\Big]<\\infty\\,, \\end{aligned}$ for any $T>0\\,.$ Furthermore, $\\begin{aligned} \\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}M_t^{\\xi}\\Big]=M_0^{\\xi}\\,. \\end{aligned}$\n\\item $\\begin{aligned} \\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T M_t\\,dt\\right)^\\xi\\right]=M_0^{\\xi}\\,. \\end{aligned}$\n\\end{enumerate}\n\\end{lemma}\n\\begin{proof}\nObserve that\n\\begin{equation}\\label{eq:power of mart}\nM_0^{\\xi}e^{k(\\xi)T}e^{-\\frac{1}{2}\\int_0^T { \\xi^2 \\theta_s^2}\\,ds+\\int_0^T \\xi\\theta_s\\,dW_s}\n\\le M_T^{\\xi}\n\\le M_0^{\\xi}e^{K(\\xi)T}e^{-\\frac{1}{2}\\int_0^T {\\xi^2 \\theta_s^2}\\,ds+\\int_0^T \\xi\\theta_s\\,dW_s}\\,, \n\\end{equation}\nwhere $k(\\xi):=\\frac{\\xi(\\xi-1)}{2}\\wedge 0$, $K(\\xi):=\\frac{\\xi(\\xi-1)}{2}\\vee 0$. \nBy taking the expectation $\\mathbb{E}^\\mathbb{Q}$ on both sides,\nwe obtain $M_0^{\\xi}e^{k(\\xi)T}\\le \\mathbb{E}^\\mathbb{Q}[M_T^{\\xi}]\\le M_0^{\\xi}e^{K(\\xi)T}$ \nbecause $\\big(e^{ -\\frac{1}{2}\\int_0^t { \\xi^2 \\theta_s^2 }\\,ds+\\int_0^t \\xi \\theta_s\\,dW_s }\\big)_{t\\ge0}$ is a continuous martingale. Take $T\\rightarrow{0}$ to get the limit $\\begin{aligned}\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}[M_T^{\\xi}]=M_0^{\\xi}\\,.\\end{aligned}$ \\\\\n\\indent Suppose that $\\xi>0\\,.$ Choose $p$ such that $p\\ge 1$ and $\\xi p >1.$\nUse the Jensen inequality and the Doob $L^p$ inequality to obtain \n\\begin{align}\n\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\Big( \\underset{0\\le t\\le T}{\\max}M_t \\Big)^{\\xi}\\right]\\right)^p \\le \\mathbb{E}^\\mathbb{Q}\\left[\\Big( \\underset{0\\le t \\le T}{\\max}M_t \\Big)^{\\xi p}\\right] &\\le \\left(\\frac{\\xi p}{\\xi p-1}\\right)^{\\xi p}\\mathbb{E}^\\mathbb{Q}[M_T^{\\xi p}]\\,.\n\\end{align}\nSince $\\mathbb{E}^\\mathbb{Q}[M_T^{\\xi p}]\\le M_0^{\\xi p}e^{K(\\xi p)T}<\\infty\\,,$\nwe get that $\\begin{aligned}\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}M_t^{\\xi}\\Big]=\\mathbb{E}^\\mathbb{Q}\\Big[\\Big(\\max_{0\\le t\\le T}M_t\\Big)^{\\xi}\\Big]<\\infty\\,\\end{aligned}$ for any $T>0\\,.$ Note that $\\underset{0\\le t \\le T}{\\max}M_t^{\\xi} \\searrow M_0^{\\xi}$ almost surely as $T\\rightarrow{0}$. Thus, from the Lebesgue dominated convergence theorem, $\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}M_t^{\\xi}\\Big]= M_0^{\\xi}\\,.$ Next, suppose that $\\xi<0\\,.$ Observe that\n\\begin{align}\n\\frac{1}{M_t}=\\frac{1}{M_0}e^{\\frac{1}{2}\\int_0^t \\theta_s^2\\,ds-\\int_0^t \\theta_s\\,dW_s}\n=\\frac{1}{M_0} e^{\\int_0^t \\theta_s^2\\,ds} e^{-\\frac{1}{2}\\int_0^t \\theta_s^2\\,ds-\\int_0^t \\theta_s\\,dW_s}\n\\le \\frac{1}{M_0}e^{C^2t}N_t\\,,\n\\end{align}\nwhere $N_t:=e^{-\\frac{1}{2}\\int_0^t(-\\theta_s)^2\\,ds+\\int_0^t-\\theta_s\\,dW_s}$. Take the $(-\\xi)$th power to the above-mentioned inequality to obtain\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}M_t^{\\xi}\\Big]\\le M_0^{\\xi}e^{-\\xi C^2T}\\,\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}N_t^{-\\xi}\\Big]\\,.\n\\end{align} Observe from the proof for the case $\\xi>0$ that we have already proved $\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}N_t^{-\\xi}\\Big]<\\infty\\,$ for any $T>0\\,.$ Therefore, $\\begin{aligned} \\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}M_t^{\\xi}\\Big]<\\infty\\, \\end{aligned}$ for any $T>0\\,.$ By the Lebesgue dominated convergence theorem, $\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}M_t^{\\xi}\\Big]= M_0^{\\xi}$ follows.\\\\\n\\indent Finally, we prove that $\\lim_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\Big(\\frac{1}{T}\\int_0^T M_t\\,dt\\Big)^\\xi\\Big]=M_0^{\\xi}\\,.$ From the Fatou lemma, it suffices to show that $\\limsup_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\Big(\\frac{1}{T}\\int_0^T M_t\\,dt\\Big)^{\\xi}\\Big]\\le M_0^{\\xi}\\,.$ This is straightforward from the inequality $\\Big(\\frac{1}{T}\\int_0^T M_t\\,dt\\Big)^{\\xi}\\le\\underset{0\\le t\\le T}{\\max}M_t^{\\xi}\\,$ for $\\xi\\in\\mathbb{R}\\,.$\n\\end{proof}\n\\begin{proof}[Proof of Lemma \\ref{lem:dummy}]\nWe now prove Lemma \\ref{lem:dummy}. Observe that the processes $(X_t)_{t\\ge0}\\,,$ $(\\tilde{X}_t)_{t\\ge0}\\,,$ $(Y_t)_{t\\ge0}\\,,$ $(\\tilde{Y}_t)_{t\\ge0}$ all satisfy the condition in Lemma \\ref{lem:dummy general}. Thus, from Lemma \\ref{lem:dummy general} and the H\\"older inequality\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}Z_t^{p_1,p_2,p_3,p_4}\\Big] \\\\\n    &\\le\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}X_t^{p_1}\\max_{0\\le t\\le T}\\tilde{\n    X}_t^{p_2}\\max_{0\\le t\\le T}Y_t^{p_3}\\max_{0\\le t\\le T}\\tilde{Y}_t^{p_4}\\Big] \\\\\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}X_t^{4p_1}\\Big]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}\\tilde{X}_t^{4p_2}\\Big]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}Y_t^{4p_3}\\Big]\\right)^{\\frac{1}{4}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}\\tilde{Y}_t^{4p_4}\\Big]\\right)^{\\frac{1}{4}}\\,,\n\\end{align}\nwe obtain $\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}Z_t^{p_1,p_2,p_3,p_4}\\Big]<\\infty\\,$ for any $T>0\\,.$ Moreover, it is followed by the upper bound $\\limsup_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}Z_t^{p_1,p_2,p_3,p_4}\\Big]\\le S_0^{p_1+p_2}$. The Fatou lemma yields the lower bound $S_0^{p_1+p_2}\\le \\liminf_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le t\\le T}{\\max}Z_t^{p_1,p_2,p_3,p_4}\\Big]\\,.$ \\\\\n\\indent The inequality Eq.\\eqref{eq:dummy} can be proved similarly. From the Fatou lemma, it is sufficient to show only the upper bound\n\\begin{align}\n    &\\limsup_{T\\rightarrow{0}}\\mathbb{E}^\\mathbb{Q}\\Bigg[Z_T^{q_1,q_2,q_3,q_4}\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)^{q_5}\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^{q_6}\n    \\left(\\frac{1}{T}\\int_0^TY_t\\,dt\\right)^{q_7}\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^{q_8}\\Bigg] \\\\\n    &\\le S_0^{q_1+q_2+q_5+q_6}\\,.\n\\end{align}\nMeanwhile, the upper bound is implied from the use of the H\\"older inequality\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\Bigg[Z_T^{q_1,q_2,q_3,q_4}\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)^{q_5}\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^{q_6}\n    \\left(\\frac{1}{T}\\int_0^TY_t\\,dt\\right)^{q_7}\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^{q_8}\\Bigg] \\\\\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}\\Big[Z_T^{5q_1,5q_2,5q_3,5q_4}\\Big]\\right)^{\\frac{1}{5}}\n    \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)^{5q_5}\\right]\\right)^{\\frac{1}{5}}\n    \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{X}_t\\,dt\\right)^{5q_6}\\right]\\right)^{\\frac{1}{5}} \\\\\n    &\\times\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T Y_t\\,dt\\right)^{5q_7}\\right]\\right)^{\\frac{1}{5}}\n    \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{5q_8}\\right]\\right)^{\\frac{1}{5}}\\,,\n\\end{align}\nwith Lemma \\ref{lem:dummy general}. Hence, we get the desired result.\n\\end{proof}\n\n\n\n\\section{Proofs of the results in Section \\ref{sec:Short maturity limit of an option price}}\n\\subsection{Proof for Lemma \\ref{lem:price drift 0}}\\label{proof:price drift negligible}\n\\begin{proof}\nDefine $\\tilde{S}_t:=e^{-(r-q)t}S_t$. Then, by Ito calculus, we get\n\\begin{equation}\n    d\\tilde{S}_t=\\sigma(t,e^{(r-q)t}\\tilde{S}_t)\\tilde{S}_t\\,dW_t\\,,\\quad \\tilde{S}_0=S_0\\,.\n\\end{equation}\nUsing $\\alpha$ in Assumption \\ref{classical assumption} and the inequality $(x+y)^2\\le 2x^2+2y^2$ for $x,y\\ge0$, we have\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{S}_t-X_t\\vert^2] \\\\\n    &= \\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert\\sigma(u,e^{(r-q)u}\\tilde{S}_u)\\tilde{S}_u-\\sigma(u,X_u)X_u\\vert^2]\\,du \\\\\n    &\\le 2\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert\\sigma(u,e^{(r-q)u}\\tilde{S}_u)\\tilde{S}_u-\\sigma(u,\\tilde{S}_u)\\tilde{S}_u\\vert^2]\\,du\n    +2\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert\\sigma(u,\\tilde{S}_u)\\tilde{S}_u-\\sigma(u,X_u)X_u\\vert^2]\\,du \\\\\n    &\\le 2\\alpha^2\\int_0^t\\vert e^{(r-q)u}-1\\vert^2\\mathbb{E}^\\mathbb{Q}[\\tilde{S}_u^4]\\,du\n    +2\\alpha^2\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{S}_u-X_u\\vert^2]\\,du\\,.\n\\end{align}\nSince $\\mathbb{E}^\\mathbb{Q}[\\tilde{S}_u^4]\\le S_0^4e^{6\\overline{\\sigma}^2u}$ holds for any $u\\ge 0$, $$2\\alpha^2\\int_0^t\\vert e^{(r-q)u}-1\\vert^2\\mathbb{E}^\\mathbb{Q}[\\tilde{S}_u^4]\\,du=\\mathcal{O}(t^3)$$ as $t\\rightarrow{0}$. Therefore, by the Gronwall inequality, we get $\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{S}_t-X_t\\vert^2]=\\mathcal{O}(t^3)$ as $t\\rightarrow{0}$. Further, note that from $(x+y)^2\\le 2x^2+2y^2$ for $x,y\\ge0$,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert^2]\n    \\le 2\\mathbb{E}^\\mathbb{Q}[\\vert S_t-\\tilde{S}_t\\vert^2]\n    +2\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{S}_t-X_t\\vert^2]\n    \\le 2\\vert e^{(r-q)t}-1\\vert^2\\mathbb{E}^\\mathbb{Q}[\\tilde{S}_t^2]\n    +2\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{S}_t-X_t\\vert^2].\n\\end{align}\nHereafter, from  $\\mathbb{E}^\\mathbb{Q}[\\tilde{S}_t^2]\\le S_0^2e^{\\overline{\\sigma}^2t}$, it is implied that $\\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert^2]=\\mathcal{O}(t^2)$. Further, note that $\\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert]\\le (\\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert^2])^{\\frac{1}{2}}=\\mathcal{O}(t)$. Thus, from the inequalities \n\\begin{align}\n    &\\left\\vert P_A(T)-e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\right]\\right\\vert\n    \\le \\beta e^{-rT}\\,\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert S_t-X_t\\vert]\\,dt=\\mathcal{O}(T)\\,, \\\\\n    &\\left\\vert P_E(T)-e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}[\\Phi(X_T)]\\right\\vert\n    \\le \\beta e^{-rT}\\,\\mathbb{E}^\\mathbb{Q}[\\vert S_T-X_T\\vert]=\\mathcal{O}(T)\\,,\n\\end{align}\nwe obtain the desired proof.\n\\end{proof}\n\n\n\n\n\\section{Proofs of the results in Section \\ref{sec:Short maturity asymptotic for a sensitivity}}\n\\subsection{Proof for Lemma \\ref{lem:delta drift 0}}\\label{proof:delta drift 0}\n\nBy the Lebesgue dominated convergence theorem, the Asian and European delta values can be  expressed in terms of the derivative $\\Phi^{\'}$  of the payoff function. \n\\begin{lemma}\\label{lem:another repre delta}\nFor the process $S$ stated in Eq.\\eqref{eq:S_t} under Assumptions \\ref{classical assumption} and \\ref{payoff assumption}, we have\n\\begin{align}\n    &\\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\right]\n    =\\frac{1}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi^{\'}\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\frac{1}{T}\\int_0^T S_t\\,dt\\right]\\,,\\\\\n    &\\frac{\\partial}{\\partial S_0}\\mathbb{E}^\\mathbb{Q}[\\Phi(S_T)]\n    =\\frac{1}{S_0}\\mathbb{E}^\\mathbb{Q}[\\Phi^{\'}(S_T)S_T]\\,.\n\\end{align}\nHere, the derivative $\\Phi^{\'}$ is defined almost everywhere with respect to the Lebesgue measure.\n\\end{lemma}\n\\begin{proof} \nBy the Lebesgue dominated convergence theorem, it suffices to show that the distribution measures of $S_T$ and $\\frac{1}{T}\\int_0^T S_t\\,dt$ are both absolutely continuous with respect to the Lebesgue measure for any $T\\ge0\\,.$ From Theorem 2.3.1 of \\cite{NualartDavid1995TMca}, a density of $S_T$ always exists. Observe from Assumption \\ref{classical assumption} that the first variation process of $S$ is the unique solution of the stochastic differential equation\n\\begin{equation}\\label{eq:process z}\n    dZ_t=(r-q)Z_t\\,dt+\\nu(t,S_t)Z_t\\,dW_t\\,, \\quad Z_0=1\\,\n\\end{equation}\nand that\n\\begin{align}\n    D_uS_t=\\frac{Z_t}{Z_u}\\sigma(u,S_u)S_u\\mathbbm{1}_{\\{u\\le t\\}}\\,.\n\\end{align}\nTherefore, from Proposition 7.1.2 of \\cite{nualart2018introduction}, it is easy to check that a density of $\\frac{1}{T}\\int_0^T S_t\\,dt$ always exists. This completes the proof.\n\\end{proof}\n\n\n\\begin{proof}[Proof of Lemma \\ref{lem:delta drift 0}] \nDefine a  process $\\theta_t:=\\frac{r-q}{\\sigma(t,S_t)},t\\ge0.$\nThen, the process $$M_t:=e^{-\\frac{1}{2}\\int_0^t\\theta^2_u\\,du-\\int_0^t\\theta_u\\,dW_u}$$   is a continuous martingale  since $\\theta$ is a bounded process.  By the Girsanov theorem, $dB_t:=dW_t+\\theta_t\\,dt$, $0\\le t\\le T$ is a Brownian motion under the measure   $d\\mathbb{P}:=M_T\\,d\\mathbb{Q}$ on $\\mathcal{F}_T^W$. Since $dS_t=\\sigma(t,S_t)S_t\\,dB_t$ under the measure $\\mathbb{P}$, \nLemma \\ref{lem:another repre delta} states that   \n\\begin{align}\n    \\Delta_A(T)\n      &  =\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi^{\'}\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\frac{1}{T}\\int_0^T S_t\\,dt\\right]\\\\    &=\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{P}\\left[\\Phi^{\'}\\left(\\frac{1}{T}\\int_0^T S_t\\,dt\\right)\\frac{1}{T}\\int_0^T S_t\\,dt\\,\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\right] \\\\\n    &=\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi^{\'}\\left(\\frac{1}{T}\\int_0^T X_t\\,dt\\right)\\frac{1}{T}\\int_0^T X_t\\,dt\\,e^{-\\frac{1}{2}\\int_0^T\\eta^2_t\\,dt+\\int_0^T\\eta_t\\,dW_t}\\right]\\,,\n\\end{align}\nwhere $\\eta_t=\\frac{r-q}{\\sigma(t,X_t)}\\,.$ Therefore, by the Cauchy--Schwarz inequality and the inequality $\\vert\\Phi^{\'}\\vert\\le \\beta$ from Assumption \\ref{payoff assumption},\n\\begin{align}\n    &\\left\\vert\\Delta_A(T)-\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi^{\'}\\left(\\frac{1}{T}\\int_0^T  X_t\\,dt\\right)\\frac{1}{T}\\int_0^T X_t\\,dt\\,\\right]\\right\\vert \\\\\n    &\\le \\beta\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_0^T X_t\\,dt\\left\\vert e^{-\\frac{1}{2}\\int_0^T\\eta^2_t\\,dt+\\int_0^T\\eta_t\\,dW_t}-1\\right\\vert\\right] \\\\\n    &\\le \\beta\\frac{e^{-rT}}{S_0}\\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[X_t^2]\\,dt\\right)^{\\frac{1}{2}}\n    \\left(e^{\\frac{(r-q)^2}{\\underline{\\sigma}^2}T}-1\\right)^{\\frac{1}{2}}.\n\\end{align}\nSince $\\mathbb{E}^\\mathbb{Q}[X_t^2]\\le S_0^2e^{\\overline{\\sigma}^2t}$ and  $e^{\\frac{(r-q)^2}{\\underline{\\sigma}^2}T}-1=\\mathcal{O}(T)\\,,$ the proof for the Asian delta value is complete. The proof for the European delta value  can be obtained similarly.\n\\end{proof}\n\n\\subsection{Proof for Lemma \\ref{lem:u,TF app}}\\label{proof: u,TF app}\n\\begin{proof}\nWe prove the first inequality of Eq.\\eqref{eq:u app}. It is sufficient to show this for $p \\ge 1$. By Assumption \\ref{classical assumption} and the Jensen inequality $(\\frac{x+y}{2})^p\\le\\frac{x^p+y^p}{2}$ for $x,y\\ge 0$,\n\\begin{align}\n    &\\vert u_t-\\tilde{u}_t \\vert^p \\\\\n    &=\\left\\vert \\frac{2(Y_t+\\tilde{Y}_t)(Y_t-\\tilde{Y}_t)}{\\sigma(t,X_t)X_t}+\\frac{2{\\tilde{Y}_t}^2(\\sigma(t,\\tilde{X}_t)\\tilde{X}_t-\\sigma(t,X_t)X_t)}{\\sigma(t,X_t)X_t\\sigma(t,\\tilde{X}_t)\\tilde{X}_t} \\right\\vert^p \\\\\n    &\\le 2^{2p-1}\\left\\vert\\frac{(Y_t+\\tilde{Y}_t)(Y_t-\\tilde{Y}_t)}{\\sigma(t,X_t)X_t}\\right\\vert^p +2^{2p-1}\\left\\vert\\frac{{\\tilde{Y}_t}^2(\\sigma(t,\\tilde{X}_t)\\tilde{X}_t-\\sigma(t,X_t)X_t)}{\\sigma(t,X_t)X_t\\sigma(t,\\tilde{X}_t)\\tilde{X}_t}\\right\\vert^p \\\\\n    &\\le \\frac{2^{2p-1}}{\\underline{\\sigma}^p}\\left\\vert\\frac{(Y_t+\\tilde{Y}_t)(Y_t-\\tilde{Y}_t)}{X_t}\\right\\vert^p +\\frac{2^{2p-1}\\alpha^p}{\\underline{\\sigma}^{2p}}\\left(\\frac{{\\tilde{Y}_t}^{2p}}{{X_t}^p{\\tilde{X}_t}^p}\\right)\\vert  X_t-\\tilde{X}_t\\vert^p\\,.\n\\end{align}\nObserve from the Jensen inequality $(\\frac{x+y}{2})^p\\le\\frac{x^p+y^p}{2}$ for $x,y\\ge 0$ that\n\\begin{align}\n    \\left\\vert\\frac{(Y_t+\\tilde{Y}_t)(Y_t-\\tilde{Y}_t)}{X_t}\\right\\vert^p\n    &=\\left\\vert\\frac{Y_t(Y_t-\\tilde{Y}_t)}{X_t}+\\frac{\\tilde{Y_t}(Y_t-\\tilde{Y}_t)}{X_t}\\right\\vert^p \\\\\n    &\\le 2^{p-1}Y_t^{p}X_t^{-p}\\vert Y_t-\\tilde{Y}_t \\vert^p\n    +2^{p-1}\\tilde{Y}_t^{p}X_t^{-p}\\vert Y_t-\\tilde{Y}_t \\vert^p\\,.\n\\end{align}\nNote from the H\\"older inequality and Lemma \\ref{lem:x,y close} that for $0\\le t\\le 1\\,,$ \n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[Y_t^{p}X_t^{-p}\\vert Y_t-\\tilde{Y}_t \\vert^p\\right]\n    &\\le\\left(\\mathbb{E}^\\mathbb{Q}\\left[Y_t^{2p}X_t^{-2p}\\right]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t\\vert^{2p}]\\right)^{\\frac{1}{2}} \\\\\n    &\\le\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(Y_s^{2p}X_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}}t^p\\,,\n\\end{align}\nwith some positive constant $B_{2p}\\,.$ By the same argument,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\tilde{Y}_t^{p}{X_t}^{-p}\\vert Y_t-\\tilde{Y}_t\\vert^p\\right]\n    \\le\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(\\tilde{Y}_s^{2p}X_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}}t^p\\,\n\\end{align}\nand\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\tilde{Y}_t^{2p}{X_t}^{-p}\\tilde{X}_t^{-p}\\vert  X_t-\\tilde{X}_t\\vert^p\\right]\n    \\le\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(\\tilde{Y}_s^{4p}X_s^{-2p}\\tilde{X}_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}}t^p\\,.\n\\end{align}\nTherefore, for $0\\le t\\le 1\\,,$ we get\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert u_t-\\tilde{u}_t \\vert^p]\n    &\\le\\frac{2^{3p-2}}{\\underline{\\sigma}^p}\\mathbb{E}^\\mathbb{Q}\\left[Y_t^{p}X_t^{-p}\\vert Y_t-\\tilde{Y}_t \\vert^p\\right]\n    +\\frac{2^{3p-2}}{\\underline{\\sigma}^p}\\mathbb{E}^\\mathbb{Q}\\left[\\tilde{Y}_t^{p}{X_t}^{-p}\\vert Y_t-\\tilde{Y}_t\\vert^p\\right] \\\\ &+\\frac{2^{2p-1}\\alpha^p}{\\underline{\\sigma}^{2p}}\\mathbb{E}^\\mathbb{Q}\\left[\\tilde{Y}_t^{2p}{X_t}^{-p}\\tilde{X}_t^{-p}\\vert  X_t-\\tilde{X}_t\\vert^p\\right] \\\\\n    &\\le D_pt^p\\,, \n\\end{align}\nwhere a constant $D_p>0$ is taken as\n\\begin{align}\n    D_p&:=\\frac{2^{3p-2}}{\\underline{\\sigma}^p} \\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(Y_s^{2p}X_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}}\n    +\\frac{2^{3p-2}}{\\underline{\\sigma}^p}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(\\tilde{Y}_s^{2p}X_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}} \\\\\n    &+\\frac{2^{2p-1}\\alpha^p}{\\underline{\\sigma}^{2p}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(\\tilde{Y}_s^{4p}X_s^{-2p}\\tilde{X}_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}(B_{2p})^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom Lemma \\ref{lem:dummy}, $D_p<\\infty$; hence, it is well defined.\n\nThe second inequality of Eq.\\eqref{eq:u app} can be obtained similarly. The only difference is that we have to handle an indicator function that appears in the definition of $\\hat{u}_s$. We may assume that $p\\ge 1\\,.$ Observe from the Jensen inequality that\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert\\tilde{u}_t-\\hat{u}_t\\vert^p] \\\\\n    &\\le 2^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\frac{2\\tilde{Y}_t^2}{\\sigma(t,\\tilde{X}_t)\\tilde{X}_t}\\right\\vert^p\\mathbbm{1}_{\\{\\hat{X}_t<\\frac{S_0}{2}\\}}\\right] \n    +2^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\tilde{Y}_t^2}{\\sigma(t,\\tilde{X}_t)\\tilde{X}_t}-\\frac{2\\hat{Y}_t^2}{\\sigma(t,\\hat{X}_t)\\hat{X}_t}\\right\\vert^p\\mathbbm{1}_{\\{\\hat{X}_t\\ge \\frac{S_0}{2}\\}}\\right].\n\\end{align}\nFollowing the arguments used to prove the first inequality of Eq.\\eqref{eq:u app}, we can show that there exists some positive constant $D_p$ such that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\tilde{Y}_t^2}{\\sigma(t,\\tilde{X}_t)\\tilde{X}_t}-\\frac{2\\hat{Y}_t^2}{\\sigma(t,\\hat{X}_t)\\hat{X}_t}\\right\\vert^p\\mathbbm{1}_{\\{\\hat{X}_t\\ge \\frac{S_0}{2}\\}}\\right]\\le D_pt^p\\,\n\\end{align}\nfor small $0\\le t\\le 1\\,.$ Now, note from Assumption \\ref{classical assumption} and the H\\"older inequality that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\frac{2\\tilde{Y}_t^2}{\\sigma(t,\\tilde{X}_t)\\tilde{X}_t}\\right\\vert^p\\mathbbm{1}_{\\{\\hat{X}_t<\\frac{S_0}{2}\\}}\\right]\n    &\\le \\frac{2^p}{\\underline{\\sigma}^p}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\tilde{Y}_t^{4p}\\tilde{X}_t^{-2p}\\right]\\right)^{\\frac{1}{2}}\\left(\\mathbb{Q}\\left\\{\\hat{X}_t<\\frac{S_0}{2}\\right\\}\\right)^{\\frac{1}{2}} \\\\\n    &\\le \\frac{2^p}{\\underline{\\sigma}^p}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\left(\\tilde{Y}_s^{4p}\\tilde{X}_s^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}\\left(\\mathbb{Q}\\left\\{\\hat{X}_t<\\frac{S_0}{2}\\right\\}\\right)^{\\frac{1}{2}}\\,.\\label{proof:u close 2.3}\n\\end{align}\nLet $Z$ be the standard normal variable with respect to the measure $\\mathbb{Q}$ and $N(\\cdot)$ be a cumulative function of $Z$. To prove the desired inequality, we rely on the property of a cumulative function $N(\\cdot)\\,.$ Since $\\hat{X}_t$ is a normal random variable, it is implied from Assumption \\ref{classical assumption} that the following inequality holds for any $t>0\\,.$\n\\begin{equation}\\label{eq:hatx vanish}\n    \\mathbb{Q}\\left\\{\\hat{X}_t<\\frac{S_0}{2}\\right\\}\n    =N\\Bigg(-\\frac{1}{2\\sqrt{\\int_0^t\\sigma(u,S_0)^2\\,du}}\\Bigg)\n    \\le N\\left(-\\frac{1}{2\\overline{\\sigma}\\sqrt{t}}\\right)\\,.\n\\end{equation}\nHowever, it is easy to check from the definition of $N(\\cdot)$ that $N\\left(-\\frac{1}{2\\overline{\\sigma}\\sqrt{t}}\\right)$ $=o(t^{q})$ for any $q>0$ as $t\\rightarrow{0}\\,.$ Furthermore, from Lemma \\ref{lem:dummy}, $\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le 1}{\\max}\\left(\\tilde{Y}_s^{4p}\\tilde{X}_s^{-2p}\\right)\\Big]<\\infty\\,.$\nThis completes the proof of Eq.\\eqref{eq:u app}.\\\\\n\\indent The proof for Eq.\\eqref{eq:TF app} is much simpler with many key ideas shared with the proof of Eq.\\eqref{eq:u app}. Assume that $p\\ge 1$. Then, by the Jensen inequality and the H\\"older inequality,\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert TF-T\\tilde{F}\\vert^p] \\\\\n    &\\le \\left(\\frac{1}{T}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t\\vert^{2p}]\\,dt\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T Y_t\\,dt\\right)^{-2p}\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-2p}\\right]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nNote from Lemma \\ref{lem:x,y close} that for $0\\le T\\le 1\\,,$\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert TF-T\\tilde{F}\\vert^p]\n    \\le \\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}Y_s^{-2p}\\max_{0\\le s\\le 1}\\tilde{Y}_s^{-2p}\\Big]\\right)^{\\frac{1}{2}}\\left(\\frac{B_{2p}}{2p+1}\\right)^{\\frac{1}{2}}T^p\\,.\n\\end{align}\nFrom Lemma \\ref{lem:dummy} and the Cauchy--Schwarz inequality, $\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le 1}{\\max}Y_s^{-2p}\\underset{0\\le s\\le 1}{\\max}\\tilde{Y}_s^{-2p}\\Big]<\\infty\\,.$ Here, we prove the first inequality of Eq.\\eqref{eq:TF app}.\\\\\n\\indent For the second inequality of Eq.\\eqref{eq:TF app}, we may also assume that $p\\ge 1\\,.$ Note from the Jensen inequality $(\\frac{x+y}{2})^p\\le\\frac{x^p+y^p}{2}\\,,$ $x,y>0\\,,$ that\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert T\\tilde{F}-T\\hat{F} \\vert^p] \\\\\n    &\\le 2^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-p}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt<\\frac{1}{2}\\}}\\right] \\\\\n    &+2^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\vert\\tilde{Y}_t-\\hat{Y}_t\\vert\\,dt\\right)^{p}\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-p}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^{-p}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt\\ge \\frac{1}{2}\\}}\\right]\\,. \n\\end{align}\nObserve from the Jensen inequality, the H\\"older inequality, and Lemma \\ref{lem:x,y close} that for $0\\le T\\le 1\\,,$\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\vert\\tilde{Y}_t-\\hat{Y}_t\\vert\\,dt\\right)^{p}\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-p}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^{-p}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt\\ge \\frac{1}{2}\\}}\\right]\\\\\n    &\\le 2^p\\,\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\vert\\tilde{Y}_t-\\hat{Y}_t\\vert\\,dt\\right)^{p}\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-p}\\right] \\\\\n    &\\le 2^p\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{Y}_t-\\hat{Y}_t \\vert^{2p}\\,dt]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\tilde{Y}_s^{-2p}\\Big]\\right)^{\\frac{1}{2}}\n    \\le D_pT^p\\,,\n\\end{align}\nwhere $D_p:=2^p\\left(\\frac{B_{2p}}{2p+1}\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le 1}{\\max}\\tilde{Y}_s^{-2p}\\Big]\\right)^{\\frac{1}{2}}$. From Lemma \\ref{lem:dummy}, $D_p<\\infty\\,.$ Next, observe from the Fubini theorem with regard to a stochastic integral and Assumption \\ref{classical assumption} that\n\\begin{align}\n    \\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt<\\frac{1}{2}\\right\\}\n    &=\\mathbb{Q}\\left\\{1+\\frac{1}{T}\\int_0^T\\int_0^T \\nu(s,S_0)\\mathbbm{1}_{\\{s\\le t\\}}\\,dW_s\\,dt<\\frac{1}{2}\\right\\} \\\\\n    &=\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T \\nu(s,S_0)(T-s)\\,dW_s<-\\frac{1}{2}\\right\\} \\\\\n    &=\\mathbb{Q}\\left\\{\\frac{1}{T}\\sqrt{\\int_0^T \\nu^2(s,S_0)(T-s)^2\\,ds}\\,Z<-\\frac{1}{2}\\right\\} \\\\\n    &\\le N\\bigg(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\bigg)\\,,\\label{eq:hatY vanish}\n\\end{align}\nwhere $Z$ denotes a standard normal random variable and $N(\\cdot)$ denotes a cumulative function of $Z\\,.$ Hence, by the Cauchy--Schwarz inequality,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right)^{-p}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T \\hat{Y}_t\\,dt<\\frac{1}{2}\\}}\\right]\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^{-2p}\\right]\\right)^{\\frac{1}{2}}\\left(N\\bigg(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\bigg)\\right)^{\\frac{1}{2}} \\\\\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le 1}\\tilde{Y}_s^{-2p}\\Big]\\right)^{\\frac{1}{2}}\\left(N\\bigg(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\bigg)\\right)^{\\frac{1}{2}}\\,\n\\end{align}\nfor $0\\le T\\le 1\\,.$ By Lemma \\ref{lem:dummy}, $\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le s\\le 1}{\\max}\\tilde{Y}_s^{-2p}\\Big]<\\infty\\,.$ Since $\\left(N\\left(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\right)\\right)^{\\frac{1}{2}}=o(T^{q})$ for any $q>0$ as $T\\rightarrow{0}\\,,$ the second inequality of \\eqref{eq:TF app} is proven, and so is Lemma \\ref{lem:u,TF app}.\n\\end{proof}\n\n\\subsection{Proof for Lemma \\ref{lem:Ds bound}}\\label{proof:Ds bound}\n\\begin{proof}\nFirst, we present the proof for Eq.\\eqref{eq:Dsx bound}. In \\cite{benhamou2000application,NualartDavid1995TMca}, $D_sX_t$ is explicitly expressed as\n\\begin{equation}\\label{eq:DsXt}\n    D_sX_t=\\frac{Y_t}{Y_s}\\sigma(s,X_s)X_s\\mathbbm{1}_{\\{s\\le t\\}}\\,\n\\end{equation}\nunder Assumption \\ref{classical assumption}.\nTherefore, for any $p>0$ and $0\\le t\\le 1\\,,$ by Assumption \\ref{classical assumption},\n\\begin{align}\n    \\vert D_sX_t\\vert^p\n    &\\le \\overline{\\sigma}^p Y_t^p\\Big[\\underset{0\\le u\\le 1}{\\max}\\left(X_u^{p}Y_u^{-p}\\right)\\Big]\\mathbbm{1}_{\\{s\\le t\\}}\\,.\n\\end{align}\nBy taking the expectation $\\mathbb{E}^\\mathbb{Q}$ on both sides, from the H\\"older inequality, we get\n\\begin{align}\n    \\underset{s\\ge 0}{\\sup}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_sX_t\\vert^p]\n    &\\le \\overline{\\sigma}^p\\left(\\mathbb{E}^\\mathbb{Q}[Y_t^{2p}]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le u\\le 1}{\\max}\\left(X_u^{2p}Y_u^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom Lemma \\ref{lem:dummy}, the proof of Eq.\\eqref{eq:Dsx bound} is complete.\\\\\n\\indent Next, let us prove Eq.\\eqref{eq:DsY bound}. The only nontrivial inequality of Eq.\\eqref{eq:DsY bound} is the first one. From \\cite{NualartDavid1995TMca}, $D_s\\tilde{Y}_t$ and $D_s\\hat{Y}_t$ can be computed as\n\\begin{align}\n    D_s\\tilde{Y}_t=\\nu(s,S_0)\\tilde{Y}_t\\mathbbm{1}_{\\{s\\le t\\}}\\,, \\quad D_s\\hat{Y}_t=\\nu(s,S_0)\\mathbbm{1}_{\\{s\\le t\\}}\\,.\\label{proof:DtildeY, DhatY}\n\\end{align}\nTherefore, from Assumption \\ref{classical assumption} and Lemma \\ref{lem:dummy}, it is easy to check that both $\\sup_{s\\ge 0}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}\\tilde{Y}_t \\vert^p]$ and $\\sup_{s\\ge 0}\\,\\mathbb{E}^\\mathbb{Q}[\\vert D_{s}\\hat{Y}_t \\vert^p]$ are bounded by some constant $E_p>0$ in $0\\le t\\le 1\\,.$ To prove the first inequality of Eq.\\eqref{eq:DsY bound}, use Malliavin calculus theory presented in \\cite{NualartDavid1995TMca} to express $D_sY_t$ by\n\\begin{equation}\\label{proof:DY}\n    D_sY_t=Y_t\\left[\\nu(s,X_s)-\\int_0^t \\nu(u,X_u)\\rho(u,X_u)D_sX_u\\,du+\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right]\\mathbbm{1}_{\\{s\\le t\\}}\\,.\n\\end{equation}\nNow, take an absolute value on both sides of Eq.\\eqref{proof:DY}. Using the inequalities $\\vert\\nu\\vert\\le\\alpha\\,,$ $\\vert\\rho\\vert\\le\\alpha$, which follow from Assumption \\ref{classical assumption}, observe that \n\\begin{equation}\n    \\vert D_sY_t\\vert \\le \\left[\\alpha Y_t+\\alpha^2 Y_t\\int_0^t \\vert D_sX_u\\vert\\,du+Y_t\\left\\vert\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert\\right]\\mathbbm{1}_{\\{s\\le t\\}}\\,.\n\\end{equation}\nWe may assume that $p\\ge 1$. For any $s\\ge0\\,,$ by the Jensen inequality,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert D_sY_t\\vert^p]\n    &\\le 3^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}[Y_t^p] +3^{p-1}\\alpha^{2p}\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\int_0^t \\vert D_sX_u\\vert^p\\,du\\right]t^{p-1} \\\\\n    &+3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\left\\vert\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^p\\right]\\,.\n\\end{align}\nNote from the Jensen inequality, the H\\"older inequality, and Eq.\\eqref{eq:Dsx bound} that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\int_0^t \\vert D_sX_u\\vert^p\\,du\\right]t^{p-1}\n    &\\le\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}\\left(\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert D_sX_u\\vert^{2p}]\\,du\\right)^{\\frac{1}{2}}t^{p-\\frac{1}{2}}\\\\\n    &\\le\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}(E_{2p})^{\\frac{1}{2}}t^p\n    \\le\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}(E_{2p})^{\\frac{1}{2}}\\,\\label{proof:DsY bound 1}\n\\end{align}\nfor some positive constant $E_{2p}$ and\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\left\\vert\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^p\\right] \\\\\n    &\\le\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^{2p}\\right]\\right)^{\\frac{1}{2}}\\,\\label{proof:DsY bound 2}\n\\end{align}\nfor $0\\le t\\le 1$ and $s\\ge0\\,.$ Therefore, for $0\\le t\\le 1\\,,$ $s\\ge0\\,,$ we obtain\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert D_sY_t\\vert^p]\n    &\\le 3^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}[Y_t^p] +3^{p-1}\\alpha^{2p}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}(E_{2p})^{\\frac{1}{2}} \\\\\n    &+3^{p-1}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert Y_t\\vert^{2p}]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^{2p}\\right]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom Lemma \\ref{lem:dummy}, $\\mathbb{E}^\\mathbb{Q}[Y_t^p]\\,,$ $\\mathbb{E}^\\mathbb{Q}[Y_t^{2p}]$ are both bounded by some constant in $0\\le t\\le 1\\,.$ Therefore, to establish the first inequality of Eq.\\eqref{eq:DsY bound}, it is left to show that the expectation $\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^{2p}\\right]$ is also bounded from above by some constant in $0\\le t\\le 1\\,.$ \n\\begin{claim}\\label{clm:rhoDsXu is a conti mart}\nFor any fixed $s\\ge0\\,,$ a stochastic process $\\left(\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right)_{t\\ge 0}$ is a continuous martingale adapted to the Brownian filtration $(\\mathcal{F}_t^W)_{t\\ge 0}$.\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:rhoDsXu is a conti mart}]\nFor any fixed $s\\ge0\\,,$ recall from Assumption \\ref{classical assumption} that a map $x\\mapsto\\sigma(s,x)$ is Borel measurable. Then, it is easy to check from Eq.\\eqref{eq:DsXt} that for any fixed $s\\ge0\\,,$ $\\rho(u,X_u)D_sX_u$ is $\\mathcal{F}_u^W$ measurable. From Eq.\\eqref{eq:DsXt} and Assumption \\ref{classical assumption}, check that\n\\begin{align}\n    \\vert D_sX_u\\vert^p\n    &\\le \\overline{\\sigma}^p Y_u^p\\Big[\\underset{0\\le l\\le u}{\\max}\\left(X_l^{p}Y_l^{-p}\\right)\\Big]\\mathbbm{1}_{\\{s\\le u\\}}\\,.\n\\end{align}\nNote that $\\vert\\rho\\vert\\le\\alpha$ from Assumption \\ref{classical assumption}. Therefore, from the H\\"older inequality and Lemma \\ref{lem:dummy}, we can obtain\n\\begin{align}\n    \\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert \\rho(u,X_u)D_sX_u\\vert^2]\\,du &\\le \\alpha^2\\int_0^t \\mathbb{E}^\\mathbb{Q}[\\vert D_sX_u\\vert^2]\\,du \\\\\n    &\\le \\alpha^2\\overline{\\sigma}^p\\int_0^t \\left(\\mathbb{E}^\\mathbb{Q}[Y_u^{2p}]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le l\\le u}{\\max}\\left(X_l^{2p}Y_l^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}\\,du \\\\\n    &\\le \\alpha^2\\overline{\\sigma}^p \\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le l\\le t}Y_l^{2p}\\Big]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\underset{0\\le l\\le t}{\\max}\\left(X_l^{2p}Y_l^{-2p}\\right)\\Big]\\right)^{\\frac{1}{2}}t\n    <\\infty\\,\n\\end{align}\nfor every $t>0\\,.$ Hence, the Ito integral $\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u$ is well defined and it is a continuous martingale.\n\\end{proof}\n\\noindent According to Claim \\ref{clm:rhoDsXu is a conti mart}, $\\left(\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right)_{t\\ge 0}$ is a continuous martingale starting at 0. Thus, from the Burkholder--Davis--Gundy inequality, the Jensen inequality, Assumption \\ref{classical assumption}, and Eq.\\eqref{eq:Dsx bound},\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\int_0^t\\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^{2p}\\right]\n    &\\le C_p \\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^t\\vert\\rho(u,X_u)\\vert^2\\vert D_sX_u\\vert^2\\,du\\right)^p\\right] \\\\\n    &\\le C_p\\alpha^{2p}t^{p-1}\\int_0^t\\mathbb{E}^\\mathbb{Q}[\\vert D_sX_u\\vert^{2p}]\\,du \\\\\n    &\\le C_p\\alpha^{2p}E_{2p}t^p\n    \\le C_p\\alpha^{2p}E_{2p}\\, \\label{proof:DsY bound 3}\n\\end{align}\nfor $0\\le t\\le1\\,,$ $s\\ge 0$ with some constant $C_p\\,,E_{2p}>0\\,.$ Hence, we complete the proof of Eq.\\eqref{eq:DsY bound}\\,.\\\\\n\\indent Finally, we will examine the proof of Eq.\\eqref{eq:TDsF bound}. Note from \\cite{NualartDavid1995TMca} that $TD_sF$ can be expressed as $TD_sF=-\\frac{1}{T}\\int_0^T D_sY_t\\,dt\\left(\\frac{1}{T}\\int_0^T Y_t\\,dt\\right)^{-2}$.\nAssume that $p\\ge 1\\,.$ Observe from the Jensen inequality, the H\\"older inequality, Lemma \\ref{lem:dummy}, and Eq.\\eqref{eq:DsY bound} that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert TD_sF\\vert^p]\n    &\\le \\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T \\vert D_sY_t\\vert^p\\,dt\\right)\\left(\\max_{0\\le t\\le T}Y_t^{-2p}\\right)\\right] \\\\\n    &\\le \\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert D_sY_t\\vert^{2p}]\\,dt\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le t\\le T}Y_t^{-4p}\\Big]\\right)^{\\frac{1}{2}} \\\\\n    &\\le(E_{2p})^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le u\\le 1}Y_u^{-4p}\\Big]\\right)^{\\frac{1}{2}}<\\infty\\,\n\\end{align}\nfor $0\\le T\\le 1$ and any $s\\ge 0\\,.$ This proves the first inequality of Eq.\\eqref{eq:TDsF bound}. The other two inequalities in Eq.\\eqref{eq:TDsF bound} can be established similarly.\n\\end{proof}\n\n\\subsection{Proof of Lemma \\ref{lem:Ds close}}\\label{proof:Ds close}\n\\begin{proof}\nWe will prove the first inequality of Eq.\\eqref{eq:DsY close}. Assume that $p\\ge 1\\,.$ From Eqs.\\eqref{proof:DtildeY, DhatY} and \\eqref{proof:DY}, we derive the following inequality under Assumption \\ref{classical assumption}:\n\\begin{align}\n    &\\vert D_sY_t-D_s\\tilde{Y}_t \\vert \\\\\n    &\\le \\vert Y_t\\nu(s,X_s)-\\tilde{Y}_t\\nu(s,S_0) \\vert\\mathbbm{1}_{\\{s\\le t\\}}\n    +\\alpha^2 Y_t\\int_0^t \\vert D_sX_u \\vert\\,du\n    +Y_t\\left\\vert \\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u \\right\\vert\\,\n\\end{align}\nfor any $s\\ge0\\,.$ Therefore, from the inequality $(\\frac{x+y+z}{3})^p\\le\\frac{x^p+y^p+z^p}{3}\\,,$ $x,y,z>0$ and the Jensen inequality,\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert D_sY_t-D_s\\tilde{Y}_t\\vert^p]\\\\\n    &\\le 3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\big\\vert Y_t\\nu(s,X_s)-\\tilde{Y_t}\\nu(s,S_0)\\big\\vert^p\\right]\\mathbbm{1}_{\\{s\\le t\\}}\n    +3^{p-1}\\alpha^{2p}\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\int_0^t \\vert D_sX_u\\vert^p\\,du\\right]t^{p-1}\\\\\n    &+3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\left\\vert\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^p\\right]\\,.\n\\end{align}\nObserve from the inequality Eq.\\eqref{proof:DsY bound 1} that we have already proved $\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\int_0^t \\vert D_sX_u\\vert^p\\,du\\right]t^{p-1}\\le F_pt^{\\frac{p}{2}}\\,,$ for some positive constant $F_p$ in $0\\le t\\le 1\\,.$ Similarly, the inequalities Eq.\\eqref{proof:DsY bound 2}, Eq.\\eqref{proof:DsY bound 3} imply the bound $\\mathbb{E}^\\mathbb{Q}\\left[Y_t^p\\left\\vert\\int_0^t \\rho(u,X_u)D_sX_u\\,dW_u\\right\\vert^p\\right]\\le F_pt^{\\frac{p}{2}}$ in $0\\le t\\le 1\\,$ for some positive constant $F_p\\,.$ Thus, it is sufficient to show that $\\mathbb{E}^\\mathbb{Q}\\left[\\big\\vert Y_t\\nu(s,X_s)-\\tilde{Y_t}\\nu(s,S_0)\\big\\vert^p\\right]\\mathbbm{1}_{\\{s\\le t\\}}\\le F_pt^{\\frac{p}{2}}$ for any $0\\le t\\le 1\\,,$ $s\\ge 0$ with some positive constant $F_p\\,.$ Now, observe from the Jensen inequality and Assumption \\ref{classical assumption} that\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\big\\vert Y_t\\nu(s,X_s)-\\tilde{Y_t}\\nu(s,S_0)\\big\\vert^p\\right]\\mathbbm{1}_{\\{s\\le t\\}} \\\\\n    &\\le 2^{p-1}\\mathbb{E}^\\mathbb{Q}\\Big[Y_t^p\\vert\\nu(s,X_s)-\\nu(s,S_0)\\vert^p\\Big]\\mathbbm{1}_{\\{s\\le t\\}}\n    +2^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\vert \\nu(s,S_0) \\vert^p\\vert Y_t-\\tilde{Y_t}\\vert^p\\right] \\\\\n    &\\le 2^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}\\Big[Y_t^p\\max_{0\\le s\\le t}\\vert X_s-S_0 \\vert^p\\Big]\n    +2^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t \\vert^p]\\,.\n\\end{align}\nFrom Lemma \\ref{lem:x,y close} with $0\\le t\\le 1\\,,$ $\\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t \\vert^p]\\le B_pt^{\\frac{p}{2}}\\,$ for some constant $B_p>0\\,.$ Furthermore, by the H\\"older inequality and the Doob $L^p$ inequality,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\Big[Y_t^p\\max_{0\\le s\\le t}\\vert X_s-S_0 \\vert^p\\Big]\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}[Y_t^{2p}]\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\Big[\\max_{0\\le s\\le t} \\vert X_s-S_0\\vert^{2p}\\Big]\\right)^{\\frac{1}{2}}\\\\\n    &\\le \\left(\\mathbb{E}^\\mathbb{Q}[Y_t^{2p}]\\right)^{\\frac{1}{2}}\\left(\\frac{2p}{2p-1}\\right)^{p}\\left(\\mathbb{E}^\\mathbb{Q}[\\vert X_t-S_0 \\vert^{2p}]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nSince $\\mathbb{E}^\\mathbb{Q}[Y_t^{2p}]$ is bounded by a constant in $0\\le t\\le 1$ from Lemma \\ref{lem:dummy}\\,, it is left to show that the expectation $\\mathbb{E}^\\mathbb{Q}[\\vert X_t-S_0\\vert^{2p}]$ is no greater than some constant $F_p$ times $t^p$ in $0\\le t\\le 1\\,.$ This can be obtained from the inequality $(\\frac{x+y+z}{3})^{2p}\\le\\frac{x^{2p}+y^{2p}+z^{2p}}{3}\\,,$ $x,y,z>0$, and Lemma \\ref{lem:x,y close}:\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}[\\vert X_t-S_0\\vert^{2p}]\\\\\n    &\\le 3^{2p-1}\\mathbb{E}^\\mathbb{Q}[\\vert X_t-\\tilde{X}_t\\vert^{2p}]\n    +3^{2p-1}\\mathbb{E}^\\mathbb{Q}[\\vert\\tilde{X}_t-\\hat{X}_t\\vert^{2p}]\n    +3^{2p-1}\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^{2p}]\\\\\n    &\\le 3^{2p-1}B_{2p}t^{2p}+3^{2p-1}B_{2p}t^{2p}+3^{2p-1}\\overline{\\sigma}^{2p}S_0^{2p}M(2p)t^{p}\\,,\n\\end{align}\nwhere $M(p)$ denotes $\\mathbb{E}^\\mathbb{Q}[\\vert Z \\vert^p]$ while $Z$ denotes a standard normal random variable. Note the last inequality comes from direct computation because $\\hat{X}_t$ is a normal random variable. Hence, we prove the first inequality of Eq.\\eqref{eq:DsY close}. \\\\\n\\indent Next, we prove the second inequality of Eq.\\eqref{eq:DsY close}. This easily comes from Eq.\\eqref{proof:DtildeY, DhatY} and straightforward use of Assumption \\ref{classical assumption}, the Jensen inequality, and Lemma \\ref{lem:x,y close}. Assume that $p\\le1$ and observe that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert D_s\\tilde{Y}_t-D_s\\hat{Y}_t\\vert^p]\n    &=\\mathbb{E}^\\mathbb{Q}[\\vert \\nu(s,S_0)(\\tilde{Y}_t-1)\\mathbbm{1}_{\\{s\\le t\\}}\\vert^p] \\\\\n    &\\le\\alpha^p\\mathbb{E}^\\mathbb{Q}[\\vert\\tilde{Y}_t-1\\vert^p] \\\\\n    &\\le 2^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}[\\vert \\tilde{Y}_t-\\hat{Y}_t\\vert^p]\n    +2^{p-1}\\alpha^p\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{Y}_t-1\\vert^p] \\\\\n    &\\le 2^{p-1}\\alpha^pB_pt^p\n    +2^{p-1}\\alpha^{2p}M(p)t^{\\frac{p}{2}}\\,,\n\\end{align}\nwhere $M(p)$ denotes $\\mathbb{E}^\\mathbb{Q}[\\vert Z\\vert^p]$ while $Z$ denotes a standard normal random variable. The last inequality comes from direct computation with regard to a normal random variable $\\hat{Y}_t\\,.$ This completes the proof of Eq.\\eqref{eq:DsY close}\\,. \\\\\n\\indent Finally, we will prove Eq.\\eqref{eq:TDsF close}. In \\cite{NualartDavid1995TMca}, Malliavin calculus gives\n\\begin{align}\n    TD_sF=\\frac{-\\frac{1}{T}\\int_0^TD_sY_t\\,dt}{\\left(\\frac{1}{T}\\int_0^TY_t\\,dt\\right)^2}\\,, \\quad\n    TD_s\\tilde{F}=\\frac{-\\frac{1}{T}\\int_0^TD_s\\tilde{Y}_t\\,dt}{\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^2}\\,.\n\\end{align}\nIf $p\\ge 1$, by the Jensen inequality, $\\mathbb{E}^\\mathbb{Q}[\\vert TD_sF-TD_s\\tilde{F}\\vert^p] \\le 2^{p-1}\\mathbb{E}^\\mathbb{Q}[L_T^p]+2^{p-1}\\mathbb{E}^\\mathbb{Q}[R_T^p]\\,,$\nwhere $L_T\\,,$ $R_T$ are given by\n\\begin{align}\n    L_T\n    :=\\left\\vert \\frac{\\frac{1}{T}\\int_0^TD_sY_t\\,dt}{\\left(\\frac{1}{T}\\int_0^TY_t\\,dt\\right)^2}-\\frac{\\frac{1}{T}\\int_0^TD_sY_t\\,dt}{\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^2}\\right\\vert\\,, \\quad\n    R_T\n    :=\\left\\vert\\frac{\\frac{1}{T}\\int_0^TD_sY_t\\,dt}{\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^2}-\\frac{\\frac{1}{T}\\int_0^TD_s\\tilde{Y}_t\\,dt}{\\left(\\frac{1}{T}\\int_0^T\\tilde{Y}_t\\,dt\\right)^2} \\right\\vert\\,.\n\\end{align}\nThen, for $A_T$, which satisfies the equality\n\\begin{align}\n    L_T=A_T\\left\\vert \\frac{1}{T}\\int_0^T Y_t\\,dt-\\frac{1}{T}\\int_0^T \\tilde{Y}_t\\,dt\\right\\vert\\,,\n\\end{align}\nwe observe from the Jensen inequality, the H\\"older inequality, and Lemma \\ref{lem:x,y close} that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[L_T^p]\n    \\le\\left(\\mathbb{E}^\\mathbb{Q}[A_T^{2p}]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert Y_t-\\tilde{Y}_t\\vert^{2p}]\\,dt\\right)^{\\frac{1}{2}}\n    \\le\\left(\\mathbb{E}^\\mathbb{Q}[A_T^{2p}]\\right)^{\\frac{1}{2}}\\left(\\frac{B_{2p}}{2p+1}\\right)^{\\frac{1}{2}}T^{\\frac{p}{2}}\\,\n\\end{align}\nfor $0\\le T\\le 1\\,.$ From Lemmas \\ref{lem:dummy} and \\ref{lem:Ds bound}, it is easy to prove that $\\mathbb{E}^\\mathbb{Q}[A_T^{2p}]$ is bounded above by some constant in $0\\le T\\le 1\\,.$ Therefore, $\\mathbb{E}^\\mathbb{Q}[L_T^p]\\le F_pT^{\\frac{p}{2}}$ for some positive constant $F_p\\,.$ Next, we examine $\\mathbb{E}^\\mathbb{Q}[R_T^p]\\,.$ From the Jensen inequality, the H\\"older inequality, and Eq.\\eqref{eq:DsY close} already established,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[R_T^p]\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\frac{1}{T}\\int_0^T D_sY_t-D_s\\tilde{Y}_t\\,dt\\right\\vert^p(T\\tilde{F})^{2p}\\right] \\\\\n    &\\le \\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert D_sY_t-D_s\\tilde{Y}_t\\vert^{2p}]\\,dt\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[(T\\tilde{F})^{4p}]\\right)^{\\frac{1}{2}} \\\\\n    &\\le\\left(\\frac{F_{2p}}{p+1}\\right)^{\\frac{1}{2}}T^{\\frac{p}{2}}\\left(\\mathbb{E}^\\mathbb{Q}[(T\\tilde{F})^{4p}]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom Lemma \\ref{lem:dummy}, $\\mathbb{E}^\\mathbb{Q}[(T\\tilde{F})^{4p}]$ is bounded above by a constant in $0\\le T\\le 1\\,.$ This completes the proof. The second inequality of Eq.\\eqref{eq:TDsF close} can be obtained similarly except that we should additionally control the indicator function of $\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}$ shown in the definition of $D_s^{*}\\hat{F}\\,.$ However, we can resolve this subtle difference with the same technique as that used to prove Lemma \\ref{lem:u,TF app} in Appendix \\ref{proof: u,TF app}.\n\\end{proof}\n\n\\subsection{Proof of Lemma \\ref{lem:hat u app}}\\label{proof:hat u app}\n\\begin{proof}\nObserve that $(\\hat{u}_s)_{s\\ge 0}$ is adapted to the Brownian filtration $(\\mathcal{F}_s^W)_{s\\ge 0}$, and from Assumption \\ref{classical assumption},\n\\begin{align}\n    \\int_0^t\\mathbb{E}^\\mathbb{Q}[\\hat{u}_s^2]\\,ds\n    \\le \\int_0^t\\frac{16\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_s^4]}{\\underline{\\sigma}^2S_0^2}\\,ds\n    \\le \\int_0^t\\frac{16(1+6\\alpha^2s+3\\alpha^4s^2)}{\\underline{\\sigma}^2S_0^2}\\,ds < \\infty\\,\n\\end{align}\nfor every $t>0\\,.$ For the second inequality, we directly calculate the upper bound of $\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_s^4]$ from Assumption \\ref{classical assumption} and the fact that $\\hat{Y}_s$ is a normal random variable. Thus, the Skorokhod integral $\\delta(\\hat{u})$ coincides with the Ito integral of $\\hat{u}_s$ in $s\\in[0,T]$. Furthermore, it is implied that $\\left(\\int_0^t \\hat{u}_s\\,dW_s\\right)_{t\\ge 0}$ is a continuous martingale starting at 0. Therefore, by the Burkholder--Davis--Gundy inequality and the Jensen inequality, if $p\\ge 2$,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert \\delta(\\hat{u})\\vert^p]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\int_0^T\\hat{u}_s\\,dW_s\\right\\vert^p\\right]\n    \\le C_p\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^T\\hat{u}_s^2\\,ds\\right)^\\frac{p}{2}\\right]\n    \\le C_p T^{\\frac{p}{2}-1}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^{p}]\\,ds\\,\n\\end{align}\nfor some constant $C_p>0\\,.$ Note from Assumption \\ref{classical assumption} and the Doob $L^p$ inequality that for $0\\le s\\le T\\,,$\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^p]\n    \\le \\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{Y}_s\\vert^{2p}]\n    \\le \\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\mathbb{E}^\\mathbb{Q}\\Big[\\Big(\\max_{0\\le s\\le T}\\vert\\hat{Y}_s\\vert\\Big)^{2p}\\Big]\n    \\le \\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\left(\\frac{2p}{2p-1}\\right)^{2p}\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{Y}_T\\vert^{2p}]\\,.\n\\end{align}\nThis results in the following inequality.\n\\begin{align}\n    \\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^{p}]\\,ds\n    \\le \\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\left(\\frac{2p}{2p-1}\\right)^{2p}\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{Y}_T\\vert^{2p}]\\,T\\,.\n\\end{align}\nSince $\\hat{Y}_T$ is a normal random variable with mean $1\\,,$ it is easy to confirm that $\\mathbb{E}^\\mathbb{Q}[\\vert \\hat{Y}_T\\vert^{2p}]$ is bounded by some constant, say $G_p>0$, which depends only on $p\\,,$ for any $0\\le T\\le 1\\,.$ From the inequalities established in this paragraph, we get that\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert \\delta(\\hat{u})\\vert^p]\n    \\le C_p\\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\left(\\frac{2p}{2p-1}\\right)^{2p}G_p\\,T^{\\frac{p}{2}}\\,\n\\end{align}\nholds for any $0\\le T\\le 1\\,.$ Rename the constant $C_p\\frac{4^p}{\\underline{\\sigma}^pS_0^p}\\left(\\frac{2p}{2p-1}\\right)^{2p}G_p$ as $G_p$ to get the first inequality of Lemma \\ref{lem:hat u app}.\\\\\n\\indent Next, we will prove the second inequality of Eq.\\eqref{eq:hat u app}. For simplicity, we use the notation $g_s:=\\hat{u}_s-\\frac{2}{\\sigma(s,S_0)S_0}$ for the remainder of the proof. Note from the Burkholder--Davis--Gundy inequality and the Jensen inequality that if $p\\ge 2$,\n\\begin{equation}\\label{proof:hat u close 1}\n    \\mathbb{E}^\\mathbb{Q}[\\vert\\delta(g)\\vert^p]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\int_0^T g_s\\,dW_s\\right\\vert^p\\right]\n    \\le C_p \\mathbb{E}^\\mathbb{Q}\\left[\\left(\\int_0^T g_s^2\\,ds\\right)^{\\frac{p}{2}}\\right]\n    \\le C_p T^{\\frac{p}{2}-1}\\int_0^T \\mathbb{E}^\\mathbb{Q}[\\vert g_s\\vert^p]\\,ds\\,.\n\\end{equation}\nUsing the inequality $(\\frac{x+y+z}{3})^p\\le\\frac{x^p+y^p+z^p}{3}\\,,$ $x,y,z>0\\,,$ we get\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}[\\vert g_s\\vert^p]\n    &\\le 3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\hat{u}_s-\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}\\right\\vert^p\\right]\n    +3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}\\mathbbm{1}_{\\{\\hat{X}_s<\\frac{S_0}{2}\\}}\\right\\vert^p\\right] \\\\\n    &+3^{p-1}\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}-\\frac{2}{\\sigma(s,S_0)S_0}\\right\\vert^p\\right]\\,.\\label{proof:hat u close 2}\n\\end{align}\nFirst, note that there is a positive constant $G_p^{(1)}$ that depends only on $p$ such that\n\\begin{equation}\\label{proof:hat u close 3}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\hat{u}_s-\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}\\mathbbm{1}_{\\{\\hat{X}_s\\ge\\frac{S_0}{2}\\}}\\right\\vert^p\\right]\n    \\le \\frac{4^p\\alpha^p}{\\underline{\\sigma}^{2p}S_0^{2p}}\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_s\\vert^{2p}\\vert\\hat{X}_s-S_0\\vert^p]\n    \\le G_p^{(1)}s^{\\frac{p}{2}}\\,\n\\end{equation}\nholds for any $0\\le s\\le 1\\,.$ The first inequality comes from Assumption \\ref{classical assumption}\\,. Since $\\hat{Y}_s\\,,$ $\\hat{X}_s$ are normal variables, it is easy to check that in $0\\le s\\le 1\\,,$ $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_s\\vert^{2p}\\vert\\hat{X}_s-S_0\\vert^p]$ is dominated by $s^{\\frac{p}{2}}$ up to a constant multiplication. Therefore, we can choose a proper $G_p^{(1)}\\,.$ Second, observe that there is a constant $G_p^{(2)}>0$ such that the following inequality holds for any $0\\le s\\le 1\\,.$ \n\\begin{equation}\\label{proof:hat u close 4}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}\\mathbbm{1}_{\\{\\hat{X}_s<\\frac{S_0}{2}\\}}\\right\\vert^p\\right]\n    \\le G_p^{(2)}s^{\\frac{p}{2}}\\,.\n\\end{equation}\nWe obtain this bound from the following observations. For $0\\le s\\le 1\\,,$ it is easy to check that $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_s\\vert^{2p}]$ is bounded by some constant. Furthermore, from the inequality $\\mathbb{Q}\\{\\hat{X}_s<\\frac{S_0}{2}\\}\\le N\\left(-\\frac{1}{2\\overline{\\sigma}\\sqrt{s}}\\right)\\,$ established in \\eqref{eq:hatx vanish}, we can observe that $\\mathbb{Q}\\{\\hat{X}_s<\\frac{S_0}{2}\\}=o(s^q)$ for any $q>0\\,$ as $s\\rightarrow{0}\\,.$ From these observations, we can find a suitable constant $G_p^{(2)}\\,.$ Third, observe from Assumption \\ref{classical assumption} that\n\\begin{equation}\\label{proof:hat u close 5}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\frac{2\\hat{Y}_s^2}{\\sigma(s,S_0)S_0}-\\frac{2}{\\sigma(s,S_0)S_0}\\right\\vert^p\\right]\n    \\le \\frac{2^p}{\\underline{\\sigma}^pS_0^p}\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_s^2-1\\vert^{2p}]\n    \\le G_p^{(3)}s^{\\frac{p}{2}}\\,\n\\end{equation}\nholds for any $0\\le s\\le 1\\,,$ with some positive constant $G_p^{(3)}\\,.$ The constant $G_p^{(3)}$ can also be found by direct calculation with regard to a normal random variable $\\hat{Y}_s\\,.$ Finally, from the inequalities Eqs.\\eqref{proof:hat u close 1}, \\eqref{proof:hat u close 2}, \\eqref{proof:hat u close 3}, \\eqref{proof:hat u close 4}, and \\eqref{proof:hat u close 5} established in this paragraph, we obtain the desired result from\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[\\vert\\delta(g)\\vert^p]\n    \\le C_pT^{\\frac{p}{2}-1}\\int_0^T3^{p-1}(G_p^{(1)}+G_p^{(2)}+G_p^{(3)})s^{\\frac{p}{2}}\\,ds\n    \\le G_p T^p\\,,\n\\end{equation}\nwith a suitable $G_p\\,.$ This completes the proof for Lemma \\ref{lem:hat u app}.\n\\end{proof}\n\n\n\\subsection{Proof for Proposition \\ref{prop:Asian delta app 3}}\\label{proof:Asian delta app 3}\n\\begin{proof}\nFirst, we will prove Eq.\\eqref{eq:Asian delta app 3.1}. The proof comprises five claims: Claims \\ref{clm:Asian delta 1.0}--\\ref{clm:Asian delta 1.4}. Throughout the proof, we will use the notation $\\Delta_A^{*}(T)$, which is defined as\n\\begin{equation}\n    \\Delta_A^{*}(T):=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta(\\hat{u})\\right]\\,.\n\\end{equation}\n\\begin{claim}\\label{clm:Asian delta 1.0}\n\\begin{equation}\n    \\Delta_A^{*}(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{equation}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 1.0}]\nFrom $\\mathbb{E}^\\mathbb{Q}[\\delta(\\hat{u})]=0$, $\\mathbb{E}^\\mathbb{Q}\\left[\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\right]=0$, we get\n\\begin{align}\n    &\\left\\vert \\Delta_A^{*}(T)-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T\\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\right]\\right\\vert \\\\\n    &\\le \\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\Phi\\left(\\frac{1}{T}\\int_0^T\\hat{X}_t\\,dt\\right)-\\Phi(S_0)\\right\\vert\\frac{1}{T}\\vert\\delta(g)\\vert\\right]\\,,\n\\end{align}\nwhere $g_s:=\\hat{u}_s-\\frac{2}{\\sigma(s,S_0)S_0}\\,.$ Use Assumption \\ref{payoff assumption}, the Jensen inequality, and the H\\"older inequality to note that\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert \\Phi\\left(\\frac{1}{T}\\int_0^T\\hat{X}_t\\,dt\\right)-\\Phi(S_0)\\right\\vert\\frac{1}{T}\\vert\\delta(g)\\vert\\right] \\\\\n    &\\le \\beta \\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^2]\\,dt\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\delta(g)\\right)^2\\right]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom the inequality $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^2]\\,dt=\\mathcal{O}(T)$ and Lemma \\ref{lem:hat u app} where $\\mathbb{E}^\\mathbb{Q}[\\delta(g)^2]\\le G_2T^2\\,,$ we get\n\\begin{align}\n    \\Delta_A^{*}(T)=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T\\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\nNext, from the Fubini theorem with regard to a stochastic integral,\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T\\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{1}{T}\\int_0^T\\int_0^t\\sigma(s,S_0)S_0\\,dW_s\\,dt\\right)\\frac{1}{T}\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}\\,dW_s\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{S_0}{T}\\int_0^T\\sigma(s,S_0)(T-s)\\,dW_s\\right)\\frac{1}{T}\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}\\,dW_s\\right]\\,.\n\\end{align}\nThen, $X:=\\int_0^T\\sigma(s,S_0)(T-s)\\,dW_s$ and $Y:=\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}\\,dW_s$ are multivariate normal random variables satisfying\n\\begin{equation}\n    X \\,\\perp\\, Y-\\frac{T^2/S_0}{\\int_0^T\\sigma^2(s,S_0)(T-s)^2\\,ds}X\\,.\n\\end{equation}\nThis is followed by\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{S_0}{T}X\\right)\\frac{1}{T}Y\\right]\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(S_0+\\frac{S_0}{T}X\\right)\\frac{1}{T}\\left(\\frac{T^2/S_0}{\\int_0^T\\sigma^2(s,S_0)(T-s)^2\\,ds}X\\right)\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_A(T)\\sqrt{T}Z)}{S_0\\sigma_A(T)\\sqrt{T}}Z\\right]\\,.\n\\end{align}\nTherefore, we get the desired result.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 1.1}\n\\begin{align}\n    \\Delta_A^{*}(T)\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta(\\hat{u})\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 1.1}]\nWe may assume that $\\Phi(0)=0\\,.$ Otherwise, consider a translation $\\Phi(\\cdot)-\\Phi(0)$ and follow the arguments below to get the result. Use Assumption \\ref{payoff assumption}, the Jensen inequality, and the H\\"older inequality to get \n\\begin{align}\n    &\\left\\vert\\Delta_A^{*}(T)\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta(\\hat{u})\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\\right]\\right\\vert \\\\\n    &=\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta(\\hat{u})\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt<\\frac{1}{2}\\}}\\right]\\right\\vert \\\\\n    &\\le \\beta\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t\\vert^3]\\,dt\\right)^{\\frac{1}{3}}\n    \\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\bigg]\\right)^{\\frac{1}{3}}\n    \\left(\\frac{\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt<\\frac{1}{2}\\right\\}}{T\\sqrt{T}}\\right)^{\\frac{1}{3}}\\,.\n\\end{align}\nSince $\\hat{X}_t$ is a normal random variable with mean $S_0\\,,$ we get the following asymptotic relation for small $T>0$: $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t\\vert^3]\\,dt=\\mathcal{O}(1)\\,.$ From Lemma \\ref{lem:hat u app}, we prove the bound $\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\right]\\le G_3\\,.$ Therefore, from the inequality $\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt<\\frac{1}{2}\\right\\}\\le N\\left(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\right)$ in Eq.\\eqref{eq:hatY vanish}, we get the desired result because $N\\left(-\\frac{\\sqrt{3}}{2\\alpha\\sqrt{T}}\\right)=o(T^q)$ for any $q>0\\,$ as $T\\rightarrow{0}\\,.$\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 1.2}\n\\begin{align}\n    \\Delta_A^{*}(T)-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right] \n    =\\Phi(S_0)\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 1.2}]\nNote the next equality from the definition of $\\hat{F}\\,$:\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\frac{1}{T}\\delta(\\hat{u})\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\\right]-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\\,.\\label{proof:asian delta 1}\n\\end{align}\nFor simplicity, denote the expectation on the right-hand side of Eq.\\eqref{proof:asian delta 1} by $E_T$ only in the proof of this claim.\nApply Claim \\ref{clm:Asian delta 1.1} to this equality. Then,\n\\begin{align}\n    \\Delta_A^{*}(T)-\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\delta(\\hat{u})\\hat{F}\\right]\n    =E_T+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\nThus, it suffices to show that\n\\begin{align}\n    E_T=\\Phi(S_0)\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\nTo show this, observe from the inequality $T\\hat{F}\\le 2\\,,$ Assumption \\ref{payoff assumption}, the Jensen inequality, and the H\\"older inequality that\n\\begin{align}\n    &\\left\\vert E_T-\\Phi(S_0)\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\\right\\vert \\\\\n    &\\le\\frac{2}{\\sqrt{T}}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)-\\Phi(S_0)\\right\\vert\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\,\\frac{1}{T}\\int_0^T\\vert\\hat{Y}_t-1\\vert\\,dt\\right] \\\\\n    &\\le \\frac{2\\beta}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_0^T\\vert\\hat{X}_t-S_0\\vert\\,dt\\,\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\,\\frac{1}{T}\\int_0^T\\vert\\hat{Y}_t-1\\vert\\,dt\\right] \\\\\n    &\\le \\frac{2\\beta}{\\sqrt{T}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^3]\\,dt\\right)^{\\frac{1}{3}}\\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\bigg]\\right)^{\\frac{1}{3}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^3]\\,dt\\right)^{\\frac{1}{3}}\\,.\n\\end{align}\nOn the one hand, note from Lemma \\ref{lem:hat u app} that $\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\right]\\le G_3\\,.$ On the other hand, by direct computation, $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^3]\\,dt$ and $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^3]\\,dt$ are both $\\mathcal{O}(T\\sqrt{T})\\,$ as $T\\rightarrow{0}$ because $\\hat{X}_t\\,,$ $\\hat{Y}_t$ are normal random variables with mean $S_0\\,,$ $1\\,,$ respectively. This proves the claim.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 1.3}\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta(\\hat{u})\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 1.3}]\nThis easily comes from straightforward use of the Jensen inequality and the H\\"older inequality with Lemma \\ref{lem:hat u app}. More precisely,\n\\begin{align}\n    &\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{u})\\hat{F}\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta(\\hat{u})\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\\right\\vert \\\\\n    &\\le \\frac{1}{\\sqrt{T}}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\vert T\\hat{F}-1\\vert\\frac{1}{T}\\int_0^T\\vert \\hat{Y}_t-1\\vert\\,dt\\right] \\\\\n    &\\le\\frac{1}{\\sqrt{T}}\\left(\\mathbb{E}^\\mathbb{Q}\\bigg[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\bigg]\\right)^{\\frac{1}{3}}\n    \\left(\\mathbb{E}^\\mathbb{Q}[\\vert T\\hat{F}-1\\vert^3]\\right)^{\\frac{1}{3}}\n    \\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^3]\\,dt\\right)^{\\frac{1}{3}}\\,.\n\\end{align}\nHere, $\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{\\sqrt{T}}\\vert\\delta(\\hat{u})\\vert\\right)^3\\right]\\le G_3$ from Lemma \\ref{lem:hat u app}. Furthermore, it is easy to check that $\\mathbb{E}^\\mathbb{Q}[\\vert T\\hat{F}-1\\vert^3]=\\mathcal{O}(T\\sqrt{T})$ and $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^3]\\,dt=\\mathcal{O}(T\\sqrt{T})\\,.$ This proves the claim.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 1.4}\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta(\\hat{u})\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\n    =\\frac{2}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds+\\mathcal{O}(\\sqrt{T})\\,. \n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 1.4}]\nObserve from the Jensen inequality and the H\\"older inequality that\n\\begin{align}\n    &\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta(\\hat{u})\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\\right\\vert \\\\\n    &\\le \\frac{1}{T}\\left(\\mathbb{E}^\\mathbb{Q}[\\delta(g)^2]\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^2]\\,dt\\right)^{\\frac{1}{2}}\\,,\n\\end{align}\nwhere $g_s:=\\hat{u}_s-\\frac{2}{\\sigma(s,S_0)S_0}\\,.$\nFrom Lemma \\ref{lem:hat u app}, $\\mathbb{E}^\\mathbb{Q}[\\delta(g)^2]\\le G_2 T^2\\,.$ Furthermore, by a direct computation, we can check that $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_t-1\\vert^2]\\,dt=\\mathcal{O}(T)\\,.$ Therefore,\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta(\\hat{u})\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\nNote from the Ito isometry that\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt-1\\right)\\right]\\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\frac{1}{T}\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}\\,dW_s\\right)\\left(\\frac{1}{T}\\int_0^T\\nu(s,S_0)(T-s)\\,dW_s\\right)\\right]\\\\\n    &=\\frac{2}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds\\,.\n\\end{align}\n\\noindent This completes the proof.\n\\end{proof}\n\\noindent Concatenate the inequalities established through Claims \\ref{clm:Asian delta 1.0}--\\ref{clm:Asian delta 1.4} to obtain Eq.\\eqref{eq:Asian delta app 3.1}. \\\\\n\\indent Now, we will prove Eq.\\eqref{eq:Asian delta app 3.2}. We divide the proof into four claims.\n\\begin{claim}\\label{clm:Asian delta 2.1}\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\n    =\\Phi(S_0)\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 2.1}]\nObserve from Assumption \\ref{payoff assumption}, the Jensen inequality, and the H\\"older inequality that\n\\begin{align}\n    &\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\Phi\\left(\\frac{1}{T}\\int_0^T \\hat{X}_t\\,dt\\right)\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]-\\Phi(S_0)\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\\right\\vert \\\\\n    &\\le \\beta\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^2]\\,dt\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\hat{u}_s^2( TD_s^{*}\\hat{F})^2]\\,ds\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom direct calculation, we obtain $\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_t-S_0\\vert^2]\\,dt=\\mathcal{O}(T)\\,.$ Note from Lemma \\ref{lem:Ds bound} and the H\\"older inequality that for any $0\\le s \\le T\\le 1\\,,$\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[\\hat{u}_s^2( TD_s^{*}\\hat{F})^2]\n    \\le \\left(\\mathbb{E}^\\mathbb{Q}[\\hat{u}_s^4]\\right)^{\\frac{1}{2}}(E_4)^{\\frac{1}{2}}\n    \\le \\frac{16}{\\underline{\\sigma}^2S_0^2}\\left(\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_s^8]\\right)^{\\frac{1}{2}}(E_4)^{\\frac{1}{2}}\\,.\n\\end{equation}\nWe obtain the second inequality from the definition of $\\hat{u}_s$ and Assumption \\ref{classical assumption}. Moreover, it is easy to check that $s\\mapsto\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_s^8]$ is bounded by some constant in $0\\le s\\le 1\\,.$ Therefore, as $T\\rightarrow{0}\\,,$\n\\begin{equation}\\label{proof:asian delta 2}\n    \\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^2\\vert TD_s^{*}\\hat{F}\\vert^2]\\,ds=\\mathcal{O}(1)\\,.\n\\end{equation}\nThus, we achieve Claim \\ref{clm:Asian delta 2.1}.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 2.2}\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\n    =\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 2.2}]\nFrom the Jensen inequality and the H\\"older inequality,\n\\begin{align}\n    &\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]\\right\\vert \\\\\n    &\\le\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^2\\vert TD_s^{*}\\hat{F}\\vert^2]\\,ds\\right)^{\\frac{1}{2}}\\left(\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2-1\\right)^2\\right]\\right)^{\\frac{1}{2}}\\,.\n\\end{align}\nFrom the following two observations, we can complete the proof.\n\\begin{equation}\n    \\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{u}_s\\vert^2\\vert TD_s^{*}\\hat{F}\\vert^2]\\,ds=\\mathcal{O}(1)\\,, \\quad \\mathbb{E}^\\mathbb{Q}\\left[\\left(\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2-1\\right)^2\\right]=\\mathcal{O}(T)\\,.\n\\end{equation}\nThe first equality has already been proved in Eq.\\eqref{proof:asian delta 2} whereas the second equality comes easily from direct computation.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 2.3}\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right] \\\\\n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 2.3}]\nFrom the Jensen inequality and the H\\"older inequality,\n\\begin{align}\n    &\\left\\vert\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T \\hat{u}_{s}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]\n    -\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]\\right\\vert \\\\\n    &\\le \\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[g_s^2(TD_s^{*}\\hat{F})^2]\\,ds\\right)^{\\frac{1}{2}}\\left(\\frac{1}{T}\\int_0^T\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_t^4]\\,dt\\right)^{\\frac{1}{2}}\\,,\n\\end{align}\nwhere $g_s:=\\hat{u}_s-\\frac{2}{\\sigma(s,S_0)S_0}\\,.$ Note from Lemma \\ref{lem:Ds bound} and the H\\"older inequality that for any $0\\le s\\le T\\le 1\\,,$\n\\begin{equation}\n    \\mathbb{E}^\\mathbb{Q}[g_s^2(TD_s^{*}\\hat{F})^2]\n    \\le \\left(\\mathbb{E}^\\mathbb{Q}[g_s^4]\\right)^\\frac{1}{2}(E_4)^{\\frac{1}{2}}\\,.\n\\end{equation}\nObserve from the inequalities Eqs.\\eqref{proof:hat u close 2}, \\eqref{proof:hat u close 3}, \\eqref{proof:hat u close 4}, and \\eqref{proof:hat u close 5} that there exists some constant $G>0$ such that $\\mathbb{E}^\\mathbb{Q}[g_s^4]\\le G s^2\\,,$ for $0\\le s\\le 1\\,.$ Furthermore, by direct computation, we can check that $\\mathbb{E}^\\mathbb{Q}[\\hat{Y}_t^4]$ is bounded by some constant in small $T>0\\,.$ This proves the claim.\n\\end{proof}\n\\begin{claim}\\label{clm:Asian delta 2.4}\n\\begin{align}\n    \\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right]\n    =-\\frac{2}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds+\\mathcal{O}(\\sqrt{T})\\,.\n\\end{align}\n\\end{claim}\n\\begin{proof}[Proof of Claim \\ref{clm:Asian delta 2.4}]\nFrom the definition of $D_s^{*}\\hat{F}$ and the computation of $D_s\\hat{Y}_t$ in Eq.\\eqref{proof:DtildeY, DhatY}, we get\n\\begin{equation}\n    D_s^{*}\\hat{F}=-\\frac{\\int_0^TD_s\\hat{Y}_t\\,dt}{\\left(\\int_0^T\\hat{Y}_t\\,dt\\right)^2}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\n    =-\\frac{1}{T^2}\\frac{\\nu(s,S_0)(T-s)}{\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2}\\mathbbm{1}_{\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge\\frac{1}{2}\\}}\\,.\n\\end{equation}\nUsing this identity,\n\\begin{align}\n    &\\mathbb{E}^\\mathbb{Q}\\left[\\int_0^T\\frac{2}{\\sigma(s,S_0)S_0}(D_{s}^{*}\\hat{F})\\,ds\\left(\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\right)^2\\right] \\\\\n    &=-\\frac{2}{S_0}\\frac{1}{T^2}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}(T-s)\\,ds\\,\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt\\ge \\frac{1}{2}\\right\\}\\,.\n\\end{align}\nFrom Eq.\\eqref{eq:hatY vanish}, $\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_0^T\\hat{Y}_t\\,dt<\\frac{1}{2}\\right\\}=o(T^q)$ for any $q>0\\,$ as $T\\rightarrow{0}\\,.$ Hence, we prove the claim.\n\\end{proof}\n\\noindent Combining Claims \\ref{clm:Asian delta 2.1}--\\ref{clm:Asian delta 2.4}, we finally get Eq.\\eqref{eq:Asian delta app 3.2}. Here, we complete the proof of Proposition \\ref{prop:Asian delta app 3}.\n\\end{proof}\n\\subsection{Proof for Proposition \\ref{prop:european delta app 2}}\\label{proof:european delta app 2}\n\\begin{proof}\nHere, we only sketch the proof. A rigorous proof can be presented by routine use of well-known inequalities such as the Jensen inequality and the H\\"older inequality, which have already been used throughout Appendix \\ref{proof:Asian delta app 3}. Hence, the details will be omitted. \\\\\n\\indent First, we will study Eq.\\eqref{eq:european delta app 2.1}. Observe that\n\\begin{align}\n    &\\frac{1}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\delta(\\hat{h})\\hat{G}\\right]\n    -\\frac{1}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(S_0)\\delta(\\hat{h})\\hat{G}\\right] \\\\\n    &=\\frac{1}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\Phi(\\hat{X}_T)-\\Phi(S_0)\\right)\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)\\hat{G}\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 1} \\\\ \n    &=\\frac{1}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\left(\\Phi(\\hat{X}_T)-\\Phi(S_0)\\right)\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)S_0\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 2}\\\\ \n    &=\\mathbb{E}^\\mathbb{Q}\\left[\\frac{\\Phi(S_0+S_0\\sigma_E(T)\\sqrt{T}Z)}{S_0\\sigma_E(T)\\sqrt{T}}Z\\right]+\\mathcal{O}(\\sqrt{T})\\,. \\label{proof:european delta 3}\n\\end{align}\nThe first equality Eq.\\eqref{proof:european delta 1} comes from $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_T-S_0\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})\\,,$ Lemma \\ref{lem:hat h app}, and $\\mathbb{E}^\\mathbb{Q}[\\hat{G}^p]=\\mathcal{O}(1)\\,$ for any $p>0\\,.$\nThe second inequality Eq.\\eqref{proof:european delta 2} holds because $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{G}-S_0\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})$ and $\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\delta\\left(\\frac{2}{\\sigma(\\cdot,S_0)S_0}\\right)\\right\\vert^p\\right]=\\mathcal{O}(T^{\\frac{p}{2}})$ for any $p>0\\,.$ We obtain the last equality Eq.\\eqref{proof:european delta 3} from direct manipulation with regard to a multivariate normal variable. The same type of manipulation technique has already been used to prove Claim \\ref{clm:Asian delta 1.0} in Appendix \\ref{proof:Asian delta app 3}. Similarly, we get \n\\begin{align}\n    \\frac{\\Phi(S_0)}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{h})\\hat{G}\\right]\n    &=\\frac{\\Phi(S_0)}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\delta(\\hat{h})(\\hat{G}-S_0)\\right] \\label{proof:european delta 4}\\\\\n    &=\\frac{\\Phi(S_0)}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)(\\hat{G}-S_0)\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 5}\\\\\n    &=\\frac{\\Phi(S_0)}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)(\\hat{G}-S_0)\\hat{Y}_T\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 6}\\\\\n    &=\\frac{\\Phi(S_0)}{S_0T}\\mathbb{E}^\\mathbb{Q}\\left[\\delta\\left(\\frac{1}{\\sigma(\\cdot,S_0)S_0}\\right)(\\hat{X}_T-S_0\\hat{Y}_T)\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 7}\\\\\n    &=\\frac{\\Phi(S_0)}{S_0T}\\int_0^T\\frac{\\sigma(s,S_0)-\\nu(s,S_0)}{\\sigma(s,S_0)}\\,ds+\\mathcal{O}(\\sqrt{T})\\,. \\label{proof:european delta 8}\n\\end{align}\nFor the first equality Eq.\\eqref{proof:european delta 4}, we use $\\mathbb{E}^\\mathbb{Q}[\\delta(\\hat{h})]=0\\,.$ Lemma \\ref{lem:hat h app} and $\\mathbbm{E}^\\mathbb{Q}[\\vert \\hat{G}-S_0\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})\\,$ for any $p>0$ are used to establish the second equality Eq.\\eqref{proof:european delta 5}. Next, the equality Eq.\\eqref{proof:european delta 6} is easily obtained from $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_T-1\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})\\,.$ The fourth equality Eq.\\eqref{proof:european delta 7} follows from the observation $\\mathbb{Q}\\{\\hat{Y}_T<\\frac{1}{2}\\}=o(T^q)\\,$ for any $q>0\\,.$ This is similar to Eqs.\\eqref{eq:hatx vanish} and \\eqref{eq:hatY vanish}, which are used to analyze the Asian option. The last equality Eq.\\eqref{proof:european delta 8} comes from the Ito isometry. From these arguments, we can prove Eq.\\eqref{eq:european delta app 2.1}. \\\\\n\\indent Now, we analyze Eq.\\eqref{eq:european delta app 2.2} from a series of asymptotic relations below.\n\\begin{align}\n    \\frac{1}{T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\hat{h}_s \\hat{H}_s\\,ds\\right]\n    &=\\frac{1}{T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\frac{1}{\\sigma(s,S_0)S_0} \\hat{H}_s\\,ds\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 9}\\\\\n    &=\\frac{1}{T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\frac{1}{\\sigma(s,S_0)S_0} \\hat{H}_s\\,ds\\,\\hat{Y}_T^2\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 10}\\\\\n    &=\\frac{1}{T}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\Phi(\\hat{X}_T)\\int_0^T \\frac{\\nu(s,S_0)}{\\sigma(s,S_0)S_0}\\,ds\\,\\hat{X}_T\\right]+\\mathcal{O}(\\sqrt{T}) \\label{proof:european delta 11} \\\\\n    &=\\frac{\\Phi(S_0)}{T}\\int_0^T\\frac{\\nu(s,S_0)}{\\sigma(s,S_0)}\\,ds+\\mathcal{O}(\\sqrt{T})\\label{proof:european delta 12}\\,.\n\\end{align}\nNote that the first equality Eq.\\eqref{proof:european delta 9} is obtained by the asymptotic relations $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_T\\vert^p]=\\mathcal{O}(1)\\,,$ $\\underset{s\\ge0}{\\sup}\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{H}_s\\vert^p]=\\mathcal{O}(1)$ and $\\mathbb{E}^\\mathbb{Q}\\left[\\left\\vert\\hat{h}_s-\\frac{1}{\\sigma(s,S_0)S_0}\\right\\vert^p\\right]=\\mathcal{O}(s^{\\frac{p}{2}})\\,,$ as $T\\rightarrow{0}\\,.$ Similarly, we get the second equality Eq.\\eqref{proof:european delta 10} using $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{Y}_T^2-1\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})\\,.$ The third equality Eq.\\eqref{proof:european delta 11} comes directly from the definition of $\\hat{H}_s$ with the observation that $\\mathbb{Q}\\{\\hat{Y}_T<\\frac{1}{2}\\}=o(T^q)\\,,$ for any $q>0\\,.$ The last equality Eq.\\eqref{proof:european delta 12} follows from $\\mathbb{E}^\\mathbb{Q}[\\vert\\hat{X}_T-S_0\\vert^p]=\\mathcal{O}(T^{\\frac{p}{2}})\\,,$ $p>0\\,.$ The proof for Eq.\\eqref{eq:european delta app 2.2} is thus complete.\n\\end{proof}\n\n\n\n\\section{Proofs of the results in Section \\ref{sec:Special case}}\n\\subsection{Proof for Theorem \\ref{thm:otm delta}}\\label{proof:otm delta}\n\\begin{proof}\nFirst, we will show Eq.\\eqref{eq:otm cdelta}. Using Lemma \\ref{lem:another repre delta}, the H\\"older inequality, and the Jensen inequality, for $1/p+1/p^{\'}=1\\,,$ $1<p\\,, p^{\'}<\\infty\\,,$\n\\begin{align}\n    \\Delta_A^{\\textnormal{call}}(T)\n    &=\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_{0}^{T} S_t\\,dt\\mathbbm{1}_{\\{\\frac{1}{T}\\int_{0}^{T} S_t\\,dt \\ge K\\}}\\right] \\\\\n    &\\le\\frac{e^{-rT}}{S_0}\\left(\\frac{1}{T}\\int_{0}^{T}\\mathbb{E}^\\mathbb{Q}[S_t^p]\\,dt\\right)^{\\frac{1}{p}}\\left(\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_{0}^{T}S_t\\,dt\\ge K\\right\\}\\right)^{\\frac{1}{p^{\'}}} \\\\\n    &\\le\\frac{e^{-rT}}{S_0}\\left(\\frac{1}{T}\\int_{0}^{T}S_0^pe^{p(r-q)t}e^{K(p)\\overline{\\sigma}^2t}\\,dt\\right)^{\\frac{1}{p}}\\left(\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_{0}^{T}S_t\\,dt \\ge K\\right\\}\\right)^{\\frac{1}{p^{\'}}}\\,.\n\\end{align}\nFor the last inequality, we use Eq.\\eqref{eq:power of mart} with $K(p):=\\frac{p(p-1)}{2}\\vee 0\\,.$ By taking $T\\rightarrow{0}\\,,$ we get the following inequality from Remark \\ref{rmk:ldp},\n\\begin{equation}\n    \\limsup_{T\\rightarrow0}T\\log\\Delta_A^{\\textnormal{call}}(T)\\le\\frac{-\\mathcal{I}(K,S_0)}{p^{\'}}\\,.\n\\end{equation}\nTake $p^{\'}\\rightarrow{1}$ to get an upper bound.\nNext, a lower bound for Eq.\\eqref{eq:otm cdelta} is obtained from the following inequality and Remark \\ref{rmk:ldp},\n\\begin{align}\n    \\Delta_A^{\\textnormal{call}}(T)=\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_{0}^{T} S_t\\, dt \\,\\mathbbm{1}_{\\{\\frac{1}{T}\\int_{0}^{T} S_t\\, dt \\ge K\\}}\\right]\\, \\ge \\frac{e^{-rT}K}{S_0}\\mathbb{Q}\\left\\{\\frac{1}{T}\\int_{0}^{T}S_t\\,dt \\ge K\\right\\}\\,.\n\\end{align}\nProving Eq.\\eqref{eq:otm pdelta} is simpler. From Lemma \\ref{lem:another repre delta}, an upper bound for Eq.\\eqref{eq:otm pdelta} is easily obtained from\n\\begin{align}\n-\\Delta_A^{\\textnormal{put}}(T)=\\frac{e^{-rT}}{S_0}\\,\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_{0}^{T} S_t\\, dt\\,\\mathbbm{1}_{\\{K \\ge \\frac{1}{T}\\int_{0}^{T} S_t\\, dt\\}}\\right]\n\\le \\frac{e^{-rT}K}{S_0}\\mathbb{Q}\\left\\{K\\ge \\frac{1}{T}\\int_{0}^{T}S_t\\,dt\\right\\}\\,.\n\\end{align}\nFor a lower bound, observe that for any $0<L<K\\,,$\n\\begin{align}\n    -\\Delta_A^{\\textnormal{put}}(T)\n     &=\\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_{0}^{T} S_t\\, dt\\,\\mathbbm{1}_{\\{K \\ge \\frac{1}{T}\\int_{0}^{T} S_t\\, dt\\}}\\right] \\\\\n     &\\ge \\frac{e^{-rT}}{S_0}\\mathbb{E}^\\mathbb{Q}\\left[\\frac{1}{T}\\int_{0}^{T} S_t\\, dt\\,\\mathbbm{1}_{\\{K \\ge \\frac{1}{T}\\int_{0}^{T} S_t\\, dt>L\\}}\\right]\n     \\ge \\frac{e^{-rT}\\epsilon}{S_0}\\mathbb{Q}\\left\\{K \\ge \\frac{1}{T}\\int_{0}^{T} S_t\\, dt>L\\right\\}\\,.\n\\end{align}\nTherefore, from Remark \\ref{rmk:ldp},\n\\begin{align}\n    \\liminf_{T\\rightarrow0}T\\log\\left(-\\Delta_A^{\\textnormal{put}}(T)\\right)\n    \\ge \\liminf_{T\\rightarrow0}T\\log\\left(\\mathbb{Q}\\left\\{K \\ge \\frac{1}{T}\\int_{0}^{T} S_t\\, dt>L\\right\\}\\right)\n    =-\\inf_{K\\ge x>L}\\mathcal{I}(x,S_0)\\,.\n\\end{align}\nThe last term is equal to $-\\mathcal{I}(K,S_0)\\,$ because of Proposition \\ref{prop:rate function}.\n\\end{proof}\n\n\\newpage\n\n\n\n\n\n\n\n\n\\begin{small}\n\\begin{spacing}{0.5}\n\\bibliographystyle{plainnat}\n', 'meta': {'timestamp': datetime.datetime(2019, 12, 2, 2, 19, 48), 'yymm': '1911', 'arxiv_id': '1911.12944', 'language': 'en', 'url': 'https://arxiv.org/abs/1911.12944'}} or rewrite tokenize_example in data.py 
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:00:10,623] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I1231 16:00:10.624498 88148 tcp_utils.cc:107] Retry to connect to 10.3.242.26:58758 while the server is not yet listening.
I1231 16:00:13.624799 88148 tcp_utils.cc:130] Successfully connected to 10.3.242.26:58758
I1231 16:00:13.643435 88148 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:13.643445 88148 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:13,643] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:00:13.646412 88148 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:00:13.648288 88148 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-31 16:00:15,641] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:00:15,641] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:00:15.641837 88148 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.641861 88148 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,641] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:00:15.641939 88148 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.641944 88148 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,642] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:00:15,642] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:00:15,642] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:00:15,642] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,642] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,642] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,642] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:00:15,643] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,644] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:00:15,645] [   DEBUG][0m - [0m
[32m[2024-12-31 16:00:15,647] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[33m[2024-12-31 16:00:15,647] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:00:15,647] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:00:15,649] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:00:15,649] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:00:15,650] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:01:34,152] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:02:37,128] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:02:37,129] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:02:37,131] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:02:38,708] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:02:38,769] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:02:38,769] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:02:46,212] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-12-31 16:02:46,322] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:02:46,425] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:02:46,425] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:02:46,426] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:02:46,427] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - dataset_rank                  : 2[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:02:46,428] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:02:46,429] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:02:46,430] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-00-10_ubuntu[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:02:46,431] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - optimizer_name_suffix         : shard02[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:02:46,432] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:02:46,433] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:02:46,434] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - sharding_parallel_rank        : 2[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:02:46,435] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - [0m
[32m[2024-12-31 16:02:46,439] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:02:46,477] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:02:47,275] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:02:47,275] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:02:47,277] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:02:47,866] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:02:47) [0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:02:47,871] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 85, in tokenize_unsupervised_example
    max_length=data_args.scaled_max_length,
AttributeError: 'DataConfig' object has no attribute 'scaled_max_length'. Did you mean: 'pad_to_max_length'?
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:04:53,571] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I1231 16:04:53.572849 96376 tcp_utils.cc:107] Retry to connect to 10.3.242.26:54723 while the server is not yet listening.
I1231 16:04:56.573160 96376 tcp_utils.cc:130] Successfully connected to 10.3.242.26:54723
I1231 16:04:56.591431 96376 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:56.591455 96376 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:56,591] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:04:56.593436 96376 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:04:56.595503 96376 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-12-31 16:04:58,905] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:04:58,905] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:04:58.905645 96376 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:58.905671 96376 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:58,905] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:04:58.905764 96376 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:58.905769 96376 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:58,905] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:04:58,905] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:04:58,906] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:04:58,907] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:04:58,908] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:04:58,909] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:04:58,910] [   DEBUG][0m - [0m
[32m[2024-12-31 16:04:58,912] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[33m[2024-12-31 16:04:58,912] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:04:58,913] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:04:58,915] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:04:58,915] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:04:58,916] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:06:08,694] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:07:21,242] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:07:21,243] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:07:21,247] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:07:23,169] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:07:23,236] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:07:23,237] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:07:26,416] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-12-31 16:07:26,563] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_rank                  : 2[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-04-53_ubuntu[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - optimizer_name_suffix         : shard02[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_rank        : 2[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - [0m
[32m[2024-12-31 16:07:26,665] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:07:26,668] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:07:27,474] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:07:27,475] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:07:27,479] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:07:28,065] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:07:28) [0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:07:28,070] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2360, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2300, in compute_loss
    outputs = model(**inputs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 2101, in forward
    outputs = self.llama(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1741, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1583, in _prepare_decoder_attention_mask
    expanded_attn_mask = expanded_attn_mask & combined_attention_mask
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/tensor/logic.py", line 1216, in bitwise_and
    return _C_ops.bitwise_and(x, y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_bitwise_and(_object*, _object*, _object*)
1   bitwise_and_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   bitwise_and_ad_func(paddle::Tensor const&, paddle::Tensor const&)
3   paddle::experimental::bitwise_and(paddle::Tensor const&, paddle::Tensor const&)
4   void phi::BitwiseAndKernel<bool, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
5   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::Allocator::Allocate(unsigned long)
14  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
15  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
16  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 21.846952GB memory on GPU 2, 61.020020GB memory has been allocated and available memory is only 18.130981GB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:14:26,279] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:14:26,279] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:14:26,280] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:14:26,280] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:14:26,280] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 16:14:26.281881 112007 tcp_utils.cc:130] Successfully connected to 10.3.242.26:55726
I1231 16:14:26.367348 112007 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.367377 112007 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:26.367751 112007 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.367762 112007 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:26,367] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:14:26.382365 112007 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:14:26.383831 112007 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
[2024-12-31 16:14:29,189] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:14:29,190] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:14:29,191] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:14:29.191490 112007 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.191556 112007 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:29.191726 112007 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.191747 112007 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:29,192] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [0, 1, 2, 3], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:14:29,192] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:14:29,195] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:14:29,203] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:14:29,204] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:14:29,205] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:14:29,206] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:14:29,206] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:14:29,206] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:14:29,207] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:14:29,207] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:14:29,207] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:14:29,207] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:14:29,208] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:14:29,208] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:14:29,208] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:14:29,208] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:14:29,209] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:14:29,209] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,209] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,210] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,210] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,210] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,211] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:14:29,211] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:14:29,211] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:14:29,211] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:14:29,212] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:14:29,212] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:14:29,212] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:14:29,212] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:14:29,213] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:14:29,213] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:14:29,213] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:14:29,214] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:14:29,214] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:14:29,214] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:14:29,214] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,215] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,215] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:14:29,215] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,215] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,216] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:14:29,216] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:14:29,216] [   DEBUG][0m - [0m
[32m[2024-12-31 16:14:29,222] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[33m[2024-12-31 16:14:29,222] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:14:29,225] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:14:29,230] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:14:29,231] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:14:29,232] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:15:49,073] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:631 - stage=0, global_rank=2 ,layer_number=9
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,080] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:631 - stage=1, global_rank=2 ,layer_number=8
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:631 - stage=2, global_rank=2 ,layer_number=8
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:631 - stage=3, global_rank=2 ,layer_number=10
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:15:49,081] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:15:49,251] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:16:06,044] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:16:06,044] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:16:07,432] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:16:07,438] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:16:07,439] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:16:14,089] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[32m[2024-12-31 16:16:14,225] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:16:14,248] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:16:14,249] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:16:14,250] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:16:14,251] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-14-26_ubuntu[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:16:14,252] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - optimizer_name_suffix         : pp02[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:16:14,253] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - pipeline_parallel_rank        : 2[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,254] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,255] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:16:14,256] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - weight_name_suffix            : pp02[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:16:14,257] [   DEBUG][0m - [0m
[32m[2024-12-31 16:16:14,257] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:16:14,259] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:16:14,259] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 2
[2024-12-31 16:16:14,259] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:16:14,259] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:16:14) [0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:16:14,260] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:16:14,261] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:28:46,434] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:28:46,434] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:28:46,434] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I1231 16:28:46.435882 129650 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40539
I1231 16:28:46.436687 129650 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.436722 129650 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:46.437143 129650 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.437156 129650 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:46,437] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:28:46.439450 129650 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:28:46.441550 129650 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
[2024-12-31 16:28:49,488] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:28:49,488] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:28:49,489] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:28:49.489493 129650 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.489524 129650 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:49.489630 129650 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.489645 129650 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:49,489] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [0, 1, 2, 3], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:28:49,490] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:28:49,491] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:28:49,494] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:28:49,495] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:28:49,496] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:28:49,497] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:28:49,498] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:28:49,499] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:28:49,500] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,501] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:28:49,502] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:28:49,503] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,504] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,505] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:28:49,505] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:28:49,505] [   DEBUG][0m - [0m
[32m[2024-12-31 16:28:49,509] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[33m[2024-12-31 16:28:49,510] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:28:49,512] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:28:49,517] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:28:49,519] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:28:49,519] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:30:22,253] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:631 - stage=0, global_rank=2 ,layer_number=9
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:631 - stage=1, global_rank=2 ,layer_number=8
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:631 - stage=2, global_rank=2 ,layer_number=8
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,258] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:631 - stage=3, global_rank=2 ,layer_number=10
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:30:22,259] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:30:22,335] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:30:41,347] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:30:41,348] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:30:42,644] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:30:42,651] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:30:42,651] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:30:45,120] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[32m[2024-12-31 16:30:45,259] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:30:45,282] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:30:45,283] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:30:45,284] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:30:45,285] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-28-46_ubuntu[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:30:45,286] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - optimizer_name_suffix         : pp02[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:30:45,287] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - pipeline_parallel_rank        : 2[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:30:45,288] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:30:45,289] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:30:45,290] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - weight_name_suffix            : pp02[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:30:45,291] [   DEBUG][0m - [0m
[32m[2024-12-31 16:30:45,291] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:30:45,298] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:30:45,298] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 2
[2024-12-31 16:30:45,298] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:30:45,298] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:30:45) [0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:30:45,299] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:30:45,299] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:30:45,299] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:30:45,300] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:30:45,300] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Exception in thread Thread-2 (_thread_loop):
ValueError: 8192 is not a valid PaddingStrategy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 81, in tokenize_unsupervised_example
    tokenized_source = tokenizer(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2440, in __call__
    return self.encode(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2510, in encode
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2106, in _get_padding_truncation_strategies
    padding_strategy = PaddingStrategy(padding)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 384, in __call__
    return cls.__new__(cls, value)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 708, in __new__
    raise exc
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 692, in __new__
    result = cls._missing_(value)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 120, in _missing_
    raise ValueError(
ValueError: 8192 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:31:44,200] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:31:44,200] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:31:44,200] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I1231 16:31:44.202006 133206 tcp_utils.cc:107] Retry to connect to 10.3.242.26:51282 while the server is not yet listening.
I1231 16:31:47.202316 133206 tcp_utils.cc:130] Successfully connected to 10.3.242.26:51282
I1231 16:31:47.227449 133206 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.227464 133206 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:47.227898 133206 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.227910 133206 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:47,227] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:31:47.229487 133206 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:31:47.230468 133206 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
[2024-12-31 16:31:50,165] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:31:50,166] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:31:50,166] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:31:50.167109 133206 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.167183 133206 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:50.167371 133206 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.167397 133206 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:50,167] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [0, 1, 2, 3], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:31:50,168] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:31:50,171] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:31:50,184] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,184] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,184] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:31:50,184] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:31:50,184] [   DEBUG][0m - [0m
[32m[2024-12-31 16:31:50,190] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[33m[2024-12-31 16:31:50,190] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:31:50,193] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:31:50,199] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:31:50,200] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:31:50,201] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:32:54,061] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:32:54,066] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:32:54,066] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:32:54,066] [    INFO] pp_layers.py:631 - stage=0, global_rank=2 ,layer_number=9
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:631 - stage=1, global_rank=2 ,layer_number=8
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:631 - stage=2, global_rank=2 ,layer_number=8
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:631 - stage=3, global_rank=2 ,layer_number=10
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:32:54,067] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:32:54,149] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:33:12,867] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:33:12,868] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:33:14,186] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:33:14,191] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:33:14,191] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:33:20,764] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[32m[2024-12-31 16:33:20,910] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-31-44_ubuntu[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:33:20,940] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - optimizer_name_suffix         : pp02[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - pipeline_parallel_rank        : 2[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:33:20,941] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:33:20,942] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:33:20,943] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - weight_name_suffix            : pp02[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:33:20,944] [   DEBUG][0m - [0m
[32m[2024-12-31 16:33:20,945] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:33:20,948] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:33:20,948] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 2
[2024-12-31 16:33:20,948] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:33:20,948] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:33:20) [0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:33:20,949] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:33:20,949] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:33:20,949] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:33:20,950] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:33:20,950] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:37:07,381] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:37:07,382] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:37:07,382] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:37:07,382] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:37:07,382] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I1231 16:37:07.383432 138018 tcp_utils.cc:107] Retry to connect to 10.3.242.26:40133 while the server is not yet listening.
I1231 16:37:10.383731 138018 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40133
I1231 16:37:10.411406 138018 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.411419 138018 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:10.411790 138018 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.411800 138018 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:10,411] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:37:10.415941 138018 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:37:10.416728 138018 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
[2024-12-31 16:37:12,986] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:37:12,986] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:37:12,986] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:37:12.986886 138018 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.986909 138018 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:12.986950 138018 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.986955 138018 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:12,987] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [0, 1, 2, 3], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:37:12,987] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:37:12,988] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:37:12,993] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:37:12,994] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:37:12,995] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:37:12,996] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:37:12,997] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,998] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:37:12,999] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:37:13,000] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:37:13,001] [   DEBUG][0m - [0m
[32m[2024-12-31 16:37:13,006] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[33m[2024-12-31 16:37:13,006] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:37:13,009] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:37:13,014] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:37:13,015] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:37:13,015] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:38:18,425] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:631 - stage=0, global_rank=2 ,layer_number=9
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,430] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:631 - stage=1, global_rank=2 ,layer_number=8
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:631 - stage=2, global_rank=2 ,layer_number=8
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:631 - stage=3, global_rank=2 ,layer_number=10
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:38:18,431] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:38:18,505] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:38:37,638] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:38:37,640] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:38:39,036] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:38:39,042] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:38:39,042] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:38:41,289] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[32m[2024-12-31 16:38:41,383] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:38:41,405] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:38:41,406] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:38:41,407] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-37-07_ubuntu[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:38:41,408] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - optimizer_name_suffix         : pp02[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:38:41,409] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - pipeline_parallel_rank        : 2[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:38:41,410] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:38:41,411] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - weight_name_suffix            : pp02[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:38:41,412] [   DEBUG][0m - [0m
[32m[2024-12-31 16:38:41,413] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 2
[2024-12-31 16:38:41,474] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:38:41,474] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:38:41) [0m
[32m[2024-12-31 16:38:41,474] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:38:41,474] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:38:41,476] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:38:41,477] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:41:54,576] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:41:54,576] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:41:54,576] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:41:54,576] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:41:54,576] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I1231 16:41:54.578097 141492 tcp_utils.cc:130] Successfully connected to 10.3.242.26:47469
I1231 16:41:54.603355 141492 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.603375 141492 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:54.603689 141492 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.603700 141492 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:54,603] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:41:54.605268 141492 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:41:54.606264 141492 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
[2024-12-31 16:41:57,562] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:41:57,562] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:41:57,563] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:41:57.563584 141492 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.563624 141492 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:57.563727 141492 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.563741 141492 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:57,564] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [2],  sharding_group: [2], pp_group: [0, 1, 2, 3], dp_group: [2], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:41:57,564] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:41:57,566] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:41:57,568] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:41:57,569] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:41:57,570] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:41:57,571] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:41:57,572] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:41:57,573] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:41:57,574] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,575] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:41:57,576] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:41:57,577] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:41:57,578] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,579] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,580] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:41:57,580] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:41:57,580] [   DEBUG][0m - [0m
[32m[2024-12-31 16:41:57,585] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[33m[2024-12-31 16:41:57,585] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:41:57,587] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:41:57,593] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:41:57,594] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:41:57,595] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:43:06,596] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:43:06,601] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:631 - stage=0, global_rank=2 ,layer_number=9
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:631 - stage=1, global_rank=2 ,layer_number=8
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:631 - stage=2, global_rank=2 ,layer_number=8
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:631 - stage=3, global_rank=2 ,layer_number=10
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:43:06,602] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:43:06,675] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:43:26,410] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.10.input_layernorm.weight', 'llama.layers.10.mlp.down_proj.weight', 'llama.layers.10.mlp.gate_proj.weight', 'llama.layers.10.mlp.up_proj.weight', 'llama.layers.10.post_attention_layernorm.weight', 'llama.layers.10.self_attn.k_proj.weight', 'llama.layers.10.self_attn.o_proj.weight', 'llama.layers.10.self_attn.q_proj.weight', 'llama.layers.10.self_attn.v_proj.weight', 'llama.layers.11.input_layernorm.weight', 'llama.layers.11.mlp.down_proj.weight', 'llama.layers.11.mlp.gate_proj.weight', 'llama.layers.11.mlp.up_proj.weight', 'llama.layers.11.post_attention_layernorm.weight', 'llama.layers.11.self_attn.k_proj.weight', 'llama.layers.11.self_attn.o_proj.weight', 'llama.layers.11.self_attn.q_proj.weight', 'llama.layers.11.self_attn.v_proj.weight', 'llama.layers.12.input_layernorm.weight', 'llama.layers.12.mlp.down_proj.weight', 'llama.layers.12.mlp.gate_proj.weight', 'llama.layers.12.mlp.up_proj.weight', 'llama.layers.12.post_attention_layernorm.weight', 'llama.layers.12.self_attn.k_proj.weight', 'llama.layers.12.self_attn.o_proj.weight', 'llama.layers.12.self_attn.q_proj.weight', 'llama.layers.12.self_attn.v_proj.weight', 'llama.layers.13.input_layernorm.weight', 'llama.layers.13.mlp.down_proj.weight', 'llama.layers.13.mlp.gate_proj.weight', 'llama.layers.13.mlp.up_proj.weight', 'llama.layers.13.post_attention_layernorm.weight', 'llama.layers.13.self_attn.k_proj.weight', 'llama.layers.13.self_attn.o_proj.weight', 'llama.layers.13.self_attn.q_proj.weight', 'llama.layers.13.self_attn.v_proj.weight', 'llama.layers.14.input_layernorm.weight', 'llama.layers.14.mlp.down_proj.weight', 'llama.layers.14.mlp.gate_proj.weight', 'llama.layers.14.mlp.up_proj.weight', 'llama.layers.14.post_attention_layernorm.weight', 'llama.layers.14.self_attn.k_proj.weight', 'llama.layers.14.self_attn.o_proj.weight', 'llama.layers.14.self_attn.q_proj.weight', 'llama.layers.14.self_attn.v_proj.weight', 'llama.layers.15.input_layernorm.weight', 'llama.layers.15.mlp.down_proj.weight', 'llama.layers.15.mlp.gate_proj.weight', 'llama.layers.15.mlp.up_proj.weight', 'llama.layers.15.post_attention_layernorm.weight', 'llama.layers.15.self_attn.k_proj.weight', 'llama.layers.15.self_attn.o_proj.weight', 'llama.layers.15.self_attn.q_proj.weight', 'llama.layers.15.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.layers.8.input_layernorm.weight', 'llama.layers.8.mlp.down_proj.weight', 'llama.layers.8.mlp.gate_proj.weight', 'llama.layers.8.mlp.up_proj.weight', 'llama.layers.8.post_attention_layernorm.weight', 'llama.layers.8.self_attn.k_proj.weight', 'llama.layers.8.self_attn.o_proj.weight', 'llama.layers.8.self_attn.q_proj.weight', 'llama.layers.8.self_attn.v_proj.weight', 'llama.layers.9.input_layernorm.weight', 'llama.layers.9.mlp.down_proj.weight', 'llama.layers.9.mlp.gate_proj.weight', 'llama.layers.9.mlp.up_proj.weight', 'llama.layers.9.post_attention_layernorm.weight', 'llama.layers.9.self_attn.k_proj.weight', 'llama.layers.9.self_attn.o_proj.weight', 'llama.layers.9.self_attn.q_proj.weight', 'llama.layers.9.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:43:26,411] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:43:27,696] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:43:27,701] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:43:27,701] [   DEBUG][0m - 2: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:43:30,723] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 242.[0m
[32m[2024-12-31 16:43:30,818] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:43:30,844] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:43:30,845] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - current_device                : gpu:2[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:43:30,846] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:43:30,847] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,848] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - local_process_index           : 2[0m
[35m[2024-12-31 16:43:30,849] [   DEBUG][0m - local_rank                    : 2[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-41-54_ubuntu[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - logical_process_index         : 2[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:43:30,850] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - optimizer_name_suffix         : pp02[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,851] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - pipeline_parallel_rank        : 2[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - process_index                 : 2[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:43:30,852] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:43:30,853] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:43:30,854] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:43:30,855] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - weight_name_suffix            : pp02[0m
[35m[2024-12-31 16:43:30,856] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:43:30,857] [   DEBUG][0m - [0m
[32m[2024-12-31 16:43:30,857] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:43:30,899] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:43:30,900] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 2
[2024-12-31 16:43:30,900] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:43:30,900] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:43:30) [0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:43:30,900] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:43:30,901] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:43:30,902] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:43:30,902] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
W1231 16:43:34.443584 141492 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
