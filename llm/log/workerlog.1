/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:14:20,005] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1230 11:14:20.006544 4019288 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48565
I1230 11:14:20.027858 4019288 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:14:20.027918 4019288 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:14:20,028] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:14:20.034686 4019288 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:14:20.038483 4019288 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-30 11:14:23,557] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:14:23,558] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:14:23.558167 4019288 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:14:23.558194 4019288 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:14:23,558] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:14:23.558293 4019288 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:14:23.558298 4019288 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:14:23,558] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:14:23,558] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:14:23,559] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 96, in main
    gen_args, model_args, reft_args, data_args, training_args = parser.parse_json_file_and_cmd_lines()
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/argparser.py", line 299, in parse_json_file_and_cmd_lines
    return self.common_parse(args, return_remaining_strings)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/argparser.py", line 252, in common_parse
    raise ValueError(f"Some specified arguments are not used by the PdArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the PdArgumentParser: ['--rope_scaling_type', 'linear']
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:19:32,050] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I1230 11:19:32.051762 4022987 tcp_utils.cc:107] Retry to connect to 10.3.242.26:38255 while the server is not yet listening.
I1230 11:19:35.051957 4022987 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38255
I1230 11:19:35.115453 4022987 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:35.115523 4022987 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:35,116] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:19:35.127756 4022987 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:19:35.129428 4022987 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-30 11:19:37,482] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:19:37,482] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:19:37.482379 4022987 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:37.482401 4022987 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:37,482] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:19:37.482482 4022987 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:19:37.482486 4022987 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:19:37,482] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:19:37,482] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:19:37,483] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:19:37,483] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:19:37,484] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:19:37,485] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - [0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:19:37,486] [   DEBUG][0m - [0m
[32m[2024-12-30 11:19:37,487] [    INFO][0m - The global seed is set to 43, local seed is set to 51 and random seed is set to 42.[0m
[33m[2024-12-30 11:19:37,487] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
[32m[2024-12-30 11:19:37,488] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:19:37,490] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:19:37,490] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-30 11:26:13,375] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1230 11:26:13.377390 4028344 tcp_utils.cc:107] Retry to connect to 10.3.242.26:38214 while the server is not yet listening.
I1230 11:26:16.377683 4028344 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38214
I1230 11:26:16.383368 4028344 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:16.383381 4028344 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:16,383] [    INFO] topology.py:370 - Total 8 pipe comm group(s) create successfully!
W1230 11:26:16.387491 4028344 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1230 11:26:16.388321 4028344 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-30 11:26:18,610] [    INFO] topology.py:370 - Total 8 data comm group(s) create successfully!
[2024-12-30 11:26:18,610] [    INFO] topology.py:370 - Total 8 model comm group(s) create successfully!
I1230 11:26:18.610487 4028344 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:18.610514 4028344 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:18,610] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1230 11:26:18.610605 4028344 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1230 11:26:18.610610 4028344 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-30 11:26:18,610] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 8, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3, 4, 5, 6, 7], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7]
[32m[2024-12-30 11:26:18,610] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-30 11:26:18,611] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-30 11:26:18,611] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:18,611] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-30 11:26:18,611] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:18,611] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-30 11:26:18,612] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-30 11:26:18,613] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-30 11:26:18,614] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - [0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-30 11:26:18,615] [   DEBUG][0m - [0m
[32m[2024-12-30 11:26:18,617] [    INFO][0m - The global seed is set to 43, local seed is set to 51 and random seed is set to 42.[0m
[33m[2024-12-30 11:26:18,617] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 8, distributed training: True, 16-bits training: True[0m
[32m[2024-12-30 11:26:18,618] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-30 11:26:18,619] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241230",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-30 11:26:18,620] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:51:45,531] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I1231 15:51:45.533743 82302 tcp_utils.cc:130] Successfully connected to 10.3.242.26:38873
I1231 15:51:45.534179 82302 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:45.534189 82302 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:45,534] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:51:45.537035 82302 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:51:45.537751 82302 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-31 15:51:47,401] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:51:47,401] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:51:47.401710 82302 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.401726 82302 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,401] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:51:47.401798 82302 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:51:47.401803 82302 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:51:47,401] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:51:47,401] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:51:47,402] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:51:47,402] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:51:47,403] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:51:47,404] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - [0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:51:47,405] [   DEBUG][0m - [0m
[32m[2024-12-31 15:51:47,406] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[33m[2024-12-31 15:51:47,406] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:51:47,407] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:51:47,408] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:51:47,408] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:51:47,409] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:53:08,573] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 15:56:39,007] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1231 15:56:39.008468 85666 tcp_utils.cc:107] Retry to connect to 10.3.242.26:48936 while the server is not yet listening.
I1231 15:56:42.008719 85666 tcp_utils.cc:130] Successfully connected to 10.3.242.26:48936
I1231 15:56:42.027460 85666 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:42.027477 85666 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:42,027] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 15:56:42.030597 85666 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 15:56:42.031462 85666 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-31 15:56:43,090] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 15:56:43,090] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 15:56:43.090369 85666 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:43.090395 85666 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:43,090] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 15:56:43.090459 85666 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 15:56:43.090464 85666 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 15:56:43,090] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 15:56:43,090] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 15:56:43,090] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 15:56:43,091] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 15:56:43,092] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - [0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:56:43,093] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 15:56:43,094] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 15:56:43,094] [   DEBUG][0m - [0m
[32m[2024-12-31 15:56:43,095] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[33m[2024-12-31 15:56:43,095] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 15:56:43,095] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 15:56:43,096] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 15:56:43,097] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 15:56:43,097] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 15:57:43,930] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 15:58:45,644] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 15:58:45,645] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 15:58:45,647] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 15:58:47,237] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 15:58:47,298] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 15:58:47,298] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 15:58:52,436] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[32m[2024-12-31 15:58:52,573] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 15:58:52,661] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataset_rank                  : 1[0m
[35m[2024-12-31 15:58:52,662] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 15:58:52,663] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 15:58:52,664] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_15-56-39_ubuntu[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 15:58:52,665] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - optimizer_name_suffix         : shard01[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 15:58:52,666] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 15:58:52,667] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - sharding_parallel_rank        : 1[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 15:58:52,668] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 15:58:52,669] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 15:58:52,670] [   DEBUG][0m - [0m
[32m[2024-12-31 15:58:52,671] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 15:58:52,678] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 15:58:53,438] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 15:58:53,439] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 15:58:53,440] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 15:58:54,058] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 15:58:54) [0m
[32m[2024-12-31 15:58:54,058] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 15:58:54,058] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 15:58:54,059] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 15:58:54,063] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 78, in tokenize_unsupervised_example
    raise DataFormatError(
utils.data.DataFormatError: Example format is wrong, please check: {'text': '\\section{Introduction}\n\nConformal invariance was first recognised to be of physical interest when it was realized that the Maxwell equations are covariant under the $15$-dimensional conformal group \\cite{Cu,Bat}, a fact that motivated a more detailed analysis of conformal invariance in other physical contexts such as General Relativity, Quantum Mechanics or high energy physics \\cite{Ful}. These applications further suggested to study conformal invariance in connection with the physically-relevant groups, among which the Poincar\\\'e and Galilei groups were the first to be considered. In this context, conformal extensions of the Galilei group have been considered in Galilei-invariant field theories, in the study of possible dynamics of interacting particles as well as in the nonrelativistic AdS/CFT correspondence\n\\cite{Bar54,Hag,Hav,Zak,Fig}. Special cases as the (centrally extended) Schr\\"odinger algebra $\\widehat{\\mathcal{S}}(n)$ corresponding to the maximal invariance group of the \nfree Schr\\"odinger equation have been studied in detail by various authors, motivated by different applications such as the kinematical invariance of hierarchies of partial differential equations,  Appell systems, quantum groups or representation theory \\cite{Ni72,Ni73,Do97,Fra}. The class of Schr\\"odinger algebras can be generalized in natural manner to the so-called conformal Galilei algebras $\\mathfrak{g}_{\\ell}(d)$ for (half-integer) values $\\ell\\geq \\frac{1}{2}$, \nalso corresponding to semidirect products of the semisimple Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(d)$ with a Heisenberg algebra but with a higher dimensional characteristic representation.\\footnote{By characteristic representation we mean the representation of $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(d)$  that describes the action on the Heisenberg algebra.} Such algebras, that can be interpreted as a nonrelativistic analogue of the conformal algebra, have been used in a variety of contexts, ranging from classical (nonrelativistic) mechanics, electrodynamics and fluid dynamics to higher-order Lagrangian mechanics \\cite{Ai12,Tac,Du11,St13}\nThe algebraic structure of the conformal Galilei algebra $\\mathfrak{g}_{\\ell}(d)$ for values of $\\ell\\geq \\frac{3}{2}$ and its representations have been analyzed in some detail, and an algorithmic procedures to compute their Casimir operators have been proposed (see e.g. \\cite{Als17,Als19} and references therein). In the recent note \\cite{raub}, a synthetic formula for the Casimir operators of the $\\mathfrak{g}_{\\ell}(d)$ algebra has been given. Although not cited explicitly, the  \nprocedure used there corresponds to the so-called ``virtual-copy" method, a technique well-known for some years that enables to compute the Casimir operators of a Lie algebra using those of its maximal semisimple subalgebra (\\cite{Que,C23,C45,SL3} and references therein). \n\n\\medskip\n\\noindent \nIn this work, we first propose a further generalization of the conformal Galilei algebras $\\mathfrak{g}_{\\ell}(d)$, replacing the $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(d)$  subalgebra of the latter by the semisimple Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)$. As the defining representation $\\rho_d$ of $\\mathfrak{so}(p,q)$ is real for all values $p+q=d$ \\cite{Tits}, the structure of a semidirect product with a Heisenberg Lie algebra remains unaltered. The Lie algebras $\\mathfrak{Gal}_{\\ell}(p,q)$ describe a class of semidirect products of semisimple and Heisenberg Lie algebras among which $\\mathfrak{g}_{\\ell}(d)$ corresponds to the case with a largest maximal compact subalgebra. \nUsing the method developed in \\cite{C45}, we construct a virtual copy of $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)$ in the enveloping algebra of $\\mathfrak{Gal}_{\\ell}(p,q)$ for all half-integer values of $\\ell$ and any $d=p+q\\geq 3$. The Casimir operators of these Lie algebras are determined combining the analytical and the matrix trace methods, showing how to compute them explicitly in terms of the determinant of a polynomial matrix.   \n\n\n\\medskip\n\\noindent We further determine the exact number of Casimir operators for the unextended Lie algebras $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ obtained by factorizing \n$\\mathfrak{Gal}_{\\ell}(p,q)$ by its centre. Using the reformulation of the Beltrametti-Blasi formula in terms of the Maurer-Cartan equations, we show that albeit the number $\\mathcal{N}$ of invariants increases considerably for fixed $\\ell$ and varying $d$, a generic polynomial formula at most quadratic in $\\ell$ and $d$ that gives the exact value of $\\mathcal{N}$ can be established. Depending on the fact whether the relation $d\\leq 2\\ell+2$ is satisfied or not, it is shown that $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ admits a complete set of invariants formed by operators that do not depend on the generators of the Levi subalgebra. An algorithmic procedure to compute these invariants by means of a reduction to a linear system is proposed. \n   \n\n\\section{Maurer-Cartan equations of Lie algebras and Casimir operators  }\n\nGiven a Lie algebra $ \\frak{g}=\\left\\{X_{1},..,X_{n}\\; |\\;\n\\left[X_{i},X_{j}\\right]=C_{ij}^{k}X_{k}\\right\\}$ in terms of\ngenerators and commutation relations, we are principally interested\non (polynomial) operators\n$C_{p}=\\alpha^{i_{1}..i_{p}}X_{i_{1}}..X_{i_{p}}$ in the\ngenerators of $\\frak{s}$ such that the constraint $\n\\left[X_{i},C_{p}\\right]=0$,\\; ($i=1,..,n$) is satisfied. Such an\noperator can be shown to lie in the centre of the enveloping\nalgebra of $\\frak{g}$ and is called a (generalized) Casimir\noperator. For semisimple Lie algebras, the determination of\nCasimir operators can be done using structural properties\n\\cite{Ra,Gel}. However, for non-semisimple Lie algebras the relevant\ninvariant functions are often rational or even transcendental\nfunctions \\cite{Bo1,Bo2}. This suggests to develop a method in order to\ncover arbitrary Lie algebras. One convenient approach is the\nanalytical realization. The generators of the Lie algebra\n$\\frak{s}$ are realized in the space $C^{\\infty }\\left(\n\\frak{g}^{\\ast }\\right) $ by means of the differential operators:\n\\begin{equation}\n\\widehat{X}_{i}=C_{ij}^{k}x_{k}\\frac{\\partial }{\\partial x_{j}},\n\\label{Rep1}\n\\end{equation}\nwhere $\\left\\{ x_{1},..,x_{n}\\right\\}$ are the coordinates  in a dual basis of\n$\\left\\{X_{1},..,X_{n}\\right\\} $. The invariants of $\\frak{g}$ hence correspond to solutions of the following\nsystem of partial differential equations:\n\\begin{equation}\n\\widehat{X}_{i}F=0,\\quad 1\\leq i\\leq n.  \\label{sys}\n\\end{equation}\nWhenever we have a polynomial solution of (\\ref{sys}), the\nsymmetrization map defined by\n\\begin{equation}\n{\\rm Sym(}x_{i_{1}}^{a_{1}}..x_{i_{p}}^{a_{p}})=\\frac{1}{p!}\\sum_{\\sigma\\in\nS_{p}}x_{\\sigma(i_{1})}^{a_{1}}..x_{\\sigma(i_{p})}^{a_{p}}\\label{syma}\n\\end{equation}\nallows to rewrite the Casimir operators in their usual form \nas central elements in the enveloping algebra of $\\frak{g}$,\nafter replacing the variables $x_{i}$ by the corresponding\ngenerator $X_{i}$. A maximal set of functionally\nindependent invariants is usually called a fundamental basis. The\nnumber $\\mathcal{N}(\\frak{g})$ of functionally independent\nsolutions of (\\ref{sys}) is obtained from the classical criteria\nfor differential equations, and is given by the formula  \n\\begin{equation}\n\\mathcal{N}(\\frak{g}):=\\dim \\,\\frak{g}- {\\rm\nsup}_{x_{1},..,x_{n}}{\\rm rank}\\left( C_{ij}^{k}x_{k}\\right),\n\\label{BB}\n\\end{equation}\nwhere $A(\\frak{g}):=\\left(C_{ij}^{k}x_{k}\\right)$ is the matrix\nassociated to the commutator table of $\\frak{g}$ over the given\nbasis \\cite{Be}.\\newline \nThe reformulation of condition (\\ref{BB}) in terms of differential forms (see e.g. \\cite{C43})\nallows to compute  $\\mathcal{N}(\\frak{g})$ quite efficiently and even to \nobtain the Casimir\noperators under special circumstances \\cite{Peci,C72}. In terms of the\nMaurer-Cartan equations, the Lie algebra $\\frak{g}$\nis described as follows: If $\\left\\{  C_{ij}\n^{k}\\right\\}  $ denotes the structure tensor over the basis $\\left\\{  X_{1},..,X_{n}\\right\\} $,\nthe identification of the dual space $\\frak{g}^{\\ast}$ with the\nleft-invariant 1-forms on the simply connected Lie group the Lie algebra of which is isomorphic to $\\frak{g}$ allows to define an exterior\ndifferential $d$ on $\\frak{g}^{\\ast}$ by\n\\begin{equation}\nd\\omega\\left(  X_{i},X_{j}\\right)  =-C_{ij}^{k}\\omega\\left(\nX_{k}\\right) ,\\;\\omega\\in\\frak{g}^{\\ast}.\\label{MCG}\n\\end{equation}\nUsing the coboundary operator $d$, we rewrite $\\frak{g}$ as a\nclosed system of $2$-forms%\n\\begin{equation}\nd\\omega_{k}=-C_{ij}^{k}\\omega_{i}\\wedge\\omega_{j},\\;1\\leq\ni<j\\leq\\dim\\left( \\frak{g}\\right)  ,\\label{MC2}\n\\end{equation}\ncalled the Maurer-Cartan equations of $\\frak{g}$. \nIn order to reformulate equation (\\ref{BB}) in this context, we consider the linear subspace\n$\\mathcal{L}(\\frak{g})=\\mathbb{R}\\left\\{ d\\omega_{i}\\right\\}\n_{1\\leq i\\leq \\dim\\frak{g}}$ of $\\bigwedge^{2}\\frak{g}^{\\ast}$\ngenerated by the $2$-forms $d\\omega_{i}$. Now, for \na generic element  $\\omega=a^{i}d\\omega_{i}\\,\\;\\left(\na^{i}\\in\\mathbb{R}\\right)  $ of $\\mathcal{L}(\\frak{g})$ there\nexists a positive integer $j_{0}\\left( \\omega\\right)\n\\in\\mathbb{N}$ such that $\\bigwedge^{j_{0}\\left( \\omega\\right)\n}\\omega\\neq0$ and $\\bigwedge ^{j_{0}\\left( \\omega\\right)\n+1}\\omega\\equiv0$. We define the scalar $j_{0}\\left(\n\\frak{g}\\right) $ as the maximal rank of generic elements,\n\\begin{equation}\nj_{0}\\left(  \\frak{g}\\right)  =\\max\\left\\{  j_{0}\\left(\n\\omega\\right) \\;|\\;\\omega\\in\\mathcal{L}(\\frak{g})\\right\\},\n\\label{MCa1}\n\\end{equation}\nAs shown in \\cite{C43}, this is a scalar invariant of the Lie algebra $\\frak{g}$ that\nsatisfies the relation\n\\begin{equation}\n\\mathcal{N}\\left(  \\frak{g}\\right)  =\\dim\\frak{g}-2j_{0}\\left(  \\frak{g}%\n\\right). \\label{BB1}\n\\end{equation}\n\n\n\n\\medskip\n\n\\section{Virtual copies of semisimple Lie algebras}\n\n\\noindent The method of virtual copies, essentially developed in \\cite{SL3}, constitutes a natural generalization \nof a method due to Ch. Quesne (see \\cite{Que}) that combines the boson formalism and enveloping algebras of Lie algebras in \norder to compute Casimir operators  of semidirect products\n$\\frak{s}\\overrightarrow {\\frak{\\oplus}}_{R}\\frak{r}$ of simple Lie algebras $\\frak{s}$ and solvable algebras $\\mathfrak{r}$. \n\n\n\\medskip\n\\noindent We briefly recall the procedure, the details of which can be found in \\cite{SL3}: Let  $\\frak{g}$ be \na non-semisimple Lie algebra admitting the Levi decomposition\n$\\frak{g}=\\frak{s}\\overrightarrow{\\frak{\\oplus}}_{\\Gamma}\\frak{r}$,\nwhere $\\frak{s}$ denotes the Levi subalgebra, $\\Gamma$ the characteristic representation and $\\frak{r}$ the\nradical, i.e., the maximal solvable\nideal of $\\frak{g}$. Let $\\left\\{\nX_{1},..,X_{n},Y_{1},..,Y_{m}\\right\\} $ be a basis such that\n$\\left\\{  X_{1},..,X_{n}\\right\\}  $ spans $\\frak{s}$ and $\\left\\{\nY_{1},..,Y_{m}\\right\\}  $ spans $\\frak{r}$. We further suppose\nthat the structure tensor in $\\frak{s}$ is given by\n\\begin{equation}\n\\left[  X_{i},X_{j}\\right]  =C_{ij}^{k}X_{k}.\\label{ST}\n\\end{equation}\nWe now define operators $X_{i}^{\\prime}$ in the enveloping algebra\nof $\\frak{g}$ by means of\n\\begin{equation}\nX_{i}^{\\prime}=X_{i}\\,f\\left(  Y_{1},..,Y_{m}\\right) +P_{i}\\left(\nY_{1},..,Y_{m}\\right)  ,\\label{OP1}%\n\\end{equation}\nwhere $P_{i}$ is a homogeneous polynomial of degree $k$ and $f$ is\nhomogeneous of degree $k-1$. We require\nthe constraints\n\\begin{eqnarray}\n\\left[  X_{i}^{\\prime},Y_{k}\\right]    & =0,\\label{Bed1}\\\\\n\\left[  X_{i}^{\\prime},X_{j}\\right]    & =\\left[\nX_{i},X_{j}\\right] ^{\\prime}:=C_{ij}^{k}\\left(\nX_{k}f+P_{k}\\right).\\label{Bed2}\n\\end{eqnarray}\nto be satisfied for all generators. This leads to \nconditions on $f$ and $P_{i}$.   It can be shown that condition (\\ref{Bed1}) leads to\n\\begin{equation}\n\\left[  X_{i}^{\\prime},Y_{j}\\right]  =\\left[  X_{i}f,Y_{j}\\right]  +\\left[  P_{i}%\n,Y_{j}\\right]  =X_{i}\\left[  f,Y_{j}\\right]  +\\left[\nX_{i},Y_{j}\\right]\n\\,f+\\left[  P_{i},Y_{j}\\right]  .\\label{Eq1}%\n\\end{equation}\nBy homogeneity, we can reorder the terms according to\ntheir degree, so that $X_{i}\\left[  f,Y_{j}\\right]  $ is \nhomogeneous of degree $k-1$ in the variables $\\left\\{\nY_{1},..,Y_{m}\\right\\}  $ and  $\\left[ X_{i},Y_{j}\\right]\n\\,f+\\left[  P_{i},Y_{j}\\right]  $ of degree $k$. Hence the conditions \n\\begin{eqnarray}\n\\left[  f,Y_{j}\\right] =0,\\;\n\\left[  X_{i},Y_{j}\\right]  \\,f+\\left[  P_{i},Y_{j}\\right]\n=0\\label{Eq1A}\n\\end{eqnarray}\nare satisfied, showing that $f$ is a Casimir operator\nof the radical $\\frak{r}$. Expanding the condition (\\ref{Bed2}) and taking into \naccount the homogeneity degrees, after a routine computation we find that the system\n\\begin{eqnarray}\n\\left[  X_{i},X_{j}\\right]  \\,f-X_{i}\\left[  X_{j},f\\right]    =C_{ij}\n^{k}X_{k}f,\\quad \n\\left[  P_{i},X_{j}\\right]    =C_{ij}^{k}P_{k}\\label{Eq3}\n\\end{eqnarray}\nis satisfied for any indices $i,j$. Using now \n(\\ref{ST}), the first identity reduces to\n\\begin{equation}\nX_{i}\\left[  X_{j},f\\right]  =0.\n\\end{equation}\nFrom this we conclude that the function $f$ is a Casimir operator of $\\frak{g}$ that depends\nonly on the variables of the radical $\\frak{r}$. The second\nidentity in (\\ref{Eq3}) implies that $P_{i}$ transforms under the\n$X_{j}^{\\prime}s$ like a generator of the semisimple part\n$\\frak{s}$. Taken together, it follows that the operators\n$X_{i}^{\\prime}$ fulfill the condition \n\\begin{eqnarray}\n\\left[  X_{i}^{\\prime},X_{j}^{\\prime}\\right]    & =f\\left[  X_{i},X_{j}\\right]  ^{\\prime}.\n\\end{eqnarray}\nWe shall say that the operators $X_{i}^{\\prime}$\ngenerate a virtual copy of $\\frak{s}$ in the enveloping algebra of\n$\\frak{g}$. If $f$ can be\nidentified with a central element of $\\mathfrak{g}$, as happens for a radical isomorphic to a Heisenberg\nalgebra, the virtual copy actually generates a copy in\n$\\mathcal{U}\\left(  \\frak{g}\\right) $ \\cite{Que,C23}.  The computation of the invariants of $\\mathfrak{g}$ reduces to application of the following result proved in \\cite{SL3}:\n\n\\begin{theorem}\nLet $\\frak{s}$ be the Levi subalgebra of $\\frak{g}$ and\nlet $X_{i}^{\\prime}=X_{i}\\,f\\left(  \\mathbf{Y}\\right)\n+P_{i}\\left(\\mathbf{Y}\\right)  $ be homogeneous polynomials in the\ngenerators of $\\frak{g}$ satisfying equations (\\ref{Eq1A}) and\n(\\ref{Eq3}). If $C=\\sum\\alpha ^{i_{1}..i_{p}}X_{i_{1}}..X_{i_{p}}$\nis a Casimir operator of $\\frak{s}$ having degree $p$, then\n$C^{\\prime}=\\sum\\alpha^{i_{1}..i_{p}}X_{i_{1}}^{\\prime\n}..X_{i_{p}}^{\\prime}$ is a Casimir operator of $\\frak{g}$ of\ndegree $(\\deg f+1)p$. In particular, $\\mathcal{N}\\left(  \\frak{g}\\right)\n\\geq\\mathcal{N}\\left( \\frak{s}\\right)  +1.$\n\\end{theorem}\n\n\n\\medskip\n\\noindent \nThe independence of the invariants obtained in such manner follows at once from the\nconditions (\\ref{Bed1}) and (\\ref{Bed2}). For the particular case of a radical isomorphic to a \nHeisenberg Lie algebra, it follows that the number of non-central invariants is given by the rank \nof the semisimple part, i.e., $\\mathcal{N}\\left(  \\frak{g}\\right)\n=\\mathcal{N}\\left( \\frak{s}\\right)  +1$ (see \\cite{C45} for a proof). \n\n\n\\section{The conformal generalized pseudo-Galilean Lie algebra $\\mathfrak{Gal}_{\\ell}(p,q)$ }\n\n\\noindent Structurally, the conformal Galilean algebra  $\\mathfrak{\\widehat{g}}_\\ell(d)$ is a semidirect product of the semisimple Lie algebra $\\mathfrak{s}=\\mathfrak{sl}(2,\\mathbb{R})\\oplus \\mathfrak{so}(d)$ and a Heisenberg Lie algebra of dimension $N= d(2\\ell+1)+1$. The action of $\\mathfrak{s}$ over the radical is given by the characteristic representation \n$\\widehat{\\Gamma}=\\left(D_{\\ell}\\otimes \\rho_d\\right)\\oplus \\Gamma_0$, where  $D_{\\ell}$ denotes the irreducible representation of $\\mathfrak{sl}(2,\\mathbb{R})$ with highest weight $2\\ell$ and dimension $2\\ell+1$, $rho_d$ is the defining $d$-dimensional representation of $\\mathfrak{so}(d)$ and $\\Gamma_0$ denotes the trivial representation. \n\n\\noindent \nConsidering the basis (see e.g \\cite{Als17}) given by the generators  $\\left\\{ H, D, C, E_{ij}=-E_{ji}, P_{n,i}\\right\\}$\nwith $n = 0,1, 2, \\ldots, 2\\ell; \\, i,j =1, 2, \\ldots,  d$, \nthe commutators are   \n\\begin{eqnarray}\n\\fl [D,H]= 2H,\\quad [D,C]= -2C,\\quad [C, H]=D,  \\nonumber\\\\\n\\fl[H,P_{n,i}]=-nP_ {n-1,i},\\,\\,[D,P_{n,i}]=2(\\ell - n)P_ {n,i}, \\,\\,[C,P_{n,i}]=(2\\ell - n)P_{n+1,i},\\nonumber \\\\\n\\fl[E_{ij}, P_{n,k} ]=\\delta_{ik}P_{n,j} - \\delta_{jk}P_{n,i},  \\,\\,\n[E_{ij}, E_{k\\ell}] =\\delta_{ik}E_{j\\ell} + \\delta_{j \\ell}E_{ik} - \\delta_{i \\ell}E_{jk} - \\delta_{jk}E_{i \\ell} \\nonumber\\\\\n\\fl \\left[ P_{m,i},P_{n,j}\\right]= \\delta _{ij}  \\delta _{m+n,2\\ell} I_{m}  M ,\\quad\\qquad I_{m}= (-1)^{m+\\ell+1/2}  (2\\ell -m)! \\,\\ m! \\label{CG3}\n\\end{eqnarray}\n\n\\noindent The invariants can be deduced from the Casimir operators of the semisimple subalgebra of $\\mathfrak{g}$ by replacing the generators by expressions of the type \n(\\ref{OP1}) that generate a virtual copy of $\\mathfrak{s}$. For the case of the conformal generalized Galilean algebra $\\widehat{\\mathfrak{g}}_{\\ell}(d)$, these invariants have recently been given implicitly in \\cite{raub} essentially applying this method, although the nomenclature use there is referred to as ``disentaglement" of the generators. \n\n\n\\subsection{The conformal generalized pseudo-Galilean algebra}\n\n\\noindent Introducing a non-degenerate metric tensor of signature $(p,q)$, the structure of conformal Galilean algebras can be easily extended to the pseudo-orthogonal Lie algebras $\\mathfrak{so}(p,q)$ ($p+q=d$) along the same lines. \nThe pseudo-orthogonal algebra $\\frak{so}(p,q)$ with\n$d=p+q$ is given by the  $\\frac{1}{2}d(d-1)$ operators\n$E_{\\mu\\nu}=-E_{\\nu\\mu}$   satisfying:\n\\begin{eqnarray*}\n\\left[ E_{\\mu \\nu },E_{\\lambda \\sigma }\\right]  &=&g_{\\mu \\lambda\n}E_{\\nu \\sigma }+g_{\\mu \\sigma }E_{\\lambda \\nu }-g_{\\nu \\lambda\n}E_{\\mu \\sigma\n}-g_{\\nu \\sigma }E_{\\lambda \\mu } \\\\\n\\left[ E_{\\mu \\nu },P_{\\rho }\\right]  &=&g_{\\mu \\rho }P_{\\nu\n}-g_{\\nu \\rho }P_{\\mu },\n\\end{eqnarray*}\nwhere $g={\\rm diag}\\left( 1,..,1,-1,..,-1\\right)$ is the matrix of the non-degenerate metric. Let $\\rho_1$ be the $d$-dimensional defining representation  of $\\frak{so}(p,q)$ and define the tensor product $\\Gamma=D_{\\ell}\\otimes \\rho_1$ for $\\ell\\in\\mathbb{Z}+\\frac{1}{2}$. Then $\\Gamma$ is an irreducible representation of the semisimple Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus \\mathfrak{so}(p,q)$ that satisfies the condition $\\Gamma_0\\subset \\Gamma\\wedge \\Gamma $, i.e., the wedge product of $\\Gamma$ contains a copy of the trivial representation. Following the characterization given in \\cite{C45}, this implies that the Lie algebra $\\left(\\mathfrak{sl}(2,\\mathbb{R})\\oplus \\mathfrak{so}(d)\\right)\\overrightarrow{\\oplus}_{\\Gamma\\oplus\\Gamma_0}\\mathfrak{h}_{N}$\nwith $N=d(2\\ell+1)$ is well defined. Over the basis ${ H, D, C, E_{ij}=-E_{ji}, P_{n,i},M}$ with  $0\\leq n \\leq 2\\ell$  and $1\\leq i<j\\leq p+q$, the brackets  are given by \n\\begin{eqnarray}\n[D,H]= 2H,\\quad [D,C]= -2C,\\quad [C, H]=D,  \\nonumber\\\\\n\\,\\,[H,P_{n,i}]=-nP_ {n-1,i},\\,\\,[D,P_{n,i}]=2(\\ell - n)P_ {n,i}, \\,\\,[C,P_{n,i}]=(2\\ell - n)P_{n+1,i},\\label{CG3} \\\\\n\\,\\,[E_{ij}, P_{n,k} ]=g_{ik}P_{n,j} - g_{jk}P_{n,i},  \\,\\,\n[E_{ij}, E_{k\\ell}] =g_{ik}E_{j\\ell} + g_{j \\ell}E_{ik} - g_{i \\ell}E_{jk} - g_{jk}E_{i \\ell}, \\nonumber\\\\\n\\left[ P_{n,k},P_{m,l}\\right]= g_{ij}  \\delta _{m+n,2\\ell} I_{m}  M ,\\quad\\qquad I_{m}= (-1)^{m+\\ell+1/2}  (2\\ell -m)! \\,\\ m!.\\nonumber\n\\end{eqnarray}\n\n\\noindent As commented above, the number of Casimir operators is given by $2+\\left[\\frac{d}{2}\\right]$ and can be deduced in closed form by \nmeans of the virtual copy method. \n\n\\begin{proposition}\nFor any $\\ell\\in\\mathbb{Z}+\\frac{1}{2}\\geq \\frac{1}{2}$, the operators%\n\\begin{eqnarray}\n\\fl \\widetilde{D}=D\\,M+\\sum_{i=1}^{d}\\sum_{s=0}^{q}\\left( -1\\right) ^{s+q-1}\\frac{\\mu\n^{1}\\left( s,q\\right)}{g_{ii}} P_{s,i}P_{2l-s,i},\\nonumber \\\\\n\\fl \\widetilde{H}=H\\,M+\\sum_{i=1}^{d}\\sum_{s=0}^{q-1}\\left( -1\\right) ^{s+q-1}\\frac{\\mu\n^{2}\\left( s,q\\right)}{g_{ii}} P_{s,i}P_{2l-1-s,i}-\\sum_{i=1}^{d}\\frac{1}{2\\Gamma(q+1)^2g_{ii}}   P_{q,i}^2,\\nonumber \\\\\n\\fl\\widetilde{C}=C\\,M+\\sum_{i=1}^{d}\\sum_{s=0}^{q}\\left( -1\\right) ^{s+q}\\frac{\\mu\n^{3}\\left( s,q\\right)}{g_{ii}} P_{s,i}P_{2l+1-s,i}-\\sum_{i=1}^{d}\\frac{1}{2\\Gamma(q+1)^2g_{ii}}   P_{q+1,i}^2,\\nonumber \\\\\n\\fl \\widetilde{E}_{i,j}=M E_{i,j}+ \\sum_{s=0}^{l}\\frac{(-1)^{\\frac{2l-1}{2}+s}}{s!\\; (2l-s)!}\\left(P_{s,i}P_{2l-s,j}-P_{s,j}P_{sl-s,i}\\right),\\; 1\\leq i<j\\leq d,\n\\label{NE3}\n\\end{eqnarray}\nwith coefficients defined by \n\\begin{eqnarray}\n\\fl \\mu ^{1}\\left( s,q\\right) =2^{\\frac{s-2}{2}}\\left( 1+\\sqrt{2}+\\left( -1\\right)\n^{s}\\left( \\sqrt{2}-1\\right) \\right) \\prod_{a=0}^{\\left[ \\frac{s+1}{2}\\right]\n-1}\\left( q-\\left[ \\frac{s}{2}\\right] -a\\right) \\prod_{b=s+1-\\left[ \\frac{s}{%\n2}\\right] }^{s}\\left( 2q+3-2b\\right) ,\\nonumber\\\\\n\\fl \\mu ^{2}\\left( s,q\\right) =\\frac{1}{s!\\; \\Gamma(2q+1-s)},\\quad \\mu ^{3}\\left( s,q\\right) =\\frac{1}{(s-1)!\\; \\Gamma(2q+2-s)}\n\\end{eqnarray}\ngenerate a (virtual) copy of $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\frak{so}\\left(p,q\\right)  $ in the\nenveloping algebra of $\\mathfrak{Gal}_{\\ell}(p,q)$.\n\\end{proposition}\n\n\\noindent The proof, albeit long and computationally cumbersome, is completely straightforward  and reduces to a direct verification of the\nconditions (\\ref{Bed1}) and (\\ref{Bed2}) with the choice $f=M$, taking into account the following relations between the generators and quadratic products: \n\\begin{eqnarray*}\n \\left[D,P_{n,i}P_{m,j}\\right]=2\\left(2\\ell-m-n\\right)\\;P_{n,i}P_{m,j},\\\\\n \\left[H,P_{n,i}P_{m,j}\\right]=-\\left(n P_{n-1,i}P_{m,j}+m P_{n,i}P_{m-1,j}\\right),\\\\\n \\left[D,P_{n,i}P_{m,j}\\right]=(2\\ell-m)P_{n+1,i}P_{m,j}+(2\\ell-m)M P_{n,i}P_{m+1,j},\\\\\n \\left[M E_{i,j},M E_{k,l}\\right]=M^2\\left(g_{i,k}E_{j,l}+g_{j,l}E_{i,k}-g_{i,l}E_{j,k}-g_{j,k}E_{i,l}\\right),\\\\\n \\left[E_{i,j},P_{n,k}P_{m,l}\\right]=-\\left(g_{i,k}P_{n,j}P_{m,l}-g_{j,l}P_{n,k}P_{m,i}+g_{i,l}P_{n,k}P_{m,j}-g_{j,k}P_{n,i}P_{m,l}\\right) ,\\\\\n\\left[P_{n,i}P_{m,j},P_{q,k}\\right]=-I_{q}M\\left(g_{i,k}\\delta_{n+q}^{2\\ell}P_{m,j}+g_{j,k}\\delta_{m+q}^{2\\ell}P_{n,i}\\right).\\\\\n\\end{eqnarray*}\nIn particular, for the metric tensor $g_{ii}=1$ corresponding to the compact orthogonal algebra $\\mathfrak{so}(d)$, we obtain an equivalent realization to \nthe disentaglement conditions given in \\cite{raub}. \n\n\n\\subsection{Explicit formulae for the Casimir operators of $\\mathfrak{Gal}_{\\ell}(p,q)$} \n\n\\noindent Once the (virtual) copy of the semisimple Lie algebra $\\mathfrak{Gal}_{\\ell}(p,q)$ is found, explicit expression for the Casimir operators can be immediately deduced, in its \nunsymmetrized analytic form, by means of the well known trace methods (see e.g. \\cite{Ra,Gel,Gr64,Per,Po66,Ok77,Mac}). To this extent, let $\\left\\{d,h,c,e_{i,j},p_{n,k}\\right\\}$ \nbe the coordinates in $\\mathfrak{Gal}_{\\ell}(p,q)^{\\ast}$ and let $\\left\\{\\widehat{d},\\widehat{h},\\widehat{c},\\widehat{e}_{i,j},\\widehat{p}_{n,k}\\right\\}$ denote the analytical counterpart of the operators in (\\ref{NE3}). As the simple subalgebras  $\\mathfrak{sl}(2,\\mathbb{R})$ and $\\mathfrak{so}(p,q)$ commute, it follows at once that any invariant of $\\mathfrak{Gal}_{\\ell}(p,q)$ must be also an invariant of the subalgebra $\\mathfrak{sl}(2,\\mathbb{R})\\overrightarrow{\\oplus}_{\\Gamma}\\mathfrak{h}_N$. Semidirect products of $\\mathfrak{sl}(2,\\mathbb{R})$ and a Heisenberg\nLie algebra are well-known to possess only one Casimir operators besides the central generator \\cite{C45}, the analytic expression of which is given by \n\\begin{equation}\nC^{\\prime}_{4}= \\widehat{d}^2-4\\widehat{c}\\widehat{h}\\label{ins}\n\\end{equation}\nThis invariant can also be described as a determinant as follows (see e.g. \\cite{C23}): Let $B=\\left\\{X_{2\\ell(k-1)+k+2+s}\\:\\; 1\\leq k\\leq d,\\; 1\\leq s\\leq 2\\ell+1\\right\\}$ be a basis of \n$\\left(\\mathfrak{sl}(2,\\mathbb{R})\\overrightarrow{\\oplus}_{\\Gamma}\\mathfrak{h}_N\\right)$ such that $\\left\\{D,H,C\\right\\}=\\left\\{X_1,X_2,X_3\\right\\}$ and such that the element \n$X_{2\\ell(k-1)+k+2+s}$ coresponds to the generator $P_{s,k}$ for $1\\leq k\\leq d,\\; 1\\leq s\\leq 2\\ell+1$. The commutators of $\\left(\\mathfrak{sl}(2,\\mathbb{R})\\overrightarrow{\\oplus}_{\\Gamma}\\mathfrak{h}_N\\right)$ are then described in uniform manner by \n\\begin{equation}\n\\left[X_i,X_j\\right]= C_{ij}^{k}X_{k},\\; 1\\leq i<j,k\\leq (d+1)(2\\ell+1).\n\\end{equation}\nLet $B^{\\ast}=\\left\\{x_{2\\ell(k-1)+k+2+s}\\:\\; 1\\leq k\\leq d,\\; 1\\leq s\\leq 2\\ell+1\\right\\}$ be the dual basis of $B$ and define be the polynomial matrix  $A$  of order $4 + (2 \\ell + 1) d$, the entries of which are given by \n\\begin{eqnarray}\nA_{i,j}= C_{ij}^{k} x_{k},\\quad 1\\leq i,j\\leq 3 + (2 \\ell + 1) d,\\nonumber\\\\\nA_{i,2 + (2 \\ell + 1) d}= -A_{2 + (2 \\ell + 1) d,i}x_{i},\\quad 1\\leq i\\leq 3,\\label{maxa}\\\\\nA_{j,2 + (2 \\ell + 1) d}=-A_{2 + (2 \\ell + 1) d,j}=\\frac{1}{2}x_j,\\quad j\\geq 4.\\nonumber\n\\end{eqnarray} \nIt follows from the analysis in \\cite{C23} that the determinant $\\det{A}$ provides the non-central Casimir invariant of  the Lie algebra $\\left(\\mathfrak{sl}(2,\\mathbb{R})\\overrightarrow{\\oplus}_{\\Gamma}\\mathfrak{h}_N\\right)$ . Comparing the result with that deduced from (\\ref{ins}) using the copy in the enveloping algebra, we have the relation  \n\\begin{equation}\n\\det(A)=\\prod_{s=1}^{d}\\prod_{m=0}^{2\\ell}\\left(2\\ell-m\\right)!m!\\;M^{2\\ell d+d-4}\\left(C_{4}^{\\prime}\\right)^2.\\label{insa}\n\\end{equation}\n \n\\medskip\n\\noindent Similarly, we can consider the invariants of $\\mathfrak{Gal}_{\\ell}(p,q)$ that are simultaneously invariants of the subalgebra $\\mathfrak{so}(p,q)\\overrightarrow{\\oplus}_{\\Gamma}\\mathfrak{h}_N$ with $d=p+q$.  \nFor the pseudo-orthogonal Lie algebra $\\frak{so}(p,q)$, a maximal set of Casimir operators is well known to be given\nby the coefficients $C_{k}$ of the characteristic polynomial $P(T)$ of the matrix \n\\begin{equation}\nB_{p,q}:=\\left(\n\\begin{array}{cccccc}\n0 & .. & -g_{jj}e_{1j} & .. & -g_{NN}e_{1,N}  \\\\\n: &  & : &  & : &  \\\\\ng_{11}e_{1j} & .. & 0 & .. & -g_{NN}e_{j,N}  \\\\\n: &  & : &  & : &  \\\\\ng_{11}e_{1,N} & .. & g_{jj}e_{j,N} & .. & 0 & \n\\end{array}\n\\right)\\label{MA2}\n\\end{equation}\n \n\\noindent The same formula, replacing the generators $e_{i,j}$ by those $\\widetilde{e}_{i,j}$ of the virtual copy will provide us with  the invariants of $\\mathfrak{Gal}_{\\ell}(p,q)$ that only depend on the generators of $\\frak{so}(p,q)$ and the characteristic representation $\\Gamma$.  \n\n\\begin{proposition}\nA maximal set of $\\left[\\frac{d}{2}\\right]$ independent Casimir operators of $\\mathfrak{Gal}_{\\ell}(p,q)$ depending only on the generators of  $\\frak{so}(p,q)$ and the $\\left\\{P_{0,i},\\cdots P_{2\\ell,i}\\right\\}$ with $1\\leq i\\leq p+q=d$  \nis given by the coefficients $\\widetilde{C}_{k}$ of the polynomial $P(T)$ defined by\n\\begin{equation}\nP(T):=\\det \\left( B_{p,q}-T\\;\\mathrm{Id}_{N}\\right)  ,  \\label{Pol1}\n\\end{equation}\nwhere\n\\begin{equation}\nB_{p,q}:=\\left(\n\\begin{array}{cccccc}\n0 & .. & -g_{jj}\\widetilde{e}_{1j} & .. & -g_{NN}\\widetilde{e}_{1,N}  \\\\\n: &  & : &  & : &  \\\\\ng_{11}\\widetilde{e}_{1j} & .. & 0 & .. & -g_{NN}\\widetilde{e}_{j,N}  \\\\\n: &  & : &  & : &  \\\\\ng_{11}\\widetilde{e}_{1,N} & .. & g_{jj}\\widetilde{e}_{j,N} & .. & 0 & \n\\end{array}\n\\right)\n\\end{equation}\n\\end{proposition}\nThe actual symmetric representatives ${\\rm Sym}(\\widetilde{C}_k)$ of the invariants as elements in the enveloping algebra are obtained from the symmetrization map (\\ref{syma}). \n\n\\medskip\n\\noindent It follows that the orders of the $1+\\left[\\frac{p+q}{2}\\right]$ non-central invariants of $\\mathfrak{Gal}_{\\ell}(p,q)$ are \n\\begin{itemize}\n\\item $4,4,8,\\cdots ,2(p+q-1)$ if $d=p+q$ is odd, \n\n\\item $4,4,8,\\cdots ,2(p+q)-4,p+q$ if $d=p+q$ is even. \n\\end{itemize}\n\n\n\n\\section{The unextended case}\n\n\\noindent As the centre of the Lie algebra $\\mathfrak{Gal}_{\\ell}(p,q)$ is one-dmensional, the corresponding factor algebra  $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)=\\mathfrak{Gal}_{\\ell}(p,q)/Z\\left(\\mathfrak{Gal}_{\\ell}(p,q)\\right)$ inherits the structure of a semidirect product of the semisimple Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus \\mathfrak{so}(p,q)$ with the Abelian Lie algebra of dimension $d(2\\ell+1)$, where the characteristic representation $\\Gamma$ is given by $D_{\\ell}\\otimes \\rho_1$. As this Lie algebra contains in particular the affine Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\overrightarrow{\\oplus}_{D_{\\ell}^{d}} \\mathbb{R}^{(2\\ell d+d)}$ as well as the multiply-inhomogeneous algebra $\\mathfrak{so}(p,q)\\overrightarrow{\\oplus}_{\\rho^{2\\ell+1}} \\mathbb{R}^{(2\\ell d+d)}$, it is expected that the number of Casimir invariants of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ will be much higher than that of $\\mathfrak{Gal}_{\\ell}(p,q)$. An exception is given by the special case $\\overline{\\mathfrak{Gal}}_{\\frac{1}{2}}(p,q)$, isomorphic to the unextended Schr\\"odinger algebra $\\widehat{\\mathcal{S}}(p+q)$, for which the number of invariants is given by $\\mathcal{N}(\\widehat{\\mathcal{S}}(p+q))=1+\\left[\\frac{p+q}{2}\\right]$, constituting the only case where the number of (non-central) Casimir operators of the extension is preserved when passing to the factor Lie algebra.    \n\n\n\\begin{proposition}\\label{pro4}\nFor any $\\ell\\in \\mathbb{Z}+\\frac{1}{2}\\geq \\frac{1}{2}$ and $p+q=d\\geq 3$ the number $\\mathcal{N}(\\mathfrak{g})$ of Casimir operators of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ is given by \n\\begin{eqnarray}\n\\mathcal{N}(\\mathfrak{g})=\\left\\{\n\\begin{array}[c]{rc}\n1+\\left[\\frac{d}{2}\\right], & \\ell=\\frac{1}{2},\\; d\\geq 3\\\\[0.1cm]\n\\frac{1}{2}\\left(4\\ell d+3d-d^2-6\\right), & \\ell\\geq \\frac{3}{2},\\;  d\\leq 2\\ell+2\\\\[0.1cm]\n2\\ell^2+2\\ell-\\frac{5}{2}+\\left[\\frac{d}{2}\\right], &  \\ell\\geq \\frac{3}{2},\\; d\\geq 2\\ell+3 \\\\\n\\end{array}\n\\right.\n\\end{eqnarray}\n\\end{proposition}\n\n\n\\noindent To prove the assertion, the best strategy is to use the reformulation of the formula (\\ref{BB}) in terms of differential forms \\cite{C43}. \nLet $\\left\\{\\theta_1,\\theta_2,\\theta_3,\\omega_{i,j},\\sigma_{n,j}\\right\\}$ with $1\\leq i,j\\leq d$, $0\\leq n\\leq 2\\ell$ be a basis of 1-forms dual to the basis $\\left\\{H,D,C,E_{i,j},P_{n,j}\\right\\}$ of  $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$. Then the Maurer-Cartan equations are given by \n\n\\begin{eqnarray}\nd\\theta_1=-\\theta_2\\wedge\\theta_3,\\quad d\\theta_2=2\\theta_1\\wedge\\theta_2,\\quad d\\theta_3=-2\\theta_1\\wedge\\theta_3,\\nonumber\\\\\nd\\omega_{i,j}=\\sum_{s=1}^{d} g_{ss} \\omega_{i,s}\\wedge\\omega_{j,s},\\quad 1\\leq i<j\\leq d,\\nonumber\\\\\nd\\sigma_{0,j}=2\\ell \\theta_1\\wedge\\sigma_{0,j}-\\theta_2\\wedge\\sigma_{1,j}+\\sum_{s=1}^{d}g_{ss}\\omega_{s,j}\\wedge\\sigma_{0,s},,\\quad 1\\leq j\\leq d,\\label{MCA}\\\\\nd\\sigma_{n,j}=2(\\ell-n) \\theta_1\\wedge\\sigma_{n,j}-(n+1)\\theta_2\\wedge\\sigma_{n+1,j}+(2\\ell+1-n)\\theta_3\\wedge\\sigma_{n-1,j}\\nonumber\\\\\n\\quad +\\sum_{s=1}^{d}g_{ss}\\omega_{s,j}\\wedge\\sigma_{n,s},\\quad 1\\leq n\\leq 2\\ell-1,\\; 1\\leq j\\leq d,\\nonumber\\\\\nd\\sigma_{2\\ell,j}=-2\\ell \\theta_1\\wedge\\sigma_{2\\ell,j}+\\theta_3\\wedge\\sigma_{2\\ell-1,j}+\\sum_{s=1}^{d}g_{ss}\\omega_{s,j}\\wedge\\sigma_{n,s},,\\quad 1\\leq j\\leq d,.\\nonumber \n\\end{eqnarray}\nWe first consider the case $\\ell=\\frac{1}{2}$ corresponding to the unextended Schr\\"{o}dinger algebra. For $%\nd\\leq 4$ the assertion follows at once considering the 2-form \n\\begin{equation*}\n\\Xi _{1}=d\\sigma _{0,1}+d\\sigma _{1,d},\n\\end{equation*}\nthat has rank $5$ for $d=3$ and rank $7$ for $d=4$ respectively. For values $%\nd\\geq 5$ we define the forms \n\\begin{equation*}\n\\Xi _{1}=d\\sigma _{0,1}+d\\sigma _{1,d},\\;\\Xi _{2}=\\sum_{s=0}^{\\alpha\n}d\\omega _{2+2s,3+2s},\\;\\alpha =\\frac{2d-11-\\left( -1\\right) ^{d}}{4}.\n\\end{equation*}\nProceeding by induction, it can be easily shown that the product \n\\begin{equation*}\n\\bigwedge^{d+1}d\\sigma _{0,1}\\bigwedge^{d-2}d\\sigma\n_{0,1}\\bigwedge^{d-4}d\\omega _{2,3}\\cdots \\bigwedge^{d-4-2\\alpha }d\\sigma\n_{2+2\\alpha ,3+2\\alpha }\n\\end{equation*}\ncontains all of the 1-forms associated to generators of the Lie algebra $\n\\widehat{\\mathcal{S}}\\left( d\\right) $ with the following exceptions \n\\begin{equation}\n\\theta _{3},\\omega _{2,3},\\omega _{4,5},\\cdots ,\\omega _{d-2,d-1},\\sigma\n_{1,d}.  \\label{exe}\n\\end{equation}\nCounting the latter elements we conclude that \n\\begin{equation}\n2d-1+\\sum_{s=0}^{\\alpha }\\left( d-4-2s\\right) =\\mu =\\frac{1}{4}\\left(\nd^{2}+3d-4-2\\left[ \\frac{d}{2}\\right] \\right) .  \\label{exec}\n\\end{equation}\nTherefore, taking the 2-form $\\Xi =\\Xi _{1}+\\Xi _{2}$, it is straightforward\nto verify that it satisfies \n\\begin{equation*}\n\\bigwedge^{\\mu }\\Xi =\\bigwedge^{d+1}d\\sigma _{0,1}\\bigwedge^{d-2}d\\sigma\n_{0,1}\\bigwedge^{d-4}d\\omega _{2,3}\\cdots \\bigwedge^{d-4-2\\alpha }d\\omega\n_{2+2\\alpha ,3+2\\alpha }+\\cdots \\neq 0,\n\\end{equation*}\nshowing that \n\\begin{equation*}\n\\mathcal{N}\\left( \\widehat{\\mathcal{S}}\\left( d\\right) \\right) =1+\\left[ \\frac{d}{2}\\right] .\n\\end{equation*}\n\n\\noindent This argumentation, with slight modifications, generalizes naturally for any value $\\ell\\geq \\frac{3}{2}$, where it is also necessary to distinguish two cases,  depending whether $d=p+q\\leq 2\\ell +2$ or $d>2\\ell+2$. \n\n\n\\begin{enumerate}\n\\item Let $d=p+q\\leq 2\\ell +2$. In this case the dimension of the characteristic representation $\\Gamma$ is clearly larger than that of the Levi subalgebra, so that a 2-form of maximal rank can be constructed using only the differential forms associated to the generators $P_{n,k}$. Consider the 2-form in (\\ref{MCA}) given by $\\Theta=\\Theta_1+\\Theta_2$, where \n\\begin{eqnarray}\n\\Theta_1=d\\sigma_{0,1}+d\\sigma_{2\\ell,d}+d\\sigma_{2\\ell-1,d-1},\\; \n\\Theta_2=\\sum_{s=1}^{d-4} d\\sigma_{s,s+1}.\\label{difo1}\n\\end{eqnarray}\nUsing the decomposition formula $\\bigwedge^{a}\\Theta=\\sum_{r=0}^{a} \\left(\\bigwedge^{r}\\Theta_1\\right) \\wedge \\left(\\bigwedge^{a-r}\\Theta_2\\right)$ we obtain that \n\\begin{eqnarray}\n\\fl \\bigwedge^{\\frac{1}{2}\\left(6-d+d^2\\right)}\\Theta= &\\bigwedge^{d+1}d\\sigma_{0,1}\\wedge\\bigwedge^{d-1}d\\sigma_{2\\ell,d}\\wedge\\bigwedge^{d-3}d\\sigma_{2\\ell-1,d-1}\\wedge\n\\bigwedge^{d-4}d\\sigma_{1,2}\\wedge\\nonumber\\\\\n& \\wedge\\bigwedge^{d-5}d\\sigma_{2,3}\\wedge\\bigwedge^{d-6}d\\sigma_{3,4}\\wedge\\cdots \\bigwedge^{2}d\\sigma_{d-5,d-4}\\wedge d\\sigma_{d-4,d-3}+\\cdots \\neq 0.\\label{pro2}\n\\end{eqnarray}\nAs $\\frac{1}{2}\\left(6-d+d^2\\right)=\\dim\\left(\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)\\right)$, the 2-form $\\Theta$ is necessarily of maximal rank, as all the generators of the Levi subalgebra appear in some term of the product (\\ref{pro2}) and no products of higher rank are possible due to the Abelian nilradical. We therefore conclude that $j(\\mathfrak{g})=\\frac{1}{2}\\left(6-d+d^2\\right)$ and by formula (\\ref{BB1}) we have \n\\begin{equation}\n\\mathcal{N}(\\mathfrak{g})= \\frac{1}{2}\\left(4\\ell d+3d-d^2-6\\right).\\label{inva1}\n\\end{equation}\n\n\\item Now let $d \\geq 2\\ell +3$. The main difference with respect to the previous case is that a generic form $\\omega\\in\\mathcal{L}(\\mathfrak{g})$ of maximal rank must necessarily contain linear combinations of the 2-forms $d\\omega_{i,j}$ corresponding to the semisimple part of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$. Let us consider first  the 2-form \n\\begin{equation}\n\\Xi_1= \\Theta_1+\\Theta_2,\n\\end{equation}\nwhere $\\Theta_1$ is the same as in (\\ref{difo1}) and $\\Theta_2$ is defined as\n\\begin{equation}\n\\Theta_2=\\sum_{s=0}^{2\\ell-3} d\\sigma_{1+s,2+s}.\n\\end{equation}\nIn analogy with the previous case, for the index  $\\mu_1=(2\\ell+1)d+(\\ell+2)(1-2\\ell)$ the first term of the following product does not vanish: \n\\begin{equation}\n\\fl \\bigwedge^{\\mu_1}\\Xi_1=\\bigwedge^{d+1}d\\sigma_{0,1}\\bigwedge^{d-1}d\\sigma_{2\\ell,d}\\bigwedge^{d-3}d\\sigma_{2\\ell-1,d-1} \n\\bigwedge^{d-4}d\\sigma_{1,2}\\cdots \\bigwedge^{d-1-2\\ell}d\\sigma_{2\\ell-2,2\\ell-1}+\\cdots \\neq 0.\\label{Pot1}\n\\end{equation}\nThis form, although not maximal in $\\mathcal{L}(\\mathfrak{g})$, is indeed of maximal rank when restricted to the subspace $\\mathcal{L}(\\mathfrak{r})$ generated by the 2-forms $d\\sigma_{n,k}$ with $0\\leq n\\leq 2\\ell$, $1\\leq k\\leq d$. \nThis means that the wedge product of $\\bigwedge^{\\mu_1}\\Xi_1$  with any other $d\\sigma_{n,k}$ is identically zero. Hence, in order to construct a 2-form of maximal rank in $\\mathcal{L}(\\mathfrak{g})$, we have to consider a 2-form $\\Xi_2$ that is a linear combination of the  differential forms associated to the generators of the Levi subalgebra of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$. As follows at once from (\\ref{Pot1}), the forms $\\theta_1,\\theta_2,\\theta_3$ associated to $\\mathfrak{sl}(2,\\mathbb{R})$-generators have already appeared, thus it suffices to restrict our analysis to linear combinations of the forms $d\\omega_{i,j}$ corresponding to the pseudo-orthogonal Lie algebra $\\mathfrak{so}(p,q)$. Specifically, we make the choice \n\\begin{equation}\n\\Xi_2= \\sum_{s=0}^{\\nu}d\\omega_{3+2s,4+2s},\\quad \\nu=\\frac{1}{4}\\left(2d-4\\ell-9+(-1)^{1+d}\\right).\n\\end{equation} \nConsider the integer $\\mu_2=\\frac{1}{4}\\left(11+(d-4\\ell)(1+d)-4\\ell^2-2\\left[\\frac{d}{2}\\right]\\right)$ and take the 2-form $\\Xi=\\Xi_1+\\Xi_2$. A long but routine computation shows that following identity is satisfied:\n\\begin{eqnarray}\n\\fl \\bigwedge^{\\mu_1+\\mu_2}\\Xi =& \\left(\\bigwedge^{\\mu_1}\\Xi_1\\right)\\wedge \\left(\\bigwedge^{\\mu_2}\\Xi_2\\right) \\nonumber\\\\\n& =  \\left(\\bigwedge^{\\mu_1}\\Xi_1\\right)\\wedge\\bigwedge^{d-6}d\\omega_{3,4}\\bigwedge^{d-8}d\\omega_{5,6}\\cdots \\bigwedge^{d-6-2\\nu}d\\omega_{3+2\\nu,4+2\\nu}+\\cdots \\neq 0.\\label{pro1}\n\\end{eqnarray}\nWe observe that this form involves $\\mu_1+2\\mu_2$ forms $\\omega_{i,j}$ from $\\mathfrak{so}(p,q)$, hence there remain $\\frac{d(d-1)}{2}-\\mu_1-2\\mu_2$ elements of the pseudo-orthogonal that do not appear in the first term in (\\ref{pro1}). From this product and (\\ref{MCA}) it can be seen that these uncovered elements are of the type $\\left\\{\\omega_{i_1,i_1+1},\\omega_{i_2,i_2+1},\\cdots \\omega_{i_r,i_r+1}\\right\\}$ with the subindices satisfying $i_{\\alpha+1}-i_{\\alpha}\\geq 2$ for $1\\leq \\alpha\\leq r$, from which we deduce that no other 2-form $d\\omega_{i_\\alpha,i_\\alpha+1}$, when multiplied with  $\\bigwedge^{\\mu_1+\\mu_2}\\Xi $ will be different from zero. \nWe conclude that $\\Xi$ has maximal rank equal to  $j_0(\\mathfrak{g})=\\mu_1+\\mu_2$, thus applying (\\ref{BB1}) we find that \n\\begin{equation}\n\\fl \\mathcal{N}(\\mathfrak{g})= 3 + \\frac{d(d-1)}{2}+ (2 \\ell + 1) d-2(\\mu_1+\\mu_2)=  2\\ell^2+2\\ell-\\frac{5}{2}+\\left[\\frac{d}{2}\\right],\n\\end{equation}\nas asserted.\n\\end{enumerate}\n\n\\medskip\n\\noindent In Table \\ref{Tabelle1} we give the numerical values for the number of Casimir operators of the Lie algebras $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ with $d=p+q\\leq 12$, and where the linear increment with respect to $\\ell$ can be easily recognized. \n \n\\smallskip\n\\begin{table}[h!] \n\\caption{\\label{Tabelle1} Number of Casimir operators for $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$.}\n\\begin{indented}\\item[]\n\\begin{tabular}{c||cccccccccc}\n$\\;d$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$ \\\\\\hline \n{$\\ell=\\frac{1}{2}$} & $2$ & $3$ & $3$ & $4$ & $4$ & $5$ & $5$\n& $6$ & $6$ & $7$ \\\\ \n{$\\ell=\\frac{3}{2}$} & $6$ & $7$ & $7$ & $8$ & $8$ & $9$ & $9$\n& $10$ & $10$ & $11$ \\\\ \n{$\\ell=\\frac{5}{2}$} & $12$ & $15$ & $17$ & $18$ & $18$ & $19$\n& $19$ & $20$ & $20$ & $21$ \\\\ \n{$\\ell=\\frac{7}{2}$} & $18$ & $23$ & $27$ & $30$ & $32$ & $33$\n& $33$ & $34$ & $34$ & $35$ \\\\ \n{$\\ell=\\frac{9}{2}$} & $24$ & $31$ & $37$ & $42$ & $46$ & $49$\n& $51$ & $52$ & $52$ & $53$ \\\\ \n{$\\ell=\\frac{11}{2}$} & $30$ & $39$ & $47$ & $54$ & $60$ & $65\n$ & $69$ & $72$ & $74$ & $75$%\n\\end{tabular}\n\\end{indented}\n\\end{table}\n\n\\medskip\n\\noindent As follows from a general property concerning virtual copies \\cite{C45}, Lie algebras of the type $\\mathfrak{g}=\\mathfrak{s}\\overrightarrow{\\oplus} \\mathfrak{r}$ with an Abelian radical $\\mathfrak{r}$ do not admit virtual copies of $\\mathfrak{s}$ in $\\mathcal{U}\\left(\\mathfrak{g}\\right)$. Thus for Lie algebras of this type the Casimir invariants must be computed either directly from system (\\ref{sys}) or by some other procedure. Among the class  $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$, an exception is given by the unextended (pseudo-)Schr\\"odinger algebra  $\\overline{\\mathfrak{Gal}}_{\\frac{1}{2}}(p,q)\\simeq \\widehat{\\mathcal{S}}(p,q)$, where the invariants can be deduced from those of the central extension $\\widehat{\\mathcal{S}}(p,q)$ by the widely used method of contractions (see e.g. \\cite{IW,We}). For the remaining values $\\ell\\geq \\frac{3}{2}$ the contraction procedure is useless in practice, given the high number of invariants.  However, an interesting property concerning the invariants of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ emerges when we try to find the Casimir operators $F$ that only depend on variables $p_{n,k}$ associated to generators $P_{n,k}$ of the radical, i.e., such that the condition \n\\begin{equation}\n\\quad \\frac{\\partial F}{\\partial x}=0,\\quad \\forall x\\in\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q).\\label{kond}\n\\end{equation}\nis satisfied. As will be shown next, the number of such solutions tends to stabilize for high values of $d=p+q$, showing that almost any invariant will depend on all of the variables in $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$, implying that finding a complete set of invariants is a computationally formidable task, as there is currently no general method to derive these invariants in closed form. \n\n\\begin{proposition}\nLet $\\ell\\geq \\frac{3}{2}$. For sufficiently large $d$, the number of Casimir invariants of  $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$ depending only on the variables $p_{n,k}$ of the Abelian radical is constant and given by \n\\begin{equation}\n\\mathcal{N}_1(S)=2\\ell^2+3\\ell-2.\\label{sr2}\n\\end{equation}\n\\end{proposition}\n\n\\noindent The proof follows analyzing the rank of the subsystem of (\\ref{sys}) corresponding to the differential operators $\\widehat{X}$ associated to the generators of the Levi subalgebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)$ and such that condition (\\ref{kond}) is fulfilled. Specifically, this leads to the system $S$ of PDEs\n\\begin{eqnarray}\n\\widehat{D}^{\\prime}(F):=\\sum_{n=0}^{2\\ell}\\sum_{i=1}^{d} (2\\ell-n)p_{n,i}\\frac{\\partial F}{\\partial p_{n,i}}=0,\\; \n\\widehat{H}^{\\prime}(F):=\\sum_{n=0}^{2\\ell}\\sum_{i=1}^{d} n p_{n-1,i}\\frac{\\partial F}{\\partial p_{n,i}}=0,\\nonumber\\\\\n\\widehat{C}^{\\prime}(F):=\\sum_{n=0}^{2\\ell}\\sum_{i=1}^{d} (2\\ell-n)p_{n+1,i}\\frac{\\partial F}{\\partial p_{n,i}}=0,\\label{kond2}\\\\\n\\widehat{E}_{j,k}^{\\prime}(F):=\\sum_{n=0}^{2\\ell}\\sum_{i=1}^{d} \\left( g_{ij} p_{n,k} -g_{ik} p_{n,j}\\right) \\frac{\\partial F}{\\partial p_{n,i}}=0, 1\\leq j<k\\leq d.\\nonumber\n\\end{eqnarray}\nThis system consists of $\\frac{1}{2}\\left(6-d+d^2\\right)$ equations in $(2\\ell+1)d$ variables that becomes overdetermined for increasing values of $d$ (and fixed $\\ell$). In Table \\ref{Tabelle2} the rank of such systems is given for values $d\\leq 15$, showing that for fixed $\\ell$, from $d\\geq 2\\ell+1$ onwards, the rank of the system increases always by the same constant amount, given precisely by $2\\ell+1$.  \n\n\\begin{table}[h!] \n\\caption{\\label{Tabelle2} Rank of system (\\ref{kond2}).}\n\\begin{indented}\\item[]\n\\begin{tabular}{c||ccccccccccccc}\n$d$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$ & $13$& $14$& $15$ \\\\ \\hline\n$\\ell =\\frac{3}{2}$ & 6 & 9 & 13 & 17 & 21 & 25 & 29 & 33 & 37 & 41 & 45 & 49  & 53\\\\\n$\\ell =\\frac{5}{2}$ & 6 & 9 & 13 & 18 & 24 & 30 & 36 & 42 & 48 & 54 & 60 & 66 & 72\\\\\n$\\ell =\\frac{7}{2}$ & 6 & 9 & 13 & 18 & 24 & 31 & 39 & 47 & 55 & 63 & 71 & 79\n& 87 \\\\ \n$\\ell =\\frac{9}{2}$ & 6 & 9 & 13 & 18 & 24 & 31 & 39 & 48 & 58 & 68 & 78 & 88\n& 98 \\\\ \n$\\ell =\\frac{11}{2}$ & 6 & 9 & 13 & 18 & 24 & 31 & 39 & 48 & 58 & 69 & 81 &\n93 & 105 \\\\ \n$\\ell =\\frac{13}{2}$ & 6 & 9 & 13 & 18 & 24 & 31 & 39 & 48 & 58 & 69 & 81 & 94 & 108\n\\end{tabular}\n\\end{indented}\n\\end{table}\n\\noindent With these observations, it is not difficult to establish that for any $\\ell\\geq \\frac{3}{2}$ and $d\\geq 2\\ell+1$ the rank of the system (\\ref{kond2}) is given by \n\\begin{equation}\n{\\rm rank}\\; S =\\left(2+d\\right)+\\ell\\left(2d-3\\right)-2\\ell^2.\\label{kond3}\n\\end{equation}\nAs the number of variables is $(2\\ell+1)d$, we conclude that the system admits exactly\n\\begin{equation}\n\\mathcal{N}_1(S)= (2\\ell+1)d- {\\rm rank}\\; S = 2\\ell^2+3\\ell-2\n\\end{equation}\nsolutions satisfying the constraint (\\ref{kond}). Further, comparison with Proposition \\ref{pro4} allows us to establish that for any fixed $\\ell$ and $d\\leq 2\\ell+2$, the following identity holds:\n\\begin{equation}\n\\mathcal{N}\\left(\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)\\right)=\\mathcal{N}_1(S).\\label{trox}\n\\end{equation}\nFor increasing values of $d$, there appear additional invariants that necessarily depend on variables associated to the generators of the Levi subalgebra of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q) $. \n\n\\medskip\n\\noindent Although there is currently no algorithmic procedure to construct a complete set of invariants of these Lie algebras for arbitrary values $d>2\\ell+2$, those invariants of $\\mathfrak{Gal}_{\\ell}(p,q)$ satisfying the condition (\\ref{kond}) can be easily computed by means of a reduction argument that leads to a linear system. To this extent, consider the last of the equations in (\\ref{kond2}). As the generators of $\\mathfrak{so}(p,q)$ permute the generators of the Abelian radical, it is straightforward to verify that the quadratic polynomials \n\\begin{equation}\n\\Phi_{n,s}= \\sum_{k=1}^{d} \\frac{g_{11}}{g_{kk}}\\;p_{n,k}p_{n+s,k},\\; 0\\leq n\\leq 2\\ell,\\; 0\\leq s\\leq 2\\ell-n.\\label{ELE}\n\\end{equation}\nare actually solutions of these equations. Indeed, any solution of the type (\\ref{kond}) is built up from these functions. Let $\\mathcal{M}_d=\\left\\{\\Phi_{n,s},\\; 0\\leq n\\leq 2\\ell,\\; 0\\leq s\\leq 2\\ell-n\\right\\}$. The cardinal of this set is given by $2\\ell^2+3\\ell+1$, and we observe that not all of the elements in $\\mathcal{M}_d$ are independent. It follows by a short computation that \n\\begin{equation}\n\\widehat{D}^{\\prime}(\\mathcal{M}_d)\\subset \\mathcal{M}_d,\\; \\widehat{H}^{\\prime}(\\mathcal{M}_d)\\subset \\mathcal{M}_d,\\; \\widehat{C}^{\\prime}(\\mathcal{M}_d)\\subset \\mathcal{M}_d,\\label{ELE2}\n\\end{equation}\nshowing that this set is invariant by the action of $\\mathfrak{sl}(2,\\mathbb{R})$. Therefore, we can construct the solutions of system (\\ref{kond2}) recursively using polynomials in the new variables $\\Phi_{n,s}$. Specifically, renumbering the elements in $\\mathcal{M}_d$ as $\\left\\{u_{1},\\cdots ,u_{2\\ell^2+3\\ell+1}\\right\\}$, for any $r\\geq 2$ we define a polynomial of degree $2r$ as  \n\\begin{equation}\n\\Psi_r= \\sum_{1\\leq i_1< \\cdots <i_r\\leq |\\mathcal{M}_d|} \\alpha^{i_1\\cdots i_r}  u_{i_1}u_{i_2}\\cdots u_{i_r},\\; i_1+\\cdots i_r=r.\\label{poly}\n\\end{equation}\nNow, imposing the constraints\n\\begin{equation}\n\\widehat{D}^{\\prime}(\\Psi_r)=0,\\; \\widehat{H}^{\\prime}(\\Psi_r)=0,\\; \\widehat{C}^{\\prime}(\\Psi_r)=0,\\label{ELE3}\n\\end{equation}\nleads to a linear system in the coefficients $\\alpha^{i_1\\cdots i_r}$, the solutions of which enable us to find the polynomials that satisfy system (\\ref{kond2}). Alternatively, the functions \n$\\Phi_{n,s}$ can be used as new variables to reduce the equations in (\\ref{ELE3}) to a simpler form, which may be computationally more effective, albeit the underlying argument is essentially the same \\cite{Dick}. In the case where the identity (\\ref{trox}) holds, this reduction procedure allows us to obtain a complete set of invariants for the Lie algebra $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q) $. \n\n\n\\medskip\n\\noindent As an example to illustrate the reduction, consider the 18-dimensional Lie\nalgebra $\\overline{\\frak{Gal}}_{\\frac{3}{2}}\\left( 3\\right) $. As $d<2\\ell+2$,\nformula (\\ref{trox}) applies and the algebra has 6 Casimir operators. From these,\ntwo of order four in the generators can be derived from the central\nextension $\\frak{Gal}_{\\frac{3}{2}}\\left( 3\\right) $ by contraction \\cite\n{We}. In this case, the set $\\mathcal{M}_{3}$ has ten elements that we\nenumerate as follows:      \n\\begin{equation*}\n\\left\\{ \\Phi _{00},\\Phi _{01},\\Phi _{02},\\Phi _{03},\\Phi _{10},\\Phi\n_{11},\\Phi _{12},\\Phi _{20},\\Phi _{21},\\Phi _{30}\\right\\} =\\left\\{\nu_{1},\\cdots ,u_{10}\\right\\} .\n\\end{equation*}\nThe action of the differential operators associated to $\\frak{sl}\\left( 2,%\n\\mathbb{R}\\right) $ on $\\mathcal{M}_{3}$ is explicitly given in Table \\ref{Tabelle3}. \n\n\n\\begin{table}[h!] \n\\caption{\\label{Tabelle3} Transformation rules of variables $u_i$ under the $\\mathfrak{sl}(2,\\mathbb{R})$-action (\\ref{kond2}).}\n\\footnotesize\\rm\n\\begin{tabular}{@{}*{1}{c|cccccccccc}}\n& $u_{1}$ & $u_{2}$ & $u_{3}$ & $u_{4}$ & $u_{5}$ & $u_{6}$ & $u_{7}$ & $%\nu_{8}$ & $u_{9}$ & $u_{10}$ \\\\[0.1cm] \\hline  \n$\\widehat{D}^{\\prime }$ & $6u_{1}$ & $4u_{2}$ & $2u_{3}$ & $0$ & $2u_{5}$ & $%\n0$ & $-2u_{7}$ & $-2u_{8}$ & $-4u_{9}$ & $-6u_{10}$ \\\\ \n$\\widehat{H}^{\\prime }$ & $0$ & $-u_{1}$ & $-2u_{2}$ & $-3u_{3}$ & $-2u_{2}$\n& $-u_{3}-2u_{5}$ & $-u_{4}-3u_{6}$ & $-4u_{6}$ & $-2u_{7}-3u_{8}$ & $-6u_{9}\n$ \\\\ \n$\\widehat{C}^{\\prime }$ & $6u_{2}$ & $2u_{3}+3u_{5}$ & $u_{4}+3u_{6}$ & $%\n3u_{7}$ & $4u_{6}$ & $u_{2}+2u_{8}$ & $2u_{9}$ & $2u_{9}$ & $u_{10}$ & $0$%\n\\end{tabular}\n\\end{table}\nIt follows from this action that polynomials $\\Psi _{r}$ in the $u_{i}$ that satisfy the system (\\ref\n{ELE3}) are the solutions of the following system of linear first-order\npartial differential equations:   \n\n \n{\\footnotesize\n\\begin{equation}\n\\fl\n\\begin{tabular}{rr}\n$6u_{1}\\frac{\\partial F}{\\partial u_{1}}+4u_{2}\\frac{\\partial F}{\\partial\nu_{2}}+2u_{3}\\frac{\\partial F}{\\partial u_{3}}+2u_{5}\\frac{\\partial F}{%\n\\partial u_{5}}-2u_{7}\\frac{\\partial F}{\\partial u_{7}}-2u_{8}\\frac{\\partial\nF}{\\partial u_{8}}-4u_{9}\\frac{\\partial F}{\\partial u_{9}}-6u_{10}\\frac{%\n\\partial F}{\\partial u_{10}}$ & $=0,$ \\\\ \n$-u_{1}\\frac{\\partial F}{\\partial u_{2}}-2u_{2}\\frac{\\partial F}{\\partial\nu_{3}}-3u_{3}\\frac{\\partial F}{\\partial u_{4}}-2u_{2}\\frac{\\partial F}{%\n\\partial u_{5}}-\\left( u_{3}+2u_{5}\\right) \\frac{\\partial F}{\\partial u_{6}}%\n-\\left( u_{4}+3u_{6}\\right) \\frac{\\partial F}{\\partial u_{7}}-4u_{6}\\frac{%\n\\partial F}{\\partial u_{8}}$ &  \\\\ \n$-\\left( 2u_{7}+3u_{8}\\right) \\frac{\\partial F}{\\partial u_{9}}-6u_{9}\\frac{%\n\\partial F}{\\partial u_{10}}$ & $=0,$ \\\\ \n$6u_{2}\\frac{\\partial F}{\\partial u_{1}}+\\left( 2u_{3}+3u_{5}\\right) \\frac{%\n\\partial F}{\\partial u_{2}}+\\left( u_{4}+3u_{6}\\right) \\frac{\\partial F}{%\n\\partial u_{3}}+3u_{7}\\frac{\\partial F}{\\partial u_{4}}+4u_{6}\\frac{\\partial\nF}{\\partial u_{5}}+\\left( u_{2}+2u_{8}\\right) \\frac{\\partial F}{\\partial\nu_{6}}+2u_{9}\\frac{\\partial F}{\\partial u_{7}}$ &  \\\\ \n$+2u_{9}\\frac{\\partial F}{\\partial u_{8}}+u_{10}\\frac{\\partial F}{\\partial\nu_{9}}$ & $=0.$%\n\\end{tabular}\\label{reda}\n\\end{equation}\n}\n\\noindent This system admits two quadratic solutions given by \n\\begin{eqnarray*}\nF_{1}\n&=&3u_{4}^{2}+27u_{6}^{2}-18u_{3}u_{7}-27u_{5}u_{8}+12u_{2}u_{9}-3u_{1}u_{10},\n\\\\\nF_{2}\n&=&27u_{6}^{2}-5u_{4}^{2}+18u_{4}u_{6}+12u_{3}u_{7}-4u_{1}u_{10}+24u_{2}u_{9}-36\\left( u_{5}u_{7}+u_{3}u_{8}\\right) .\n\\end{eqnarray*} \nIncidentally, these are the invariants that are obtained by contraction from those of the centrally-extended algebra $\\mathfrak{Gal}_{\n\\frac{3}{2}}\\left( 3\\right) $. \nIn addition, there exist four additional independent fourth-order solutions,\nthe explicit expression of which is omitted because of its length. We\nconclude that a complete set of Casimir operators of $\\overline{\\frak{Gal}}_{%\n\\frac{3}{2}}\\left( 3\\right) $ is given by two fourth-order polynomials in\nthe generators (corresponding to the quadratic solutions of (\\ref{reda}))\nand four invariants of order eight corresponding to the fourth-order\nsolutions of (\\ref{reda}). \n\n\n\n\\section{Final remarks}\n\nWe have seen that the generalized conformal Galilean algebras $\\widehat{\\mathfrak{g}}_{\\ell}(d)$ based on the semisimple Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(d)$ can be extended naturally to pseudo-Galilean algebras possessing a Levi subalgebra isomorphic to $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)$ introducing a nondegenerate metric tensor into the orthogonal part. Virtual copies of $\\mathfrak{sl}(2,\\mathbb{R})\\oplus\\mathfrak{so}(p,q)$ in the enveloping algebra of the semidirect product can be obtained simultaneously for all (half-integer) values of $\\ell$ and $p+q=d$. The resulting Lie algebras $\\mathfrak{Gal}_{\\ell}\\left( p,q\\right) $ can be seen, to a certain extent, as ``real" forms of the conformal Galilean algebra $\\widehat{\\mathfrak{g}}_{\\ell}(d)$, their main structural difference residing in the maximal compact subalgebra. Whether these Lie algebras $\\mathfrak{Gal}_{\\ell}\\left( p,q\\right) $ have some definite physical meaning is still an unanswered question, but it is conceivable that they appear in the context of dynamical groups of higher order Lagrangian systems or as the (maximal) invariance symmetry group of a (hierachy of) partial differential equations. The search of physical realizations of the Lie algebras $\\mathfrak{Gal}_{\\ell}\\left( p,q\\right) $ is currently being developed.\n\n\\smallskip\n\\noindent We observe that the obstructions found for integer values of $\\ell$ and leading to the so-called exotic extensions (see e.g. \\cite{Als19} and reference therein) are a direct consequence of the incompatibility of the odd-dimensional representation $D_{\\ell}$ with a Heisenberg algebra. Indeed, as shown in \\cite{C45}, the necessary and sufficient condition for a semidirect product $\\mathfrak{s}\\overrightarrow {\\oplus}_{\\Gamma\\oplus \\Gamma_0}\\mathfrak{h}_n$ to exist is that the (nontrivial) characteristic representation $\\Gamma$ satisfies the condition $\\Gamma\\wedge \\Gamma\\supset \\Gamma_0$. For the decomposition of $\\Gamma$ into irreducible components, this implies in particular that an irreducible representation of $\\mathfrak{s}$ must appear with the same multiplicity as its dual or be self-dual. Therefore, in order to further generalize the notion of Galilean algebras to $\\mathfrak{sl}(2,\\mathbb{R})$-representations with even highest weight, the characteristic representation $\\Gamma$ must have the form\n\\begin{equation}\n\\Gamma =\\left(D_\\ell\\oplus D_\\ell\\right)\\otimes \\rho_d\n\\end{equation}\nAs happens with any coupling of a semisimple Lie algebra $\\mathfrak{s}$ and a Heisenberg Lie algebra $\\mathfrak{h}_n$, the (noncentral) Casimir operators of the semidirect product \n$\\mathfrak{s}\\overrightarrow {\\oplus}_{\\Gamma\\oplus \\Gamma_0}\\mathfrak{h}_n$ can be constructed using the invariants of  $\\mathfrak{s}$ by means of the virtual copy method \\cite{Que,C45}. Application of this procedure in combination with the trace method provides explicit expressions for the invariants of $\\mathfrak{Gal}_{\\ell}\\left( p,q\\right) $ for arbitrary values of $\\ell$ and $p+q=d$, comprising in particular the case $\\widehat{\\mathfrak{g}}_{\\ell}(d)=\\mathfrak{Gal}_{\\ell}\\left( d,0\\right) $ recently announced \\cite{raub}. \n\n\\medskip\n\\noindent  The case of the unextended conformal pseudo-Galilean algebra  $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q) $ corresponding to the factor of $\\mathfrak{Gal}_{\\ell}(p,q) $ by its centre has also been considered. As this Lie algebra has an Abelian radical, it does not admit a virtual copy in the corresponding enveloping algebra, hence their invariants must be computed by other means. The number of Casimir operators for arbitrary values of the parameters has been computed by means of the Maurer-Cartan equations of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q)$, where a varying increasement behaviour for the number of invariants in dependence of the proportion between the dimension of the pseudo-orthogonal subalgebra and the dimension $2\\ell+1$ of the $\\mathfrak{sl}(2,\\mathbb{R})$-representation $D_\\ell$ has been observed. Although explicit formulae for  the Casimir invariants of $\\overline{\\mathfrak{Gal}}_{\\ell}(p,q) $ with $\\ell\\geq \\frac{3}{2}$ can probably not be found generically, it has been shown that the functions depending only on variables of the radical provide a complete set of invariants for the Lie algebra whenever the condition $d\\leq 2\\ell+2$ is satisfied. A procedure that reduces the computation of such invariants to solving a linear system has been proposed. However, even with this systematization, the problem still involves cumbersome computations, as the orders of such invariants are quite elevated and there is currently no result that allows to predict these orders. For values $d\\geq 2\\ell+3$, where there exist Casimir operators that do not satisfy the condition (\\ref{kond}), no valuable ansatz has been found that allows to find them systematically. Any kind of progress in this direction would constitute a useful tool for the generic analysis of invariant functions of semidirect products of semisimple and Abelian Lie algebras, a class that up to certain relevant special cases has still not been exhaustively studied. \n\n\\medskip\n\\noindent \n\n\n\\section*{Acknowledgment}\nDuring the\npreparation of this work, the RCS was financially supported by\nthe research project MTM2016-79422-P of the AEI/FEDER (EU). IM was supported\nby the Australian Research Council Discovery Grant DP160101376 and Future Fellowship FT180100099. \n\n\\section*{References}\n', 'meta': {'timestamp': datetime.datetime(2019, 4, 24, 2, 4, 30), 'yymm': '1904', 'arxiv_id': '1904.10101', 'language': 'en', 'url': 'https://arxiv.org/abs/1904.10101'}} or rewrite tokenize_example in data.py 
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:00:10,623] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 16:00:10.624498 88146 tcp_utils.cc:107] Retry to connect to 10.3.242.26:58758 while the server is not yet listening.
I1231 16:00:13.624799 88146 tcp_utils.cc:130] Successfully connected to 10.3.242.26:58758
I1231 16:00:13.687353 88146 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:13.687376 88146 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:13,687] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:00:13.692692 88146 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:00:13.695051 88146 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-31 16:00:15,593] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:00:15,593] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:00:15.593845 88146 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.593870 88146 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,593] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:00:15.593947 88146 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:00:15.593952 88146 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:00:15,594] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:00:15,594] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:00:15,594] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:00:15,594] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,594] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,594] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,594] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:00:15,595] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,596] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - [0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:00:15,597] [   DEBUG][0m - [0m
[32m[2024-12-31 16:00:15,599] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[33m[2024-12-31 16:00:15,599] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:00:15,600] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:00:15,601] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:00:15,602] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:00:15,603] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:01:39,381] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:02:43,314] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:02:43,315] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:02:43,319] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:02:45,071] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:02:45,151] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:02:45,152] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:02:46,214] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[32m[2024-12-31 16:02:46,346] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:02:46,436] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:02:46,437] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_rank                  : 1[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:02:46,438] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:02:46,439] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-00-10_ubuntu[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:02:46,440] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - optimizer_name_suffix         : shard01[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:02:46,441] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:02:46,442] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:02:46,443] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - sharding_parallel_rank        : 1[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:02:46,444] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:02:46,445] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:02:46,446] [   DEBUG][0m - [0m
[32m[2024-12-31 16:02:46,448] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:02:46,477] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:02:47,266] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:02:47,266] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:02:47,268] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:02:47,866] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:02:47) [0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:02:47,867] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:02:47,871] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
Exception in thread Thread-2 (_thread_loop):
Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 85, in tokenize_unsupervised_example
    max_length=data_args.scaled_max_length,
AttributeError: 'DataConfig' object has no attribute 'scaled_max_length'. Did you mean: 'pad_to_max_length'?
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[2024-12-31 16:04:53,583] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 16:04:53.585142 96374 tcp_utils.cc:107] Retry to connect to 10.3.242.26:54723 while the server is not yet listening.
I1231 16:04:56.585414 96374 tcp_utils.cc:130] Successfully connected to 10.3.242.26:54723
I1231 16:04:56.591427 96374 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:56.591446 96374 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:56,591] [    INFO] topology.py:370 - Total 4 pipe comm group(s) create successfully!
W1231 16:04:56.594808 96374 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:04:56.595554 96374 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/group.py:128: UserWarning: Current global rank 1 is not in group _default_pg11
  warnings.warn(
[2024-12-31 16:04:58,900] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:04:58,900] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
I1231 16:04:58.901063 96374 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:58.901091 96374 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:58,901] [    INFO] topology.py:370 - Total 1 sharding comm group(s) create successfully!
I1231 16:04:58.901188 96374 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:04:58.901193 96374 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:04:58,901] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:04:58,901] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:04:58,902] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:04:58,902] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:04:58,903] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:58,904] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - [0m
[35m[2024-12-31 16:04:58,905] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:04:58,906] [   DEBUG][0m - [0m
[32m[2024-12-31 16:04:58,907] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[33m[2024-12-31 16:04:58,908] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:04:58,908] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:04:58,910] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:04:58,911] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:04:58,912] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:06:10,174] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2024-12-31 16:07:23,124] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-12-31 16:07:23,125] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-12-31 16:07:23,128] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/generation_config.json[0m
[32m[2024-12-31 16:07:25,046] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:07:25,109] [   DEBUG][0m - Frozen parameters: 6.74e+09 || Trainable parameters:2.00e+07 || Total parameters:6.76e+09|| Trainable:0.30%[0m
[35m[2024-12-31 16:07:25,110] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:07:26,417] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 42.[0m
[32m[2024-12-31 16:07:26,563] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:07:26,654] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:07:26,655] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_rank                  : 1[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - dataset_world_size            : 4[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:07:26,656] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_batch_size               : 8[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:07:26,657] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-04-53_ubuntu[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:07:26,658] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - optimizer_name_suffix         : shard01[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:07:26,659] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_eval_batch_size    : 8[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - per_device_train_batch_size   : 4[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_degree      : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pipeline_parallel_rank        : 0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:07:26,660] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_degree      : 4[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:07:26,661] [   DEBUG][0m - sharding_parallel_rank        : 1[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_model_state       : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - train_batch_size              : 4[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:07:26,662] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - weight_name_suffix            : [0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:07:26,663] [   DEBUG][0m - [0m
[32m[2024-12-31 16:07:26,665] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:07:26,668] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-12-31 16:07:27,453] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-12-31 16:07:27,453] [    INFO] dygraph_sharding_optimizer.py:71 - init DygraphShardingOptimizer
[2024-12-31 16:07:27,455] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:07:28,065] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:07:28) [0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:07:28,065] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 64[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total optimization steps = 23[0m
[32m[2024-12-31 16:07:28,066] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:07:28,070] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (per device)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2360, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2300, in compute_loss
    outputs = model(**inputs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 2101, in forward
    outputs = self.llama(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/nn/layer/layers.py", line 1532, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1741, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/llama/modeling.py", line 1607, in _prepare_decoder_attention_mask
    expanded_attn_mask = paddle.where(expanded_attn_mask.cast("bool"), 0.0, paddle.finfo(dtype).min)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/tensor/search.py", line 790, in where
    broadcast_x = paddle.add(x, broadcast_zeros)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/tensor/math.py", line 735, in add
    return _C_ops.add(x, y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_add(_object*, _object*, _object*)
1   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
3   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
4   void phi::AddRawKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::Allocator::Allocate(unsigned long)
14  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
15  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
16  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 15.738022GB memory on GPU 1, 68.799316GB memory has been allocated and available memory is only 10.351685GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:14:26,279] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:14:26,279] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:14:26,279] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:14:26,279] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:14:26,280] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I1231 16:14:26.280999 112005 tcp_utils.cc:130] Successfully connected to 10.3.242.26:55726
I1231 16:14:26.323311 112005 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.323338 112005 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:26.323660 112005 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:26.323670 112005 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:26,323] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:14:26.327505 112005 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:14:26.328287 112005 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-12-31 16:14:29,188] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:14:29,188] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:14:29,189] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:14:29.189210 112005 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.189227 112005 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:14:29.189272 112005 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:14:29.189278 112005 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:14:29,189] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [1], pp_group: [0, 1, 2, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:14:29,189] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:14:29,190] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:14:29,191] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:14:29,192] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:14:29,192] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:14:29,192] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:14:29,192] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:14:29,192] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:14:29,193] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:14:29,194] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:14:29,195] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:14:29,196] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:14:29,197] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:14:29,198] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:14:29,199] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:14:29,200] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - [0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:14:29,201] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:14:29,202] [   DEBUG][0m - [0m
[32m[2024-12-31 16:14:29,207] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[33m[2024-12-31 16:14:29,207] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:14:29,209] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:14:29,214] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:14:29,216] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:14:29,216] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:15:52,027] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:631 - stage=0, global_rank=1 ,layer_number=9
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,032] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:631 - stage=1, global_rank=1 ,layer_number=8
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:631 - stage=2, global_rank=1 ,layer_number=8
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:631 - stage=3, global_rank=1 ,layer_number=10
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:15:52,033] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:15:52,109] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:16:10,964] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:16:10,965] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:16:12,245] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:16:12,251] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:16:12,252] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:16:14,088] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[32m[2024-12-31 16:16:14,198] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:16:14,222] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:16:14,222] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:16:14,222] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:16:14,222] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:16:14,223] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:16:14,224] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:16:14,225] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-14-26_ubuntu[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:16:14,226] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - optimizer_name_suffix         : pp01[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,227] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - pipeline_parallel_rank        : 1[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:16:14,228] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:16:14,229] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:16:14,230] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - weight_name_suffix            : pp01[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:16:14,231] [   DEBUG][0m - [0m
[32m[2024-12-31 16:16:14,232] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:16:14,258] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:16:14,259] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 1
[2024-12-31 16:16:14,259] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:16:14,259] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:16:14) [0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:16:14,259] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:16:14,260] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:16:14,261] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py:2097: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:28:46,434] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:28:46,434] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:28:46,434] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:28:46,434] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I1231 16:28:46.435878 129648 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40539
I1231 16:28:46.436589 129648 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.436627 129648 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:46.437084 129648 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:46.437098 129648 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:46,437] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:28:46.438900 129648 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:28:46.440963 129648 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-12-31 16:28:49,486] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:28:49,487] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:28:49,487] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:28:49.487290 129648 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.487319 129648 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:28:49.487367 129648 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:28:49.487373 129648 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:28:49,487] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [1], pp_group: [0, 1, 2, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:28:49,487] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:28:49,488] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:28:49,488] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,488] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,488] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,488] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:28:49,489] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:28:49,490] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:28:49,491] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:28:49,492] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - [0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:28:49,493] [   DEBUG][0m - [0m
[32m[2024-12-31 16:28:49,495] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[33m[2024-12-31 16:28:49,495] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:28:49,497] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:28:49,499] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:28:49,500] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:28:49,500] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:30:24,647] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:631 - stage=0, global_rank=1 ,layer_number=9
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:631 - stage=1, global_rank=1 ,layer_number=8
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:631 - stage=2, global_rank=1 ,layer_number=8
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:631 - stage=3, global_rank=1 ,layer_number=10
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,652] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:30:24,653] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:30:24,726] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:30:43,223] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:30:43,223] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:30:44,463] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:30:44,468] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:30:44,468] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:30:45,119] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[32m[2024-12-31 16:30:45,250] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:30:45,268] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:30:45,269] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:30:45,270] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-28-46_ubuntu[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:30:45,271] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - optimizer_name_suffix         : pp01[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - pipeline_parallel_rank        : 1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:30:45,272] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:30:45,273] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:30:45,274] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - weight_name_suffix            : pp01[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:30:45,275] [   DEBUG][0m - [0m
[32m[2024-12-31 16:30:45,275] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:30:45,297] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:30:45,298] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 1
[2024-12-31 16:30:45,298] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:30:45,298] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:30:45) [0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:30:45,298] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:30:45,299] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:30:45,300] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Exception in thread Thread-2 (_thread_loop):
ValueError: 8192 is not a valid PaddingStrategy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 1009, in _bootstrap_inner
    self.run()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/threading.py", line 946, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/dataloader_iter.py", line 245, in _thread_loop
    batch = self._dataset_fetcher.fetch(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/io/dataloader/fetcher.py", line 77, in fetch
    data.append(self.dataset[idx])
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 266, in __getitem__
    return self._transform(self.new_data[idx]) if self._transform_pipline else self.new_data[idx]
  File "/home/sjx/tr-paddle-1230/paddlenlp/datasets/dataset.py", line 258, in _transform
    data = fn(data)
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 204, in convert_example_common
    tokenized_source = tokenize_unsupervised_example(
  File "/home/sjx/tr-paddle-1230/llm/utils/data.py", line 81, in tokenize_unsupervised_example
    tokenized_source = tokenizer(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2440, in __call__
    return self.encode(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2510, in encode
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 2106, in _get_padding_truncation_strategies
    padding_strategy = PaddingStrategy(padding)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 384, in __call__
    return cls.__new__(cls, value)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 708, in __new__
    raise exc
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/enum.py", line 692, in __new__
    result = cls._missing_(value)
  File "/home/sjx/tr-paddle-1230/paddlenlp/transformers/tokenizer_utils_base.py", line 120, in _missing_
    raise ValueError(
ValueError: 8192 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:31:44,200] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:31:44,200] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:31:44,200] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:31:44,200] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I1231 16:31:44.202103 133204 tcp_utils.cc:107] Retry to connect to 10.3.242.26:51282 while the server is not yet listening.
I1231 16:31:47.202375 133204 tcp_utils.cc:130] Successfully connected to 10.3.242.26:51282
I1231 16:31:47.275498 133204 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.275544 133204 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:47.276101 133204 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:47.276114 133204 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:47,276] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:31:47.279593 133204 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:31:47.281219 133204 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-12-31 16:31:50,164] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:31:50,165] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:31:50,166] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:31:50.166425 133204 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.166481 133204 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:31:50.166597 133204 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:31:50.166612 133204 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:31:50,166] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [1], pp_group: [0, 1, 2, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:31:50,167] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:31:50,169] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:31:50,170] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,170] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,170] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:31:50,171] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:31:50,172] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:31:50,173] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:31:50,174] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:31:50,175] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:31:50,176] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:31:50,177] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:31:50,178] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,179] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:31:50,180] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:31:50,181] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - [0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:31:50,182] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:31:50,183] [   DEBUG][0m - [0m
[32m[2024-12-31 16:31:50,188] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[33m[2024-12-31 16:31:50,188] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:31:50,192] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:31:50,197] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:31:50,199] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:31:50,200] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:32:57,405] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:631 - stage=0, global_rank=1 ,layer_number=9
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:631 - stage=1, global_rank=1 ,layer_number=8
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:631 - stage=2, global_rank=1 ,layer_number=8
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,411] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:631 - stage=3, global_rank=1 ,layer_number=10
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:32:57,412] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:32:57,493] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:33:18,903] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:33:18,903] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:33:20,161] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:33:20,167] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:33:20,167] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:33:20,762] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[32m[2024-12-31 16:33:20,906] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:33:20,930] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:33:20,931] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:33:20,932] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:33:20,933] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-31-44_ubuntu[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:33:20,934] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - optimizer_name_suffix         : pp01[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - pipeline_parallel_rank        : 1[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:33:20,935] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:33:20,936] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:33:20,937] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:33:20,938] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - weight_name_suffix            : pp01[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:33:20,939] [   DEBUG][0m - [0m
[32m[2024-12-31 16:33:20,939] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:33:20,947] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:33:20,948] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 1
[2024-12-31 16:33:20,948] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:33:20,948] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:33:20) [0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:33:20,948] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:33:20,949] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:33:20,950] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:37:07,382] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:37:07,382] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:37:07,382] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:37:07,382] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:37:07,382] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I1231 16:37:07.384053 138016 tcp_utils.cc:107] Retry to connect to 10.3.242.26:40133 while the server is not yet listening.
I1231 16:37:10.384402 138016 tcp_utils.cc:130] Successfully connected to 10.3.242.26:40133
I1231 16:37:10.411610 138016 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.411638 138016 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:10.412076 138016 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:10.412088 138016 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:10,412] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:37:10.423156 138016 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:37:10.430708 138016 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-12-31 16:37:12,985] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:37:12,986] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:37:12,986] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:37:12.986527 138016 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.986552 138016 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:37:12.986593 138016 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:37:12.986598 138016 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:37:12,986] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [1], pp_group: [0, 1, 2, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:37:12,986] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:37:12,987] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:37:12,988] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:37:12,989] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,990] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - [0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:37:12,991] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:37:12,992] [   DEBUG][0m - [0m
[32m[2024-12-31 16:37:12,993] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[33m[2024-12-31 16:37:12,993] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:37:12,995] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:37:12,997] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:37:12,998] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:37:12,998] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:38:16,735] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:631 - stage=0, global_rank=1 ,layer_number=9
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,740] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:631 - stage=1, global_rank=1 ,layer_number=8
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:631 - stage=2, global_rank=1 ,layer_number=8
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:631 - stage=3, global_rank=1 ,layer_number=10
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:38:16,741] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:38:16,822] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:38:34,249] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:38:34,249] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:38:35,461] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:38:35,466] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:38:35,466] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:38:41,290] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[32m[2024-12-31 16:38:41,427] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:38:41,451] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:38:41,452] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:38:41,453] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:38:41,454] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-37-07_ubuntu[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:38:41,455] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - optimizer_name_suffix         : pp01[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - pipeline_parallel_rank        : 1[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:38:41,456] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:38:41,457] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:38:41,458] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - weight_name_suffix            : pp01[0m
[35m[2024-12-31 16:38:41,459] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:38:41,460] [   DEBUG][0m - [0m
[32m[2024-12-31 16:38:41,460] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:38:41,474] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 1
[2024-12-31 16:38:41,474] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:38:41,475] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:38:41) [0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:38:41,475] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:38:41,476] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:38:41,477] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 679, in <module>
    main()
  File "/home/sjx/tr-paddle-1230/llm/run_finetune.py", line 419, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 876, in train
    return self._inner_training_loop(
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 1114, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, step_control=step_control)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2355, in training_step
    return self.training_pipeline_step(model, inputs)
  File "/home/sjx/tr-paddle-1230/paddlenlp/trainer/trainer.py", line 2415, in training_pipeline_step
    loss = model.forward_backward_pipeline(inputs, self.scaler if self.do_grad_scaling else None)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 611, in forward_backward_pipeline
    input_tensor = self._p2p_helper.recv_forward(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 684, in recv_forward
    self._recv_meta()
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 671, in _recv_meta
    self._send_recv_meta.recv_meta(_hcg.get_pipe_parallel_group())
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/fleet/meta_parallel/pp_utils/p2p_communication.py", line 94, in recv_meta
    paddle.distributed.recv(tensor_type, src=src_rank, group=group)
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/recv.py", line 63, in recv
    return stream.recv(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 127, in recv
    return _recv_in_dygraph(
  File "/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/distributed/communication/stream/recv.py", line 41, in _recv_in_dygraph
    task = group.process_group.recv(tensor, src_rank_in_group, sync_op)
ValueError: (InvalidArgument) TCP connection reset by peer. Details: Resource temporarily unavailable. (at ../paddle/phi/core/distributed/store/tcp_utils.h:111)

/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml
  warnings.warn(
[33m[2024-12-31 16:41:54,588] [ WARNING][0m - sharding_parallel_degree=1 means no sharding, please set sharding to empty![0m
[2024-12-31 16:41:54,588] [    INFO] distributed_strategy.py:333 - distributed strategy initialized
[32m[2024-12-31 16:41:54,588] [    INFO][0m - PP configs:{'micro_batch_size': 1, 'accumulate_steps': 4, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': True}, use master_grad: False[0m
[33m[2024-12-31 16:41:54,588] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-12-31 16:41:54,588] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False, 'overlap_p2p_comm': False, 'clear_every_step_cache': False, 'use_batch_p2p_comm': True, 'best_unbalanced_scheduler': False}[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/sjx/anaconda3/envs/paddle_test/lib/python3.10/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I1231 16:41:54.590425 141490 tcp_utils.cc:130] Successfully connected to 10.3.242.26:47469
I1231 16:41:54.603672 141490 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.603734 141490 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:54.604207 141490 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:54.604216 141490 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:54,604] [    INFO] topology.py:370 - Total 1 pipe comm group(s) create successfully!
W1231 16:41:54.605916 141490 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7
W1231 16:41:54.606652 141490 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
[2024-12-31 16:41:57,561] [    INFO] topology.py:370 - Total 4 data comm group(s) create successfully!
[2024-12-31 16:41:57,561] [    INFO] topology.py:370 - Total 4 model comm group(s) create successfully!
[2024-12-31 16:41:57,561] [    INFO] topology.py:370 - Total 4 sharding comm group(s) create successfully!
I1231 16:41:57.561921 141490 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.561949 141490 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
I1231 16:41:57.561990 141490 process_group_nccl.cc:150] ProcessGroupNCCL pg_timeout_ 1800000
I1231 16:41:57.561995 141490 process_group_nccl.cc:151] ProcessGroupNCCL nccl_comm_init_option_ 0
[2024-12-31 16:41:57,562] [    INFO] topology.py:290 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [1],  sharding_group: [1], pp_group: [0, 1, 2, 3], dp_group: [1], sep:group: None, check/clip group: [0, 1, 2, 3]
[32m[2024-12-31 16:41:57,562] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    |                         use_gpu_graph                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    |            fuse_dot_product_attention                  False                 |
    |                          fuse_resunit                  False                 |
    +==============================================================================+
[0m
[32m[2024-12-31 16:41:57,562] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m -      Model Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - aistudio_repo_id              : None[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - aistudio_repo_license         : Apache License 2.0[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - aistudio_repo_private         : True[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - aistudio_token                : None[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - attention_probs_dropout_prob  : 0.1[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - continue_training             : True[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - flash_mask                    : False[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - from_aistudio                 : False[0m
[35m[2024-12-31 16:41:57,563] [   DEBUG][0m - fuse_attention_ffn            : None[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - fuse_attention_qkv            : None[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - hidden_dropout_prob           : 0.1[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lokr                          : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lokr_dim                      : 8[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lokr_path                     : None[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lora                          : True[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lora_path                     : None[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lora_plus_scale               : 1.0[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lora_rank                     : 8[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - lora_use_mixer                : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - model_name_or_path            : meta-llama/Llama-2-7b[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - neftune                       : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - neftune_noise_alpha           : 5.0[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - num_prefix_tokens             : 128[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - pissa                         : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - prefix_path                   : None[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - prefix_tuning                 : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - reft                          : False[0m
[35m[2024-12-31 16:41:57,564] [   DEBUG][0m - rope_scaling_factor           : 1.0[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - rslora                        : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - save_to_aistudio              : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - strategy_name                 : None[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - strategy_type                 : None[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - tokenizer_name_or_path        : None[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - use_fast_layer_norm           : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - use_long_sequence_strategies  : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - use_mora                      : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - use_quick_lora                : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - vera                          : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - vera_rank                     : 8[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - weight_blocksize              : 64[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - weight_double_quant           : False[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - weight_double_quant_block_size: 256[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - weight_quantize_algo          : None[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m -       Data Configuration Arguments      [0m
[35m[2024-12-31 16:41:57,565] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - autoregressive                : True[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - chat_template                 : None[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - dataset_name_or_path          : /home/sjx/tr-paddle-1230/train_data[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - eval_with_do_generation       : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - greedy_zero_padding           : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - lazy                          : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - max_length                    : 8192[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - pad_to_max_length             : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - pad_to_multiple_of            : None[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - save_generation_output        : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - src_length                    : 4096[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - task_name                     : None[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - use_pose_convert              : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - zero_padding                  : False[0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - [0m
[35m[2024-12-31 16:41:57,566] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m -    Generation Configuration Arguments   [0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - top_k                         : 1[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - top_p                         : 1.0[0m
[35m[2024-12-31 16:41:57,567] [   DEBUG][0m - [0m
[32m[2024-12-31 16:41:57,568] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[33m[2024-12-31 16:41:57,568] [ WARNING][0m - Process rank: 1, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-12-31 16:41:57,570] [    INFO][0m - Loading configuration file /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/config.json[0m
[32m[2024-12-31 16:41:57,572] [    INFO][0m - Final model config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "dpo_config": null,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "immediate_clear_past_key_value": false,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "long_sequence_init_args": {},
  "long_sequence_strategy_name": null,
  "long_sequence_strategy_type": null,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": "3.0.0b3.post20241231",
  "pipeline_parallel_degree": 4,
  "pretraining_tp": 1,
  "recompute": true,
  "refined_recompute": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_scaling_factor": 2.0,
  "rope_scaling_type": "linear",
  "rope_theta": 10000.0,
  "seq_length": 8192,
  "tensor_parallel_output": false,
  "tie_word_embeddings": false,
  "use_fast_layer_norm": false,
  "use_flash_attention": true,
  "use_flash_attention_for_generation": false,
  "use_last_token_for_generation": false,
  "use_long_sequence_strategies": false,
  "vocab_size": 32000
}
[0m
[32m[2024-12-31 16:41:57,572] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling_pp.LlamaForCausalLMPipe'> to load 'meta-llama/Llama-2-7b'.[0m
[32m[2024-12-31 16:41:57,573] [    INFO][0m - Loading weights file from cache at /home/sjx/.paddlenlp/models/meta-llama/Llama-2-7b/model_state.pdparams[0m
[32m[2024-12-31 16:43:04,820] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:609 - start segment network..
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:615 - segment with method: layer:LlamaDecoderLayer; result: 0, 9, 17, 25, 35
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:631 - stage=0, global_rank=1 ,layer_number=9
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 0: LlamaEmbeddingPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 1: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 2: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 3: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 4: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 5: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 6: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 7: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 8: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:631 - stage=1, global_rank=1 ,layer_number=8
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 9: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 10: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 11: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 12: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 13: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 14: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 15: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 16: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:631 - stage=2, global_rank=1 ,layer_number=8
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 17: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 18: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 19: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 20: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 21: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 22: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 23: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 24: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:631 - stage=3, global_rank=1 ,layer_number=10
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 25: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 26: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 27: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 28: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 29: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 30: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 31: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 32: LlamaDecoderLayerPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 33: LlamaRMSNormPipe
[2024-12-31 16:43:04,825] [    INFO] pp_layers.py:636 - 34: LlamaLMHeadPipe
[2024-12-31 16:43:04,826] [    INFO] pp_layers.py:658 - loss: LlamaPretrainingCriterion
[2024-12-31 16:43:04,904] [    INFO] pp_layers.py:708 - flush 8 of layers into run_function
[33m[2024-12-31 16:43:22,081] [ WARNING][0m - Some weights of the model checkpoint at meta-llama/Llama-2-7b were not used when initializing LlamaForCausalLMPipe: ['llama.embed_tokens.weight', 'llama.layers.0.input_layernorm.weight', 'llama.layers.0.mlp.down_proj.weight', 'llama.layers.0.mlp.gate_proj.weight', 'llama.layers.0.mlp.up_proj.weight', 'llama.layers.0.post_attention_layernorm.weight', 'llama.layers.0.self_attn.k_proj.weight', 'llama.layers.0.self_attn.o_proj.weight', 'llama.layers.0.self_attn.q_proj.weight', 'llama.layers.0.self_attn.v_proj.weight', 'llama.layers.1.input_layernorm.weight', 'llama.layers.1.mlp.down_proj.weight', 'llama.layers.1.mlp.gate_proj.weight', 'llama.layers.1.mlp.up_proj.weight', 'llama.layers.1.post_attention_layernorm.weight', 'llama.layers.1.self_attn.k_proj.weight', 'llama.layers.1.self_attn.o_proj.weight', 'llama.layers.1.self_attn.q_proj.weight', 'llama.layers.1.self_attn.v_proj.weight', 'llama.layers.16.input_layernorm.weight', 'llama.layers.16.mlp.down_proj.weight', 'llama.layers.16.mlp.gate_proj.weight', 'llama.layers.16.mlp.up_proj.weight', 'llama.layers.16.post_attention_layernorm.weight', 'llama.layers.16.self_attn.k_proj.weight', 'llama.layers.16.self_attn.o_proj.weight', 'llama.layers.16.self_attn.q_proj.weight', 'llama.layers.16.self_attn.v_proj.weight', 'llama.layers.17.input_layernorm.weight', 'llama.layers.17.mlp.down_proj.weight', 'llama.layers.17.mlp.gate_proj.weight', 'llama.layers.17.mlp.up_proj.weight', 'llama.layers.17.post_attention_layernorm.weight', 'llama.layers.17.self_attn.k_proj.weight', 'llama.layers.17.self_attn.o_proj.weight', 'llama.layers.17.self_attn.q_proj.weight', 'llama.layers.17.self_attn.v_proj.weight', 'llama.layers.18.input_layernorm.weight', 'llama.layers.18.mlp.down_proj.weight', 'llama.layers.18.mlp.gate_proj.weight', 'llama.layers.18.mlp.up_proj.weight', 'llama.layers.18.post_attention_layernorm.weight', 'llama.layers.18.self_attn.k_proj.weight', 'llama.layers.18.self_attn.o_proj.weight', 'llama.layers.18.self_attn.q_proj.weight', 'llama.layers.18.self_attn.v_proj.weight', 'llama.layers.19.input_layernorm.weight', 'llama.layers.19.mlp.down_proj.weight', 'llama.layers.19.mlp.gate_proj.weight', 'llama.layers.19.mlp.up_proj.weight', 'llama.layers.19.post_attention_layernorm.weight', 'llama.layers.19.self_attn.k_proj.weight', 'llama.layers.19.self_attn.o_proj.weight', 'llama.layers.19.self_attn.q_proj.weight', 'llama.layers.19.self_attn.v_proj.weight', 'llama.layers.2.input_layernorm.weight', 'llama.layers.2.mlp.down_proj.weight', 'llama.layers.2.mlp.gate_proj.weight', 'llama.layers.2.mlp.up_proj.weight', 'llama.layers.2.post_attention_layernorm.weight', 'llama.layers.2.self_attn.k_proj.weight', 'llama.layers.2.self_attn.o_proj.weight', 'llama.layers.2.self_attn.q_proj.weight', 'llama.layers.2.self_attn.v_proj.weight', 'llama.layers.20.input_layernorm.weight', 'llama.layers.20.mlp.down_proj.weight', 'llama.layers.20.mlp.gate_proj.weight', 'llama.layers.20.mlp.up_proj.weight', 'llama.layers.20.post_attention_layernorm.weight', 'llama.layers.20.self_attn.k_proj.weight', 'llama.layers.20.self_attn.o_proj.weight', 'llama.layers.20.self_attn.q_proj.weight', 'llama.layers.20.self_attn.v_proj.weight', 'llama.layers.21.input_layernorm.weight', 'llama.layers.21.mlp.down_proj.weight', 'llama.layers.21.mlp.gate_proj.weight', 'llama.layers.21.mlp.up_proj.weight', 'llama.layers.21.post_attention_layernorm.weight', 'llama.layers.21.self_attn.k_proj.weight', 'llama.layers.21.self_attn.o_proj.weight', 'llama.layers.21.self_attn.q_proj.weight', 'llama.layers.21.self_attn.v_proj.weight', 'llama.layers.22.input_layernorm.weight', 'llama.layers.22.mlp.down_proj.weight', 'llama.layers.22.mlp.gate_proj.weight', 'llama.layers.22.mlp.up_proj.weight', 'llama.layers.22.post_attention_layernorm.weight', 'llama.layers.22.self_attn.k_proj.weight', 'llama.layers.22.self_attn.o_proj.weight', 'llama.layers.22.self_attn.q_proj.weight', 'llama.layers.22.self_attn.v_proj.weight', 'llama.layers.23.input_layernorm.weight', 'llama.layers.23.mlp.down_proj.weight', 'llama.layers.23.mlp.gate_proj.weight', 'llama.layers.23.mlp.up_proj.weight', 'llama.layers.23.post_attention_layernorm.weight', 'llama.layers.23.self_attn.k_proj.weight', 'llama.layers.23.self_attn.o_proj.weight', 'llama.layers.23.self_attn.q_proj.weight', 'llama.layers.23.self_attn.v_proj.weight', 'llama.layers.24.input_layernorm.weight', 'llama.layers.24.mlp.down_proj.weight', 'llama.layers.24.mlp.gate_proj.weight', 'llama.layers.24.mlp.up_proj.weight', 'llama.layers.24.post_attention_layernorm.weight', 'llama.layers.24.self_attn.k_proj.weight', 'llama.layers.24.self_attn.o_proj.weight', 'llama.layers.24.self_attn.q_proj.weight', 'llama.layers.24.self_attn.v_proj.weight', 'llama.layers.25.input_layernorm.weight', 'llama.layers.25.mlp.down_proj.weight', 'llama.layers.25.mlp.gate_proj.weight', 'llama.layers.25.mlp.up_proj.weight', 'llama.layers.25.post_attention_layernorm.weight', 'llama.layers.25.self_attn.k_proj.weight', 'llama.layers.25.self_attn.o_proj.weight', 'llama.layers.25.self_attn.q_proj.weight', 'llama.layers.25.self_attn.v_proj.weight', 'llama.layers.26.input_layernorm.weight', 'llama.layers.26.mlp.down_proj.weight', 'llama.layers.26.mlp.gate_proj.weight', 'llama.layers.26.mlp.up_proj.weight', 'llama.layers.26.post_attention_layernorm.weight', 'llama.layers.26.self_attn.k_proj.weight', 'llama.layers.26.self_attn.o_proj.weight', 'llama.layers.26.self_attn.q_proj.weight', 'llama.layers.26.self_attn.v_proj.weight', 'llama.layers.27.input_layernorm.weight', 'llama.layers.27.mlp.down_proj.weight', 'llama.layers.27.mlp.gate_proj.weight', 'llama.layers.27.mlp.up_proj.weight', 'llama.layers.27.post_attention_layernorm.weight', 'llama.layers.27.self_attn.k_proj.weight', 'llama.layers.27.self_attn.o_proj.weight', 'llama.layers.27.self_attn.q_proj.weight', 'llama.layers.27.self_attn.v_proj.weight', 'llama.layers.28.input_layernorm.weight', 'llama.layers.28.mlp.down_proj.weight', 'llama.layers.28.mlp.gate_proj.weight', 'llama.layers.28.mlp.up_proj.weight', 'llama.layers.28.post_attention_layernorm.weight', 'llama.layers.28.self_attn.k_proj.weight', 'llama.layers.28.self_attn.o_proj.weight', 'llama.layers.28.self_attn.q_proj.weight', 'llama.layers.28.self_attn.v_proj.weight', 'llama.layers.29.input_layernorm.weight', 'llama.layers.29.mlp.down_proj.weight', 'llama.layers.29.mlp.gate_proj.weight', 'llama.layers.29.mlp.up_proj.weight', 'llama.layers.29.post_attention_layernorm.weight', 'llama.layers.29.self_attn.k_proj.weight', 'llama.layers.29.self_attn.o_proj.weight', 'llama.layers.29.self_attn.q_proj.weight', 'llama.layers.29.self_attn.v_proj.weight', 'llama.layers.3.input_layernorm.weight', 'llama.layers.3.mlp.down_proj.weight', 'llama.layers.3.mlp.gate_proj.weight', 'llama.layers.3.mlp.up_proj.weight', 'llama.layers.3.post_attention_layernorm.weight', 'llama.layers.3.self_attn.k_proj.weight', 'llama.layers.3.self_attn.o_proj.weight', 'llama.layers.3.self_attn.q_proj.weight', 'llama.layers.3.self_attn.v_proj.weight', 'llama.layers.30.input_layernorm.weight', 'llama.layers.30.mlp.down_proj.weight', 'llama.layers.30.mlp.gate_proj.weight', 'llama.layers.30.mlp.up_proj.weight', 'llama.layers.30.post_attention_layernorm.weight', 'llama.layers.30.self_attn.k_proj.weight', 'llama.layers.30.self_attn.o_proj.weight', 'llama.layers.30.self_attn.q_proj.weight', 'llama.layers.30.self_attn.v_proj.weight', 'llama.layers.31.input_layernorm.weight', 'llama.layers.31.mlp.down_proj.weight', 'llama.layers.31.mlp.gate_proj.weight', 'llama.layers.31.mlp.up_proj.weight', 'llama.layers.31.post_attention_layernorm.weight', 'llama.layers.31.self_attn.k_proj.weight', 'llama.layers.31.self_attn.o_proj.weight', 'llama.layers.31.self_attn.q_proj.weight', 'llama.layers.31.self_attn.v_proj.weight', 'llama.layers.4.input_layernorm.weight', 'llama.layers.4.mlp.down_proj.weight', 'llama.layers.4.mlp.gate_proj.weight', 'llama.layers.4.mlp.up_proj.weight', 'llama.layers.4.post_attention_layernorm.weight', 'llama.layers.4.self_attn.k_proj.weight', 'llama.layers.4.self_attn.o_proj.weight', 'llama.layers.4.self_attn.q_proj.weight', 'llama.layers.4.self_attn.v_proj.weight', 'llama.layers.5.input_layernorm.weight', 'llama.layers.5.mlp.down_proj.weight', 'llama.layers.5.mlp.gate_proj.weight', 'llama.layers.5.mlp.up_proj.weight', 'llama.layers.5.post_attention_layernorm.weight', 'llama.layers.5.self_attn.k_proj.weight', 'llama.layers.5.self_attn.o_proj.weight', 'llama.layers.5.self_attn.q_proj.weight', 'llama.layers.5.self_attn.v_proj.weight', 'llama.layers.6.input_layernorm.weight', 'llama.layers.6.mlp.down_proj.weight', 'llama.layers.6.mlp.gate_proj.weight', 'llama.layers.6.mlp.up_proj.weight', 'llama.layers.6.post_attention_layernorm.weight', 'llama.layers.6.self_attn.k_proj.weight', 'llama.layers.6.self_attn.o_proj.weight', 'llama.layers.6.self_attn.q_proj.weight', 'llama.layers.6.self_attn.v_proj.weight', 'llama.layers.7.input_layernorm.weight', 'llama.layers.7.mlp.down_proj.weight', 'llama.layers.7.mlp.gate_proj.weight', 'llama.layers.7.mlp.up_proj.weight', 'llama.layers.7.post_attention_layernorm.weight', 'llama.layers.7.self_attn.k_proj.weight', 'llama.layers.7.self_attn.o_proj.weight', 'llama.layers.7.self_attn.q_proj.weight', 'llama.layers.7.self_attn.v_proj.weight', 'llama.norm.weight', 'lm_head.weight']
- This IS expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLMPipe from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[32m[2024-12-31 16:43:22,081] [    INFO][0m - All the weights of LlamaForCausalLMPipe were initialized from the model checkpoint at meta-llama/Llama-2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLMPipe for predictions without further training.[0m
[32m[2024-12-31 16:43:23,274] [    INFO][0m - Mark only lora and trainable_module as trainable.[0m
[35m[2024-12-31 16:43:23,278] [   DEBUG][0m - Frozen parameters: 1.62e+09 || Trainable parameters:5.00e+06 || Total parameters:1.62e+09|| Trainable:0.31%[0m
[35m[2024-12-31 16:43:23,279] [   DEBUG][0m - 1: waiting for the main local process to perform work[0m
[32m[2024-12-31 16:43:30,725] [    INFO][0m - The global seed is set to 43, local seed is set to 47 and random seed is set to 142.[0m
[32m[2024-12-31 16:43:30,860] [    INFO][0m - Using half precision[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - ============================================================[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m -     Training Configuration Arguments    [0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - paddle commit id              : 396a3f594f154081b444a3ab40e7f28d89c32521[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - paddlenlp commit id           : 79695ccd747b5a6b18bdec65503aacd419ca9a46.dirty[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - _no_sync_in_gradient_accumulation: True[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - adam_beta1                    : 0.9[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - adam_beta2                    : 0.999[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - adam_epsilon                  : 1e-08[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - amp_custom_black_list         : None[0m
[35m[2024-12-31 16:43:30,883] [   DEBUG][0m - amp_custom_white_list         : None[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - amp_master_grad               : False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - auto_parallel_resume_form_hybrid_parallel: False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - autotuner_benchmark           : False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - benchmark                     : False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - bf16                          : True[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - bf16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - ckpt_quant_stage              : O0[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - context_parallel_degree       : 1[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - current_device                : gpu:1[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - data_parallel_config          : [0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - data_parallel_degree          : 1[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - data_parallel_rank            : 0[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataloader_drop_last          : False[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataloader_num_workers        : 0[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataset_batch_size            : 1000[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataset_kwargs                : {}[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataset_num_proc              : None[0m
[35m[2024-12-31 16:43:30,884] [   DEBUG][0m - dataset_rank                  : 0[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - dataset_text_field            : text[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - dataset_world_size            : 1[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - ddp_find_unused_parameters    : None[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - decay_steps                   : 0[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - device                        : gpu[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - disable_tqdm                  : True[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - distributed_dataloader        : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - do_eval                       : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - do_export                     : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - do_predict                    : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - do_train                      : True[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - enable_auto_parallel          : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - enable_sharding_comm_overlap  : False[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - eval_accumulation_steps       : 16[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - eval_batch_size               : 4[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - eval_packing                  : None[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - eval_steps                    : None[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - evaluation_strategy           : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - expert_max_capacity           : 4294967296[0m
[35m[2024-12-31 16:43:30,885] [   DEBUG][0m - expert_min_capacity           : 1[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - flatten_param_grads           : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - force_reshard_pp              : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - fp16                          : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - fp16_full_eval                : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - fp16_opt_level                : O2[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - fuse_sequence_parallel_allreduce: False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - gradient_accumulation_steps   : 4[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - greater_is_better             : True[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - hybrid_parallel_topo_order    : pp_first[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - ignore_data_skip              : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - ignore_load_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - ignore_save_lr_and_optim      : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - label_names                   : None[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - lazy_data_processing          : True[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - learning_rate                 : 0.0003[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - load_best_model_at_end        : True[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - load_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - local_process_index           : 1[0m
[35m[2024-12-31 16:43:30,886] [   DEBUG][0m - local_rank                    : 1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - log_level                     : -1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - log_level_replica             : -1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - log_on_each_node              : True[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - logging_dir                   : ./checkpoints/lora_ckpts/runs/Dec31_16-41-54_ubuntu[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - logging_first_step            : False[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - logging_steps                 : 1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - logical_process_index         : 1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - lr_end                        : 1e-07[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - max_evaluate_steps            : -1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - max_grad_norm                 : 1.0[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - max_seq_length                : 2048[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - max_steps                     : -1[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - metric_for_best_model         : accuracy[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - minimum_eval_times            : None[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - model_init_kwargs             : None[0m
[35m[2024-12-31 16:43:30,887] [   DEBUG][0m - no_cuda                       : False[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - no_recompute_layers           : None[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - num_cycles                    : 0.5[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - num_train_epochs              : 1.0[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - offload_optim                 : False[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - optim                         : OptimizerNames.ADAMW[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - optimizer_name_suffix         : pp01[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - ordered_save_group_size       : 0[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - output_dir                    : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - output_signal_dir             : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - overwrite_output_dir          : False[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - past_index                    : -1[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - per_device_eval_batch_size    : 4[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - per_device_train_batch_size   : 1[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - pipeline_parallel_config      : [0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - pipeline_parallel_degree      : 4[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - pipeline_parallel_rank        : 1[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - power                         : 1.0[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - pp_recompute_interval         : 1[0m
[35m[2024-12-31 16:43:30,888] [   DEBUG][0m - prediction_loss_only          : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - process_index                 : 1[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - recompute                     : True[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - recompute_granularity         : full[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - recompute_use_reentrant       : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - refined_recompute             : [0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - release_grads                 : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - remove_unused_columns         : True[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - report_to                     : ['visualdl'][0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - resume_from_checkpoint        : None[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - run_name                      : ./checkpoints/lora_ckpts[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - save_on_each_node             : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - save_sharded_model            : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - save_steps                    : 500[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - save_strategy                 : IntervalStrategy.EPOCH[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - save_total_limit              : 1[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - scale_loss                    : 32768[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - seed                          : 42[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - sep_parallel_degree           : 1[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - sequence_parallel             : False[0m
[35m[2024-12-31 16:43:30,889] [   DEBUG][0m - sequence_parallel_config      : [0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding                      : [][0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_comm_buffer_size_MB  : -1[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_degree               : -1[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_parallel_config      : [0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_parallel_degree      : 1[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_parallel_mesh_dimension: dp[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - sharding_parallel_rank        : 0[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_load_dataset           : True[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_load_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_log                    : False[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_save                   : False[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_save_model_state       : True[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - should_save_sharding_stage1_model: False[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - skip_data_intervals           : None[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - skip_memory_metrics           : True[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - skip_profile_timer            : True[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - skip_recompute_ops            : None[0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - tensor_parallel_config        : [0m
[35m[2024-12-31 16:43:30,890] [   DEBUG][0m - tensor_parallel_degree        : 1[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - tensor_parallel_output        : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - tensor_parallel_rank          : 0[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - to_static                     : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - train_batch_size              : 1[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - unified_checkpoint            : True[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - unified_checkpoint_config     : [''][0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_async_save                : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_expert_parallel           : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_flash_attention           : True[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_fused_dropout_add         : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_fused_linear              : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_fused_rms_norm            : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_fused_rope                : False[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - use_hybrid_parallel           : True[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - virtual_pp_degree             : 1[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - wandb_api_key                 : None[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - warmup_ratio                  : 0.0[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - warmup_steps                  : 30[0m
[35m[2024-12-31 16:43:30,891] [   DEBUG][0m - weight_decay                  : 0.0[0m
[35m[2024-12-31 16:43:30,892] [   DEBUG][0m - weight_name_suffix            : pp01[0m
[35m[2024-12-31 16:43:30,892] [   DEBUG][0m - world_size                    : 4[0m
[35m[2024-12-31 16:43:30,892] [   DEBUG][0m - [0m
[32m[2024-12-31 16:43:30,892] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-12-31 16:43:30,898] [    INFO] pipeline_parallel.py:331 - dp_comm_overlap False;             sharding_comm_overlap False;             sharding_split_param False;
[2024-12-31 16:43:30,898] [    INFO] pipeline_parallel.py:404 - Pipeline Info -- num_stages: 4, stage_id: 1
[2024-12-31 16:43:30,898] [ WARNING] hybrid_parallel_optimizer.py:313 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-12-31 16:43:30,898] [    INFO][0m - [timelog] checkpoint loading time: 0.00s (2024-12-31 16:43:30) [0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m - ***** Running training *****[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Num examples = 1,510[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-12-31 16:43:30,898] [    INFO][0m -   Instantaneous batch size per device = 1[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Gradient Accumulation steps = 4[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Total optimization steps = 377[0m
[32m[2024-12-31 16:43:30,899] [    INFO][0m -   Total num train samples = 1,510[0m
[35m[2024-12-31 16:43:30,900] [   DEBUG][0m -   Number of trainable parameters = 4,997,120 (per device)[0m
[35m[2024-12-31 16:43:30,902] [   DEBUG][0m -   Number of trainable parameters = 19,988,480 (all devices, roughly)[0m
W1231 16:43:33.705417 141490 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
